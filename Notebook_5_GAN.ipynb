{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_5_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Some features stopped working so I have imported prev version of Tensorflow for this one. I originally did it with Tensorflow Keras latest version but for some reason today it wasn't working as expected when I re run some parts. Hence I disabled some prev functionalities."
      ],
      "metadata": {
        "id": "d86iiAofS7aK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GAN Generative Adversarial Networks"
      ],
      "metadata": {
        "id": "ma3I6V2VOywN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGrrt16a7kR2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import urllib\n",
        "import json\n",
        "import csv\n",
        "import requests\n",
        "import io\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "#set up the google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzSA6-II8nEg"
      },
      "source": [
        "googlepath = \"/data/AMZN.csv\"\n",
        "\n",
        "# Setting the Training Amount\n",
        "TRAINING_AMOUNT = 50000 # low to test for now\n",
        "SAVE_STEPS_AMOUNT = 10000 # testing for now\n",
        "PCT_CHANGE_AMOUNT = 5 # just want to see up down trends\n",
        "HISTORICAL_DAYS_AMOUNT = 20\n",
        "DAYS_AHEAD = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIy6WWcg-M5q"
      },
      "source": [
        "#plot confusion matrices\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh-UWwe-wyT4"
      },
      "source": [
        "**THIS IS A CLASSIFICATION PROBLEM. I WILL TRY TO PREDICT WHETHER THE STOCK PRICES CAN GO UP OR DOWN. CONFUSION MATRIX HAS 2 LABELS UP AND DOWN. UP INDICATES STOCK PRICES ARE GOING UP AND DOWN INDICATES OTHERWISE. WILL USE GAN TO TRY AND PREDICT HOW MANY TIMES I GET STOCK PRICE MOVEMENTS RIGHT.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k070Rvz8Qirt"
      },
      "source": [
        "import tensorflow as tf #Define the GAN and data generator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "tf.set_random_seed(SEED)\n",
        "\n",
        "class GAN():\n",
        "\n",
        "    def sample_Z(self, batch_size, n):\n",
        "        return np.random.uniform(-1., 1., size=(batch_size, n))\n",
        "\n",
        "    def __init__(self, num_features, num_historical_days, generator_input_size=200, is_train=True):\n",
        "        def get_batch_norm_with_global_normalization_vars(size):\n",
        "            v = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            m = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            beta = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            gamma = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            return v, m, beta, gamma\n",
        "\n",
        "        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n",
        "        self.Z = tf.placeholder(tf.float32, shape=[None, generator_input_size])\n",
        "\n",
        "        generator_output_size = num_features*num_historical_days\n",
        "        with tf.variable_scope(\"generator\"):\n",
        "            W1 = tf.Variable(tf.truncated_normal([generator_input_size, generator_output_size*10]))\n",
        "            b1 = tf.Variable(tf.truncated_normal([generator_output_size*10]))\n",
        "\n",
        "            h1 = tf.nn.sigmoid(tf.matmul(self.Z, W1) + b1)\n",
        "\n",
        "            # v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(generator_output_size*10)\n",
        "            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v1, m1,\n",
        "            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([generator_output_size*10, generator_output_size*5]))\n",
        "            b2 = tf.Variable(tf.truncated_normal([generator_output_size*5]))\n",
        "\n",
        "            h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2)\n",
        "\n",
        "            # v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(generator_output_size*5)\n",
        "            # h2 = tf.nn.batch_norm_with_global_normalization(h2, v2, m2,\n",
        "            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "\n",
        "            W3 = tf.Variable(tf.truncated_normal([generator_output_size*5, generator_output_size]))\n",
        "            b3 = tf.Variable(tf.truncated_normal([generator_output_size]))\n",
        "\n",
        "            g_log_prob = tf.matmul(h2, W3) + b3\n",
        "            g_log_prob = tf.reshape(g_log_prob, [-1, num_historical_days, 1, num_features])\n",
        "            self.gen_data = tf.reshape(g_log_prob, [-1, num_historical_days, num_features])\n",
        "            #g_log_prob = g_log_prob / tf.reshape(tf.reduce_max(g_log_prob, axis=1), [-1, 1, num_features, 1])\n",
        "            #g_prob = tf.nn.sigmoid(g_log_prob)\n",
        "\n",
        "            theta_G = [W1, b1, W2, b2, W3, b3]\n",
        "\n",
        "\n",
        "\n",
        "        with tf.variable_scope(\"discriminator\"):\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 32],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b1 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "\n",
        "            v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(32)\n",
        "\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b2 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
        "\n",
        "            v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(64)\n",
        "\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 64, 128],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b3 = tf.Variable(tf.zeros([128], dtype=tf.float32))\n",
        "\n",
        "            v3, m3, beta3, gamma3 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*128, 128]))\n",
        "            b4 = tf.Variable(tf.truncated_normal([128]))\n",
        "\n",
        "            v4, m4, beta4, gamma4 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([128, 1]))\n",
        "\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]\n",
        "\n",
        "        def discriminator(X):\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n",
        "            pool = relu\n",
        "            # pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "            # pool = tf.nn.batch_norm_with_global_normalization(pool, v1, m1,\n",
        "            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(pool)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k2,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n",
        "            pool = relu\n",
        "            #pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "            # pool = tf.nn.batch_norm_with_global_normalization(pool, v2, m2,\n",
        "            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(pool)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k3, strides=[1, 1, 1, 1], padding='VALID')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob=0.8)\n",
        "            # relu = tf.nn.batch_norm_with_global_normalization(relu, v3, m3,\n",
        "            #         beta3, gamma3, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n",
        "            print(flattened_convolution_size)\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n",
        "\n",
        "            if is_train:\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=0.8)\n",
        "\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n",
        "\n",
        "            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v4, m4,\n",
        "            #         beta4, gamma4, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "            D_logit = tf.matmul(h1, W2)\n",
        "            D_prob = tf.nn.sigmoid(D_logit)\n",
        "            return D_prob, D_logit, features\n",
        "\n",
        "        D_real, D_logit_real, self.features = discriminator(X)\n",
        "        D_fake, D_logit_fake, _ = discriminator(g_log_prob)\n",
        "\n",
        "\n",
        "        D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
        "        D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
        "        self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n",
        "        self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n",
        "        self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n",
        "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n",
        "\n",
        "\n",
        "        self.D_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.D_loss, var_list=theta_D)\n",
        "        self.G_solver = tf.train.AdamOptimizer(learning_rate=0.000055).minimize(self.G_loss, var_list=theta_G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4-qKPvQQ4zD",
        "outputId": "541c11d8-9aea-4a57-cf4e-4b7a22fac2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training the GAN\n",
        "import os\n",
        "import pandas as pd\n",
        "# from gan import GAN\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "random.seed(42)\n",
        "class TrainGan:\n",
        "\n",
        "    def __init__(self, num_historical_days, batch_size=128):\n",
        "        self.batch_size = batch_size\n",
        "        self.data = []\n",
        "#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n",
        "\n",
        "        # Google Drive Method\n",
        "        files = '/data/AMZN.csv'\n",
        "#         print(files)\n",
        "      \n",
        "        for file in files:\n",
        "            print(file)\n",
        "            #Read in file -- note that parse_dates will be need later\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            #df = df[['open','high','low','close','volume']]\n",
        "            # #Create new index with missing days\n",
        "            # idx = pd.date_range(df.index[-1], df.index[0])\n",
        "            # #Reindex and fill the missing day with the value from the day before\n",
        "            # df = df.reindex(idx, method='bfill').sort_index(ascending=False)\n",
        "            #Normilize using a of size num_historical_days\n",
        "            df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "            #Drop the last 10 day that we don't have data for\n",
        "            df = df.dropna()\n",
        "            #Hold out the last year of trading for testing\n",
        "            #Padding to keep labels from bleeding\n",
        "            df = df[400:]\n",
        "            #This may not create good samples if num_historical_days is a\n",
        "            #mutliple of 7\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                self.data.append(df.values[i-num_historical_days:i])\n",
        "\n",
        "        self.gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200)\n",
        "\n",
        "    def random_batch(self, batch_size=128):\n",
        "        batch = []\n",
        "        while True:\n",
        "            batch.append(random.choice(self.data))\n",
        "            if (len(batch) == batch_size):\n",
        "                yield batch\n",
        "                batch = []\n",
        "\n",
        "    def train(self, print_steps=100, display_data=100, save_steps=SAVE_STEPS_AMOUNT):\n",
        "        if not os.path.exists(f'{googlepath}models'):\n",
        "            os.makedirs(f'{googlepath}models')\n",
        "        sess = tf.Session()\n",
        "        \n",
        "        G_loss = 0\n",
        "        D_loss = 0\n",
        "        G_l2_loss = 0\n",
        "        D_l2_loss = 0\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        currentStep = \"0\"\n",
        "        \n",
        "        g_loss_array = []\n",
        "        d_loss_array = []\n",
        "        \n",
        "        if os.path.exists(f'{googlepath}models/checkpoint'):\n",
        "                with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "                    model_name = next(f).split('\"'.encode())[1]\n",
        "                filename = \"{}models/{}\".format(googlepath, model_name.decode())\n",
        "                currentStep = filename.split(\"-\")[1]\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\n",
        "\n",
        "        for i, X in enumerate(self.random_batch(self.batch_size)):\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "            if i % 1 == 0:\n",
        "                _, D_loss_curr, D_l2_loss_curr = sess.run([self.gan.D_solver, self.gan.D_loss, self.gan.D_l2_loss], feed_dict=\n",
        "                        {self.gan.X:X, self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                D_loss += D_loss_curr\n",
        "                D_l2_loss += D_l2_loss_curr\n",
        "            if i % 1 == 0:\n",
        "                _, G_loss_curr, G_l2_loss_curr = sess.run([self.gan.G_solver, self.gan.G_loss, self.gan.G_l2_loss],\n",
        "                        feed_dict={self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                G_loss += G_loss_curr\n",
        "                G_l2_loss += G_l2_loss_curr\n",
        "                \n",
        "            g_loss_array.append(G_loss_curr - G_l2_loss)\n",
        "            d_loss_array.append(D_loss_curr - D_l2_loss)\n",
        "            \n",
        "            \n",
        "            if (i+1) % print_steps == 0:\n",
        "                print('Step={} D_loss={}, G_loss={}'.format(i + int(currentStep), D_loss/print_steps - D_l2_loss/print_steps, G_loss/print_steps - G_l2_loss/print_steps))\n",
        "                #print('D_l2_loss = {} G_l2_loss={}'.format(D_l2_loss/print_steps, G_l2_loss/print_steps))\n",
        "                G_loss = 0\n",
        "                D_loss = 0\n",
        "                G_l2_loss = 0\n",
        "                D_l2_loss = 0\n",
        "            if (i+1) % save_steps == 0:\n",
        "                saver.save(sess, f'{googlepath}/models/gan.ckpt', i + int(currentStep))\n",
        "            \n",
        "            # end training at training_amount epochs\n",
        "            if ((i + int(currentStep)) > TRAINING_AMOUNT):\n",
        "                \n",
        "                print(\"Reached {} epochs for GAN\".format(i + int(currentStep)))\n",
        "                sess.close()\n",
        "                \n",
        "                axisX = np.arange(0,len(g_loss_array),1)\n",
        "                plt.plot(axisX, g_loss_array, label='generator loss')\n",
        "                plt.plot(axisX, d_loss_array, label='discriminator loss')\n",
        "                plt.legend()\n",
        "                plt.title('generator and discriminator loss')\n",
        "                plt.show()\n",
        "                \n",
        "                break\n",
        "\n",
        "            # if (i+1) % display_data == 0:\n",
        "            #     print('Generated Data')\n",
        "            #     print(sess.run(self.gan.gen_data, feed_dict={self.gan.Z:self.gan.sample_Z(1, 200)}))\n",
        "            #     print('Real Data')\n",
        "            #     print(X[0])\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "tf.reset_default_graph()\n",
        "gan = TrainGan(HISTORICAL_DAYS_AMOUNT, 128)\n",
        "gan.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n",
            "Tensor(\"dropout/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_1/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_2/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"dropout_4/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_5/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_6/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Step=99 D_loss=21.482124568223952, G_loss=1003.659901984036\n",
            "Step=199 D_loss=4.823235703706741, G_loss=972.0354927682877\n",
            "Step=299 D_loss=2.394378743171692, G_loss=884.8510018214583\n",
            "Step=399 D_loss=2.005262258052826, G_loss=773.7338873827457\n",
            "Step=499 D_loss=1.5702294325828554, G_loss=709.1177568811179\n",
            "Step=599 D_loss=1.1594652080535888, G_loss=689.5925608286261\n",
            "Step=699 D_loss=0.7877929711341858, G_loss=661.0052268096804\n",
            "Step=799 D_loss=0.7211698257923125, G_loss=650.790792374909\n",
            "Step=899 D_loss=0.4893750464916229, G_loss=663.5066134673358\n",
            "Step=999 D_loss=0.5363713884353636, G_loss=683.0153897365927\n",
            "Step=1099 D_loss=0.2690894949436189, G_loss=679.9851201674342\n",
            "Step=1199 D_loss=0.3260520005226135, G_loss=678.0498606917263\n",
            "Step=1299 D_loss=0.2621238470077514, G_loss=699.3449530783296\n",
            "Step=1399 D_loss=0.2678714823722841, G_loss=682.6424646419287\n",
            "Step=1499 D_loss=0.15121254682540908, G_loss=696.956688734889\n",
            "Step=1599 D_loss=0.18792173624038688, G_loss=723.1149037784338\n",
            "Step=1699 D_loss=0.097623188495636, G_loss=757.4127888515592\n",
            "Step=1799 D_loss=0.14390256762504583, G_loss=735.9834107753635\n",
            "Step=1899 D_loss=0.14697734951972974, G_loss=760.297486602366\n",
            "Step=1999 D_loss=0.07502165079116829, G_loss=767.6319904288649\n",
            "Step=2099 D_loss=0.07600696086883563, G_loss=725.0871964722871\n",
            "Step=2199 D_loss=0.11817311286926269, G_loss=722.7946602016688\n",
            "Step=2299 D_loss=0.0677603745460511, G_loss=717.4249978345632\n",
            "Step=2399 D_loss=0.12118443131446832, G_loss=691.7012282684445\n",
            "Step=2499 D_loss=0.13256907939910878, G_loss=673.8006085717678\n",
            "Step=2599 D_loss=0.10701594710350038, G_loss=676.2088297140598\n",
            "Step=2699 D_loss=0.047173702716827304, G_loss=686.3694990098477\n",
            "Step=2799 D_loss=0.09108238577842709, G_loss=667.2923990491033\n",
            "Step=2899 D_loss=0.13180714607238775, G_loss=651.6191401460767\n",
            "Step=2999 D_loss=0.05202345252037044, G_loss=585.1303417423368\n",
            "Step=3099 D_loss=0.1384107816219331, G_loss=594.0517138567567\n",
            "Step=3199 D_loss=0.13463665843009953, G_loss=573.408214686811\n",
            "Step=3299 D_loss=0.11584906816482543, G_loss=527.8816125610471\n",
            "Step=3399 D_loss=0.19725380539894122, G_loss=508.3439668381214\n",
            "Step=3499 D_loss=0.10859749674797059, G_loss=472.0866439092159\n",
            "Step=3599 D_loss=0.1671544313430786, G_loss=430.76201215952636\n",
            "Step=3699 D_loss=0.2574426710605622, G_loss=415.41488360762594\n",
            "Step=3799 D_loss=0.48880385398864745, G_loss=380.279152494967\n",
            "Step=3899 D_loss=0.6927109777927398, G_loss=385.3662822690606\n",
            "Step=3999 D_loss=0.8850274729728698, G_loss=346.3308219978214\n",
            "Step=4099 D_loss=0.9642401552200317, G_loss=323.1309340664744\n",
            "Step=4199 D_loss=0.7479719495773316, G_loss=274.8779580026865\n",
            "Step=4299 D_loss=1.0202482688426973, G_loss=230.7633512353897\n",
            "Step=4399 D_loss=0.7474058723449708, G_loss=202.41531512349843\n",
            "Step=4499 D_loss=0.9694397163391113, G_loss=163.71412862479684\n",
            "Step=4599 D_loss=0.6235198473930359, G_loss=146.71389400988815\n",
            "Step=4699 D_loss=0.44648866415023813, G_loss=132.76276130199432\n",
            "Step=4799 D_loss=0.4305442130565642, G_loss=117.48234938383102\n",
            "Step=4899 D_loss=0.6001283395290375, G_loss=100.24636439234018\n",
            "Step=4999 D_loss=0.5287986981868742, G_loss=92.07341179400683\n",
            "Step=5099 D_loss=0.5286005091667174, G_loss=80.07631469517946\n",
            "Step=5199 D_loss=0.42115316510200507, G_loss=73.41110347658396\n",
            "Step=5299 D_loss=0.29895287871360776, G_loss=65.67180239707231\n",
            "Step=5399 D_loss=0.2839674150943756, G_loss=62.85736809045076\n",
            "Step=5499 D_loss=0.2479951035976411, G_loss=59.456213299632076\n",
            "Step=5599 D_loss=0.193868237733841, G_loss=56.04013276696205\n",
            "Step=5699 D_loss=0.17896413326263438, G_loss=53.924531018137934\n",
            "Step=5799 D_loss=0.08617854356765742, G_loss=52.71308687776327\n",
            "Step=5899 D_loss=0.16655039668083194, G_loss=49.46681874930859\n",
            "Step=5999 D_loss=0.0940324151515961, G_loss=46.820494781732556\n",
            "Step=6099 D_loss=0.1309439909458161, G_loss=45.34221539020539\n",
            "Step=6199 D_loss=0.06462140917778014, G_loss=45.417413767874244\n",
            "Step=6299 D_loss=0.10111437916755683, G_loss=44.599119243919844\n",
            "Step=6399 D_loss=0.062404766082763574, G_loss=43.991257545351985\n",
            "Step=6499 D_loss=0.07087684869766231, G_loss=43.085535711050035\n",
            "Step=6599 D_loss=0.0344735157489775, G_loss=41.50346302986145\n",
            "Step=6699 D_loss=0.03172168970108036, G_loss=40.96745768070221\n",
            "Step=6799 D_loss=0.03021258711814867, G_loss=41.035902818739416\n",
            "Step=6899 D_loss=0.03840612173080449, G_loss=41.99581870734692\n",
            "Step=6999 D_loss=0.029005599021911577, G_loss=40.853871484398844\n",
            "Step=7099 D_loss=0.043684006929397556, G_loss=43.30623542636633\n",
            "Step=7199 D_loss=0.03143383741378791, G_loss=41.74386103481054\n",
            "Step=7299 D_loss=0.027016048431396378, G_loss=42.804553261995316\n",
            "Step=7399 D_loss=0.04326787233352669, G_loss=46.52576222628355\n",
            "Step=7499 D_loss=0.023229632377624432, G_loss=44.754980816841126\n",
            "Step=7599 D_loss=0.015270376205444247, G_loss=45.61937651306391\n",
            "Step=7699 D_loss=0.017479885816574114, G_loss=43.04947779178619\n",
            "Step=7799 D_loss=0.01067161083221424, G_loss=41.764116862416266\n",
            "Step=7899 D_loss=0.006972281932830837, G_loss=41.937511722743515\n",
            "Step=7999 D_loss=0.012985860109329339, G_loss=43.26608218163252\n",
            "Step=8099 D_loss=0.028124758005142247, G_loss=44.74328700572252\n",
            "Step=8199 D_loss=0.0166414856910706, G_loss=49.383515779674056\n",
            "Step=8299 D_loss=0.010055571794509888, G_loss=51.91720933407545\n",
            "Step=8399 D_loss=0.0009281861782073353, G_loss=47.493417184650895\n",
            "Step=8499 D_loss=0.004521496295929017, G_loss=45.85324155658484\n",
            "Step=8599 D_loss=0.004829933643341056, G_loss=44.31548895686865\n",
            "Step=8699 D_loss=0.0038468420505524126, G_loss=42.93443937689066\n",
            "Step=8799 D_loss=0.007531316280364964, G_loss=41.33050384670496\n",
            "Step=8899 D_loss=0.005857591629028214, G_loss=43.26003482431173\n",
            "Step=8999 D_loss=0.0031549775600432373, G_loss=44.043631010353565\n",
            "Step=9099 D_loss=0.003846890926361146, G_loss=43.75247846633196\n",
            "Step=9199 D_loss=0.01607775449752813, G_loss=44.88942289680243\n",
            "Step=9299 D_loss=0.013570747375488246, G_loss=46.664570089578625\n",
            "Step=9399 D_loss=0.0038370561599729847, G_loss=46.758994833528995\n",
            "Step=9499 D_loss=0.006801439523696828, G_loss=46.8192294126749\n",
            "Step=9599 D_loss=0.010602716207504193, G_loss=47.2521297737956\n",
            "Step=9699 D_loss=0.009301170110702639, G_loss=46.39478504508734\n",
            "Step=9799 D_loss=0.014024473428726036, G_loss=47.331220595240595\n",
            "Step=9899 D_loss=0.0031042385101318626, G_loss=51.27050706207752\n",
            "Step=9999 D_loss=0.010947915315627998, G_loss=49.08213692754507\n",
            "Step=10099 D_loss=0.0018546140193937966, G_loss=49.725262292325496\n",
            "Step=10199 D_loss=0.0002546358108519442, G_loss=46.609252310693265\n",
            "Step=10299 D_loss=0.015024766921996946, G_loss=46.35052593916655\n",
            "Step=10399 D_loss=0.004245952367782468, G_loss=44.16127638369799\n",
            "Step=10499 D_loss=0.01064187049865728, G_loss=43.24815298378467\n",
            "Step=10599 D_loss=0.002700401544570852, G_loss=47.48083198726177\n",
            "Step=10699 D_loss=0.0016549134254455478, G_loss=43.97771740049124\n",
            "Step=10799 D_loss=0.014164818525314482, G_loss=44.26180844157934\n",
            "Step=10899 D_loss=0.0004578566551209917, G_loss=47.45204474002123\n",
            "Step=10999 D_loss=0.003196210861206028, G_loss=45.754925760328774\n",
            "Step=11099 D_loss=0.004648934602737409, G_loss=45.0300453093648\n",
            "Step=11199 D_loss=0.002035174369812154, G_loss=46.97615761309862\n",
            "Step=11299 D_loss=0.006536514759063694, G_loss=47.60885443001986\n",
            "Step=11399 D_loss=0.003063472509384102, G_loss=42.36073532164097\n",
            "Step=11499 D_loss=0.00322398662567136, G_loss=44.985983563363554\n",
            "Step=11599 D_loss=0.0026061654090880904, G_loss=44.62949540317059\n",
            "Step=11699 D_loss=0.002868509292602628, G_loss=45.60677618205547\n",
            "Step=11799 D_loss=0.003558263778686621, G_loss=49.50543844014406\n",
            "Step=11899 D_loss=0.00212857604026806, G_loss=52.82649761229753\n",
            "Step=11999 D_loss=0.004116851091384932, G_loss=50.44663000226021\n",
            "Step=12099 D_loss=0.0025685966014863393, G_loss=58.0670344042778\n",
            "Step=12199 D_loss=0.006561642885208219, G_loss=53.00163272380829\n",
            "Step=12299 D_loss=0.004712405204772896, G_loss=51.35559044033289\n",
            "Step=12399 D_loss=0.006689550280571055, G_loss=54.08144679814577\n",
            "Step=12499 D_loss=0.005584955215454213, G_loss=56.16943987607956\n",
            "Step=12599 D_loss=0.01381670653820044, G_loss=58.88398895174265\n",
            "Step=12699 D_loss=0.008916932940483124, G_loss=57.058445339500906\n",
            "Step=12799 D_loss=0.01022160589694976, G_loss=56.61638762027025\n",
            "Step=12899 D_loss=0.005255486369132978, G_loss=49.61682926237583\n",
            "Step=12999 D_loss=0.009222534298896745, G_loss=48.875326712131496\n",
            "Step=13099 D_loss=0.005102611780166688, G_loss=48.67842399537563\n",
            "Step=13199 D_loss=0.002169737219810508, G_loss=52.09333392709494\n",
            "Step=13299 D_loss=0.003218753933906582, G_loss=51.221558770239355\n",
            "Step=13399 D_loss=0.001993215680122451, G_loss=51.27590317994356\n",
            "Step=13499 D_loss=0.002336700558662419, G_loss=51.17058130532503\n",
            "Step=13599 D_loss=0.006120736598968501, G_loss=52.093834061622616\n",
            "Step=13699 D_loss=0.0023420190811156427, G_loss=54.492632252275946\n",
            "Step=13799 D_loss=0.0012173545360565363, G_loss=57.577142033278946\n",
            "Step=13899 D_loss=0.0011552786827088157, G_loss=54.968311582505706\n",
            "Step=13999 D_loss=0.007292708158493033, G_loss=52.51427053898573\n",
            "Step=14099 D_loss=0.005888320207595776, G_loss=52.26409152120351\n",
            "Step=14199 D_loss=0.012318046092987034, G_loss=51.5377168622613\n",
            "Step=14299 D_loss=0.00788758456707006, G_loss=49.3435191577673\n",
            "Step=14399 D_loss=0.006694941520690834, G_loss=49.92409689158201\n",
            "Step=14499 D_loss=0.008082182407379124, G_loss=51.7006891900301\n",
            "Step=14599 D_loss=0.0036041188240051314, G_loss=55.19512050718069\n",
            "Step=14699 D_loss=0.011922987699508614, G_loss=52.56683061391115\n",
            "Step=14799 D_loss=0.010802626013755812, G_loss=52.87564666241407\n",
            "Step=14899 D_loss=0.00622356772422783, G_loss=53.10886299729347\n",
            "Step=14999 D_loss=0.005469939112663247, G_loss=50.610016568899155\n",
            "Step=15099 D_loss=0.01295071065425879, G_loss=52.26266687989235\n",
            "Step=15199 D_loss=0.005423591732978905, G_loss=50.300400310754775\n",
            "Step=15299 D_loss=0.004927147626876849, G_loss=54.99263513684273\n",
            "Step=15399 D_loss=0.0038481414318085605, G_loss=52.710344969928265\n",
            "Step=15499 D_loss=0.015698424577712977, G_loss=50.58148018091917\n",
            "Step=15599 D_loss=0.005936629176139907, G_loss=50.28980235695839\n",
            "Step=15699 D_loss=0.007749235033988944, G_loss=51.07309981524944\n",
            "Step=15799 D_loss=0.006170409917831465, G_loss=51.36927479922771\n",
            "Step=15899 D_loss=0.0027418875694275036, G_loss=52.35748892337084\n",
            "Step=15999 D_loss=0.008136807084083597, G_loss=53.06423255503178\n",
            "Step=16099 D_loss=0.010835969448089622, G_loss=52.40152679145336\n",
            "Step=16199 D_loss=0.0224424862861633, G_loss=51.913821521699425\n",
            "Step=16299 D_loss=0.01064541101455685, G_loss=58.095612840652464\n",
            "Step=16399 D_loss=0.014709033370018054, G_loss=54.64870727658272\n",
            "Step=16499 D_loss=0.01282705008983609, G_loss=51.20886054456234\n",
            "Step=16599 D_loss=0.02707134544849399, G_loss=48.65048422962427\n",
            "Step=16699 D_loss=0.0029213637113570945, G_loss=52.4882365000248\n",
            "Step=16799 D_loss=0.009241418242454613, G_loss=51.59945016175509\n",
            "Step=16899 D_loss=0.002658070325851414, G_loss=49.869310685396194\n",
            "Step=16999 D_loss=0.017738724946975704, G_loss=48.16955376267433\n",
            "Step=17099 D_loss=0.014324507713317836, G_loss=46.35820666134357\n",
            "Step=17199 D_loss=0.006556730270385724, G_loss=49.7272969570756\n",
            "Step=17299 D_loss=0.012224344015121513, G_loss=46.900634154081345\n",
            "Step=17399 D_loss=0.003491813540458688, G_loss=47.44059712260962\n",
            "Step=17499 D_loss=0.007855747342109698, G_loss=46.94674344301224\n",
            "Step=17599 D_loss=0.010688743591308558, G_loss=47.82861690849065\n",
            "Step=17699 D_loss=0.013968054652214068, G_loss=46.422756529152394\n",
            "Step=17799 D_loss=0.009192306399345429, G_loss=47.69786895424127\n",
            "Step=17899 D_loss=0.004993176460266091, G_loss=46.69709849029779\n",
            "Step=17999 D_loss=0.01459020078182216, G_loss=43.70189439415932\n",
            "Step=18099 D_loss=0.00835358500480654, G_loss=44.94802535712719\n",
            "Step=18199 D_loss=0.011427218317985544, G_loss=44.265106943845744\n",
            "Step=18299 D_loss=0.013902209401130627, G_loss=42.004621454477316\n",
            "Step=18399 D_loss=0.014417031407356284, G_loss=44.70593490600586\n",
            "Step=18499 D_loss=0.013426036834716881, G_loss=45.361297726631165\n",
            "Step=18599 D_loss=0.005269072651863049, G_loss=44.11460151165724\n",
            "Step=18699 D_loss=0.0075763809680938765, G_loss=46.44786859720945\n",
            "Step=18799 D_loss=0.006320175528526373, G_loss=45.56538430631161\n",
            "Step=18899 D_loss=0.015108918547630301, G_loss=42.669351065456866\n",
            "Step=18999 D_loss=0.005617043972015456, G_loss=40.31344820737839\n",
            "Step=19099 D_loss=0.0175771081447601, G_loss=40.6741670113802\n",
            "Step=19199 D_loss=0.013898127079009948, G_loss=41.72906393021345\n",
            "Step=19299 D_loss=0.018739547729492156, G_loss=44.09600352287292\n",
            "Step=19399 D_loss=0.013023558855056683, G_loss=42.425597890615464\n",
            "Step=19499 D_loss=0.01241889655590056, G_loss=40.171471596360206\n",
            "Step=19599 D_loss=0.010288068652153037, G_loss=40.309540656805034\n",
            "Step=19699 D_loss=0.008479738235473655, G_loss=42.71853475630284\n",
            "Step=19799 D_loss=0.00949381887912748, G_loss=40.57996098339558\n",
            "Step=19899 D_loss=0.016912739872932403, G_loss=38.72657344847918\n",
            "Step=19999 D_loss=0.006160516738891619, G_loss=39.00614632219076\n",
            "Step=20099 D_loss=0.02151648402214046, G_loss=40.921352430284024\n",
            "Step=20199 D_loss=0.00868207037448887, G_loss=38.919460914433\n",
            "Step=20299 D_loss=0.01689424932003014, G_loss=38.0299452060461\n",
            "Step=20399 D_loss=0.010523684620857177, G_loss=38.74695771306753\n",
            "Step=20499 D_loss=0.011178336143493617, G_loss=37.3201825094223\n",
            "Step=20599 D_loss=0.024267885684967072, G_loss=35.4483464050293\n",
            "Step=20699 D_loss=0.016067641973495528, G_loss=36.2382828310132\n",
            "Step=20799 D_loss=0.014034283757209831, G_loss=35.37023947536945\n",
            "Step=20899 D_loss=0.009953231215476976, G_loss=34.93150750309229\n",
            "Step=20999 D_loss=0.010584540367126527, G_loss=34.845227956175805\n",
            "Step=21099 D_loss=0.012696029543876719, G_loss=33.291042016148566\n",
            "Step=21199 D_loss=0.009647866487503043, G_loss=33.71927164822817\n",
            "Step=21299 D_loss=0.01485879361629483, G_loss=33.821060208082194\n",
            "Step=21399 D_loss=0.011019006371498108, G_loss=33.06857138037682\n",
            "Step=21499 D_loss=0.014059142470359776, G_loss=32.434310011565685\n",
            "Step=21599 D_loss=0.011290816068649345, G_loss=33.24461304754019\n",
            "Step=21699 D_loss=0.001929444074630693, G_loss=32.09508318990469\n",
            "Step=21799 D_loss=0.010860260725021331, G_loss=30.731794810295106\n",
            "Step=21899 D_loss=0.014677484035491961, G_loss=33.03600147306919\n",
            "Step=21999 D_loss=0.010095108151435772, G_loss=29.444365468025207\n",
            "Step=22099 D_loss=0.017008461952209508, G_loss=30.46860453516245\n",
            "Step=22199 D_loss=0.013336355090141305, G_loss=29.598371706306935\n",
            "Step=22299 D_loss=0.022710035443305965, G_loss=29.975353399217127\n",
            "Step=22399 D_loss=0.009416354298591667, G_loss=29.84889733403921\n",
            "Step=22499 D_loss=0.019614713191986044, G_loss=28.72736551076174\n",
            "Step=22599 D_loss=0.024115012884139975, G_loss=29.450427421927454\n",
            "Step=22699 D_loss=0.016300806999206552, G_loss=28.0626079466939\n",
            "Step=22799 D_loss=0.022667514085769613, G_loss=26.86081332445145\n",
            "Step=22899 D_loss=0.009912376999855033, G_loss=27.92025580704212\n",
            "Step=22999 D_loss=0.02564668357372285, G_loss=25.910294038951395\n",
            "Step=23099 D_loss=0.014472009241580985, G_loss=25.102209272980687\n",
            "Step=23199 D_loss=0.017053149938583312, G_loss=27.61786691188812\n",
            "Step=23299 D_loss=0.03378829926252369, G_loss=27.14655752927065\n",
            "Step=23399 D_loss=0.02022643148899078, G_loss=25.2817592138052\n",
            "Step=23499 D_loss=0.023543321192264544, G_loss=23.26524154305458\n",
            "Step=23599 D_loss=0.013332107663154569, G_loss=23.62989760667086\n",
            "Step=23699 D_loss=0.01982987999916075, G_loss=23.972131704986097\n",
            "Step=23799 D_loss=0.021274716258049053, G_loss=23.307669976353647\n",
            "Step=23899 D_loss=0.022745550572872142, G_loss=24.352969474196435\n",
            "Step=23999 D_loss=0.02728372931480405, G_loss=24.41510195374489\n",
            "Step=24099 D_loss=0.013407062590122198, G_loss=23.160842022299768\n",
            "Step=24199 D_loss=0.01424576550722123, G_loss=22.514930620789528\n",
            "Step=24299 D_loss=0.0187085756659508, G_loss=21.522176119387147\n",
            "Step=24399 D_loss=0.022186486423015572, G_loss=22.401161875724792\n",
            "Step=24499 D_loss=0.02698250770568844, G_loss=22.776010739207265\n",
            "Step=24599 D_loss=0.020700456202030204, G_loss=20.62545336008072\n",
            "Step=24699 D_loss=0.012496873140335096, G_loss=20.978166756927966\n",
            "Step=24799 D_loss=0.020514875948429157, G_loss=19.908682297170163\n",
            "Step=24899 D_loss=0.022974866032600405, G_loss=19.973481741845607\n",
            "Step=24999 D_loss=0.030105178654193876, G_loss=18.92622697919607\n",
            "Step=25099 D_loss=0.021485908925533315, G_loss=19.99172345787287\n",
            "Step=25199 D_loss=0.016806313395500205, G_loss=18.10233975708485\n",
            "Step=25299 D_loss=0.013025436103344001, G_loss=18.161052489578722\n",
            "Step=25399 D_loss=0.017629898190498317, G_loss=18.117292785942553\n",
            "Step=25499 D_loss=0.02618974983692174, G_loss=18.257374028265474\n",
            "Step=25599 D_loss=0.01736764371395111, G_loss=17.70118085205555\n",
            "Step=25699 D_loss=0.025898182392120384, G_loss=17.663751231133936\n",
            "Step=25799 D_loss=0.030428976416587872, G_loss=16.508527975678444\n",
            "Step=25899 D_loss=0.015496183931827556, G_loss=17.25269666314125\n",
            "Step=25999 D_loss=0.024042647480964685, G_loss=15.58844048857689\n",
            "Step=26099 D_loss=0.024798889160156268, G_loss=15.981775848269464\n",
            "Step=26199 D_loss=0.01791290193796158, G_loss=15.233582716584205\n",
            "Step=26299 D_loss=0.017886953949928286, G_loss=15.141105235815049\n",
            "Step=26399 D_loss=0.021448859274387333, G_loss=14.932418273091317\n",
            "Step=26499 D_loss=0.0184354278445244, G_loss=14.178917556107043\n",
            "Step=26599 D_loss=0.022692278921604192, G_loss=14.888164961338044\n",
            "Step=26699 D_loss=0.023792854845523848, G_loss=13.936572259366512\n",
            "Step=26799 D_loss=0.020367908477783192, G_loss=13.301604259610176\n",
            "Step=26899 D_loss=0.02468264669179915, G_loss=12.834676834642886\n",
            "Step=26999 D_loss=0.01778836876153944, G_loss=12.655047609508037\n",
            "Step=27099 D_loss=0.0202729558944702, G_loss=12.623079511225223\n",
            "Step=27199 D_loss=0.02301565885543827, G_loss=12.37335668116808\n",
            "Step=27299 D_loss=0.02257389307022095, G_loss=12.210100161731242\n",
            "Step=27399 D_loss=0.02891904830932618, G_loss=12.157853270173073\n",
            "Step=27499 D_loss=0.017822767198085787, G_loss=11.4826840159297\n",
            "Step=27599 D_loss=0.02403671175241473, G_loss=11.288180223107338\n",
            "Step=27699 D_loss=0.020590818226337404, G_loss=11.512916503548622\n",
            "Step=27799 D_loss=0.01924738585948943, G_loss=11.612540243268013\n",
            "Step=27899 D_loss=0.02139943838119507, G_loss=10.902496794760227\n",
            "Step=27999 D_loss=0.018052998781204255, G_loss=10.76961982280016\n",
            "Step=28099 D_loss=0.015954875946044944, G_loss=10.791099511682988\n",
            "Step=28199 D_loss=0.01606894254684449, G_loss=10.762767446637154\n",
            "Step=28299 D_loss=0.019967458546161665, G_loss=10.70295087248087\n",
            "Step=28399 D_loss=0.01972509592771532, G_loss=10.585281345844269\n",
            "Step=28499 D_loss=0.01910542160272599, G_loss=10.77777435719967\n",
            "Step=28599 D_loss=0.021944288909435283, G_loss=10.259574390351773\n",
            "Step=28699 D_loss=0.01789584904909136, G_loss=9.524610497951507\n",
            "Step=28799 D_loss=0.01580542922019962, G_loss=9.452309254705906\n",
            "Step=28899 D_loss=0.024562302827835114, G_loss=9.698191080093384\n",
            "Step=28999 D_loss=0.017056847810745235, G_loss=9.559802200198172\n",
            "Step=29099 D_loss=0.019206219613552122, G_loss=9.520542685985564\n",
            "Step=29199 D_loss=0.016006855666637454, G_loss=9.164740710556508\n",
            "Step=29299 D_loss=0.018121521174907673, G_loss=9.045177892148494\n",
            "Step=29399 D_loss=0.018781926929950732, G_loss=9.063865081965924\n",
            "Step=29499 D_loss=0.015494236052036259, G_loss=9.02638886153698\n",
            "Step=29599 D_loss=0.0162419056892395, G_loss=8.778857448399068\n",
            "Step=29699 D_loss=0.01859196543693542, G_loss=8.670038317739962\n",
            "Step=29799 D_loss=0.017229852378368382, G_loss=8.545880849659444\n",
            "Step=29899 D_loss=0.014207857549190517, G_loss=8.525912019908429\n",
            "Step=29999 D_loss=0.017955082356929775, G_loss=8.318547497093677\n",
            "Step=30099 D_loss=0.01857239902019503, G_loss=8.489872536659242\n",
            "Step=30199 D_loss=0.021102490723133083, G_loss=8.133540408313275\n",
            "Step=30299 D_loss=0.017555801868438692, G_loss=8.48079869568348\n",
            "Step=30399 D_loss=0.026962068080902124, G_loss=7.934101797044278\n",
            "Step=30499 D_loss=0.014815315902233117, G_loss=8.091654882729054\n",
            "Step=30599 D_loss=0.012859534621238733, G_loss=8.352570132911204\n",
            "Step=30699 D_loss=0.010087288618087775, G_loss=8.42231243878603\n",
            "Step=30799 D_loss=0.014494346976280215, G_loss=8.489761835634708\n",
            "Step=30899 D_loss=0.01918187886476519, G_loss=8.099732205867767\n",
            "Step=30999 D_loss=0.012708870172500608, G_loss=8.041726480424405\n",
            "Step=31099 D_loss=0.017268162965774547, G_loss=7.851744610071182\n",
            "Step=31199 D_loss=0.01651866257190704, G_loss=7.79052600055933\n",
            "Step=31299 D_loss=0.013446268141269646, G_loss=8.177651962339878\n",
            "Step=31399 D_loss=0.01820613622665407, G_loss=8.003921644985676\n",
            "Step=31499 D_loss=0.02468419939279559, G_loss=7.422919447124005\n",
            "Step=31599 D_loss=0.017536457479000067, G_loss=7.734863512814044\n",
            "Step=31699 D_loss=0.014196691215038304, G_loss=7.952317290604114\n",
            "Step=31799 D_loss=0.017767244577407804, G_loss=7.806571199297906\n",
            "Step=31899 D_loss=0.018519785702228564, G_loss=7.458819968402386\n",
            "Step=31999 D_loss=0.01594844818115232, G_loss=7.566800961494446\n",
            "Step=32099 D_loss=0.011711512804031377, G_loss=7.93610808044672\n",
            "Step=32199 D_loss=0.01153204977512362, G_loss=8.028629485666752\n",
            "Step=32299 D_loss=0.018993383944034548, G_loss=7.8458690482378\n",
            "Step=32399 D_loss=0.02138011664152145, G_loss=7.444626751542091\n",
            "Step=32499 D_loss=0.01811338990926742, G_loss=7.448327645361424\n",
            "Step=32599 D_loss=0.020318521559238467, G_loss=7.134240956902504\n",
            "Step=32699 D_loss=0.017827560305595402, G_loss=7.451444611549377\n",
            "Step=32799 D_loss=0.014241482615470924, G_loss=7.971628300249577\n",
            "Step=32899 D_loss=0.01782020568847653, G_loss=8.05303160816431\n",
            "Step=32999 D_loss=0.01544561892747881, G_loss=7.468288028240204\n",
            "Step=33099 D_loss=0.017209114730358133, G_loss=7.321218191683292\n",
            "Step=33199 D_loss=0.012879889905452746, G_loss=7.54586040109396\n",
            "Step=33299 D_loss=0.01853170126676562, G_loss=7.355096336007119\n",
            "Step=33399 D_loss=0.015861086249351508, G_loss=7.299506057202816\n",
            "Step=33499 D_loss=0.0198653346300125, G_loss=7.342872915267945\n",
            "Step=33599 D_loss=0.016093715131282826, G_loss=7.640483490824699\n",
            "Step=33699 D_loss=0.015021021664142653, G_loss=7.303717768788338\n",
            "Step=33799 D_loss=0.014978252947330517, G_loss=7.14796971321106\n",
            "Step=33899 D_loss=0.019892561733722697, G_loss=7.096399655938149\n",
            "Step=33999 D_loss=0.01684057250618931, G_loss=7.247713135182858\n",
            "Step=34099 D_loss=0.0164837111532688, G_loss=7.004489979445934\n",
            "Step=34199 D_loss=0.013440596312284475, G_loss=7.2317769581079485\n",
            "Step=34299 D_loss=0.019217854887247116, G_loss=6.996317217350006\n",
            "Step=34399 D_loss=0.015390352904796567, G_loss=7.094254433810711\n",
            "Step=34499 D_loss=0.018181682676076893, G_loss=6.91102107077837\n",
            "Step=34599 D_loss=0.018767535388469675, G_loss=7.031683187782765\n",
            "Step=34699 D_loss=0.01637621313333512, G_loss=6.9763768538832664\n",
            "Step=34799 D_loss=0.01389558941125868, G_loss=7.085117882490158\n",
            "Step=34899 D_loss=0.021114922612905512, G_loss=6.74721248537302\n",
            "Step=34999 D_loss=0.019405475407838824, G_loss=6.790938816964626\n",
            "Step=35099 D_loss=0.023046683818101876, G_loss=6.631665202379226\n",
            "Step=35199 D_loss=0.008924088925123225, G_loss=7.005169762670994\n",
            "Step=35299 D_loss=0.019633799493312842, G_loss=6.848501500189305\n",
            "Step=35399 D_loss=0.020712766498327234, G_loss=6.586146169900894\n",
            "Step=35499 D_loss=0.014437082707881937, G_loss=7.060203197300434\n",
            "Step=35599 D_loss=0.01574164003133774, G_loss=6.985348061919212\n",
            "Step=35699 D_loss=0.01544455736875533, G_loss=6.955950379967689\n",
            "Step=35799 D_loss=0.01852712854743005, G_loss=6.5710544231534005\n",
            "Step=35899 D_loss=0.018473030775785437, G_loss=6.59567348062992\n",
            "Step=35999 D_loss=0.015300169587135315, G_loss=6.840809923112393\n",
            "Step=36099 D_loss=0.01306843191385268, G_loss=6.868812111616134\n",
            "Step=36199 D_loss=0.012226602733135222, G_loss=7.085221127271652\n",
            "Step=36299 D_loss=0.01801265612244607, G_loss=6.929929268062114\n",
            "Step=36399 D_loss=0.018242779672145826, G_loss=6.730360157191753\n",
            "Step=36499 D_loss=0.015824028253555278, G_loss=7.198914824426175\n",
            "Step=36599 D_loss=0.015607806891202919, G_loss=7.189682100713252\n",
            "Step=36699 D_loss=0.018489780277013784, G_loss=7.67400651961565\n",
            "Step=36799 D_loss=0.013989184498786916, G_loss=7.176264736950397\n",
            "Step=36899 D_loss=0.012153803557157505, G_loss=7.374726130366326\n",
            "Step=36999 D_loss=0.013193256556987748, G_loss=7.402559720277786\n",
            "Step=37099 D_loss=0.01622003450989723, G_loss=7.298045434653759\n",
            "Step=37199 D_loss=0.019296870529651622, G_loss=7.192651298046112\n",
            "Step=37299 D_loss=0.012403269410133366, G_loss=6.964253259599209\n",
            "Step=37399 D_loss=0.01241863280534744, G_loss=7.164413877427578\n",
            "Step=37499 D_loss=0.013475209623575207, G_loss=7.128646585047245\n",
            "Step=37599 D_loss=0.008757234811782838, G_loss=7.355036916136742\n",
            "Step=37699 D_loss=0.020802604407072073, G_loss=7.143156455159188\n",
            "Step=37799 D_loss=0.009112170189619073, G_loss=6.812693393230439\n",
            "Step=37899 D_loss=0.014167789518833157, G_loss=7.482041392624378\n",
            "Step=37999 D_loss=0.015076686441898357, G_loss=6.8206987783312805\n",
            "Step=38099 D_loss=0.012206717282533641, G_loss=6.894082531034947\n",
            "Step=38199 D_loss=0.015079036355018616, G_loss=6.759418312907219\n",
            "Step=38299 D_loss=0.01571990504860879, G_loss=6.709038005471229\n",
            "Step=38399 D_loss=0.01789464935660362, G_loss=6.638393425345421\n",
            "Step=38499 D_loss=0.01421296834945679, G_loss=6.675013872385025\n",
            "Step=38599 D_loss=0.017540667504072183, G_loss=6.774478488862514\n",
            "Step=38699 D_loss=0.013636101782321941, G_loss=7.81746939778328\n",
            "Step=38799 D_loss=0.014673869609832774, G_loss=7.855416021943092\n",
            "Step=38899 D_loss=0.015521377474069592, G_loss=6.905665534436702\n",
            "Step=38999 D_loss=0.014732108712196351, G_loss=6.996924573183059\n",
            "Step=39099 D_loss=0.01318251132965087, G_loss=6.913927406668663\n",
            "Step=39199 D_loss=0.015195945501327524, G_loss=6.761004414558411\n",
            "Step=39299 D_loss=0.014979118257760993, G_loss=6.827261364459991\n",
            "Step=39399 D_loss=0.015235769599676152, G_loss=6.561978781819344\n",
            "Step=39499 D_loss=0.014101356118917469, G_loss=6.897035359144211\n",
            "Step=39599 D_loss=0.01616902768611908, G_loss=6.648035628497601\n",
            "Step=39699 D_loss=0.016695754528045642, G_loss=6.8411151841282845\n",
            "Step=39799 D_loss=0.020385178476572036, G_loss=6.373487092554569\n",
            "Step=39899 D_loss=0.015306692570447922, G_loss=6.995688470900059\n",
            "Step=39999 D_loss=0.015475145131349566, G_loss=6.8363670575618745\n",
            "Step=40099 D_loss=0.016100181788206075, G_loss=6.535125603377819\n",
            "Step=40199 D_loss=0.015042922347784038, G_loss=6.730128130316734\n",
            "Step=40299 D_loss=0.01776344493031501, G_loss=6.680754927694797\n",
            "Step=40399 D_loss=0.013764792233705525, G_loss=6.918525226414204\n",
            "Step=40499 D_loss=0.016999935358762736, G_loss=6.415306494832039\n",
            "Step=40599 D_loss=0.01996819704771044, G_loss=6.47637653708458\n",
            "Step=40699 D_loss=0.014117068946361527, G_loss=6.997986593842507\n",
            "Step=40799 D_loss=0.016825812608003615, G_loss=6.581196269094944\n",
            "Step=40899 D_loss=0.015498351752758044, G_loss=6.707919962108135\n",
            "Step=40999 D_loss=0.015416970998048785, G_loss=6.614316253662109\n",
            "Step=41099 D_loss=0.01791870072484017, G_loss=6.52671361118555\n",
            "Step=41199 D_loss=0.01392104595899582, G_loss=6.789419866204262\n",
            "Step=41299 D_loss=0.019496459215879447, G_loss=6.687753754258155\n",
            "Step=41399 D_loss=0.016303440034389494, G_loss=6.670328883528709\n",
            "Step=41499 D_loss=0.014757731854915634, G_loss=6.80891720622778\n",
            "Step=41599 D_loss=0.017404401153326032, G_loss=6.471890122890472\n",
            "Step=41699 D_loss=0.013346956893801679, G_loss=6.719805873334408\n",
            "Step=41799 D_loss=0.010010604634881029, G_loss=7.155765672624111\n",
            "Step=41899 D_loss=0.01779974550008774, G_loss=6.911342148780823\n",
            "Step=41999 D_loss=0.015277862548828139, G_loss=6.611052876710891\n",
            "Step=42099 D_loss=0.012812974974513058, G_loss=7.41243761986494\n",
            "Step=42199 D_loss=0.01691712282598018, G_loss=7.313137221336365\n",
            "Step=42299 D_loss=0.019055049493908877, G_loss=6.5657213655114175\n",
            "Step=42399 D_loss=0.013808443322777753, G_loss=6.6221541774272925\n",
            "Step=42499 D_loss=0.016803696602582935, G_loss=6.721048874855041\n",
            "Step=42599 D_loss=0.01779464662075042, G_loss=6.612423675060272\n",
            "Step=42699 D_loss=0.018287438377737997, G_loss=6.702301911711693\n",
            "Step=42799 D_loss=0.012979080379009245, G_loss=7.815688073933125\n",
            "Step=42899 D_loss=0.01373605847358704, G_loss=7.2619582322239875\n",
            "Step=42999 D_loss=0.012775341048836708, G_loss=6.954199881851673\n",
            "Step=43099 D_loss=0.01941371679306031, G_loss=6.758067867159843\n",
            "Step=43199 D_loss=0.02066705137491226, G_loss=6.216271668374539\n",
            "Step=43299 D_loss=0.01387583591043949, G_loss=6.676931942105293\n",
            "Step=43399 D_loss=0.0140789556503296, G_loss=6.618212114870548\n",
            "Step=43499 D_loss=0.016527695059776304, G_loss=6.46828797608614\n",
            "Step=43599 D_loss=0.01605642750859261, G_loss=6.403488684296608\n",
            "Step=43699 D_loss=0.021218299865722656, G_loss=6.301058422029018\n",
            "Step=43799 D_loss=0.02102904476225377, G_loss=6.184504772126674\n",
            "Step=43899 D_loss=0.017463591843843468, G_loss=6.259344207942486\n",
            "Step=43999 D_loss=0.016775769665837284, G_loss=6.436595912873744\n",
            "Step=44099 D_loss=0.012628315016627312, G_loss=6.908899486064911\n",
            "Step=44199 D_loss=0.016089773476123823, G_loss=6.55097588211298\n",
            "Step=44299 D_loss=0.015320808663964278, G_loss=6.429300639331341\n",
            "Step=44399 D_loss=0.017356608957052225, G_loss=6.53065102159977\n",
            "Step=44499 D_loss=0.01097386211156845, G_loss=6.612146449685096\n",
            "Step=44599 D_loss=0.01314336076378822, G_loss=6.648974351584911\n",
            "Step=44699 D_loss=0.013190065175294877, G_loss=6.657571801841259\n",
            "Step=44799 D_loss=0.015615356639027592, G_loss=6.540639698505402\n",
            "Step=44899 D_loss=0.015506910160183907, G_loss=6.571210145652294\n",
            "Step=44999 D_loss=0.015021480321884162, G_loss=6.5293222069740295\n",
            "Step=45099 D_loss=0.012940204069018371, G_loss=6.59960408627987\n",
            "Step=45199 D_loss=0.0131970027089119, G_loss=6.621117533445358\n",
            "Step=45299 D_loss=0.011491174995899203, G_loss=6.648309105038643\n",
            "Step=45399 D_loss=0.016180419102311133, G_loss=6.687531795799732\n",
            "Step=45499 D_loss=0.014249400720000277, G_loss=6.474889174997807\n",
            "Step=45599 D_loss=0.01585312433540821, G_loss=6.691924800276756\n",
            "Step=45699 D_loss=0.015296144038438791, G_loss=6.686019939780236\n",
            "Step=45799 D_loss=0.018518837913870823, G_loss=6.602697435617447\n",
            "Step=45899 D_loss=0.01806157775223255, G_loss=6.560515326261521\n",
            "Step=45999 D_loss=0.019695935845375073, G_loss=6.42187928557396\n",
            "Step=46099 D_loss=0.011180595532059662, G_loss=6.528851596713066\n",
            "Step=46199 D_loss=0.018676316738128654, G_loss=6.572536956965924\n",
            "Step=46299 D_loss=0.013239476159214975, G_loss=6.673541324138641\n",
            "Step=46399 D_loss=0.01501298576593399, G_loss=6.513258330523968\n",
            "Step=46499 D_loss=0.015606720820069322, G_loss=6.561524615287781\n",
            "Step=46599 D_loss=0.012212147563695916, G_loss=6.480910658538341\n",
            "Step=46699 D_loss=0.019785438999533653, G_loss=6.56627346277237\n",
            "Step=46799 D_loss=0.02127428501844407, G_loss=6.072236533761025\n",
            "Step=46899 D_loss=0.014785049930214886, G_loss=6.251399999856949\n",
            "Step=46999 D_loss=0.015616654679179193, G_loss=6.430800629556179\n",
            "Step=47099 D_loss=0.011176684275269508, G_loss=6.717342050075531\n",
            "Step=47199 D_loss=0.014670388400554651, G_loss=6.575935777127743\n",
            "Step=47299 D_loss=0.021758331134915362, G_loss=6.590790480971336\n",
            "Step=47399 D_loss=0.01660547085106373, G_loss=6.476042196750641\n",
            "Step=47499 D_loss=0.013388369828462601, G_loss=6.692047522068024\n",
            "Step=47599 D_loss=0.01734307207167149, G_loss=6.397187061905861\n",
            "Step=47699 D_loss=0.019957075491547585, G_loss=6.328938314914703\n",
            "Step=47799 D_loss=0.017670317813754083, G_loss=6.268137301206589\n",
            "Step=47899 D_loss=0.013145603612065307, G_loss=6.479993821680545\n",
            "Step=47999 D_loss=0.013239344507455827, G_loss=6.634086678922176\n",
            "Step=48099 D_loss=0.011080157235264773, G_loss=6.634706640541554\n",
            "Step=48199 D_loss=0.015244737043976775, G_loss=6.532838268280029\n",
            "Step=48299 D_loss=0.016301703900098802, G_loss=6.40211240708828\n",
            "Step=48399 D_loss=0.014922403991222383, G_loss=6.409661503136158\n",
            "Step=48499 D_loss=0.013407099619507784, G_loss=6.612181637585163\n",
            "Step=48599 D_loss=0.017018423229455945, G_loss=6.440338751971721\n",
            "Step=48699 D_loss=0.015666097775101667, G_loss=6.318803341984749\n",
            "Step=48799 D_loss=0.01478671863675117, G_loss=6.473092263042926\n",
            "Step=48899 D_loss=0.01477685913443566, G_loss=6.429040524959564\n",
            "Step=48999 D_loss=0.009687610194087037, G_loss=6.695107667446136\n",
            "Step=49099 D_loss=0.017392288595437996, G_loss=6.698039769232274\n",
            "Step=49199 D_loss=0.011863410957157608, G_loss=6.716769665777683\n",
            "Step=49299 D_loss=0.01235776156187058, G_loss=6.835939016640187\n",
            "Step=49399 D_loss=0.012280179038643833, G_loss=6.462606040239334\n",
            "Step=49499 D_loss=0.017044347561895844, G_loss=6.530092596411705\n",
            "Step=49599 D_loss=0.012892788685858247, G_loss=6.453404085934162\n",
            "Step=49699 D_loss=0.010710851885378361, G_loss=6.689797120392322\n",
            "Step=49799 D_loss=0.014977850504219542, G_loss=6.575139033794403\n",
            "Step=49899 D_loss=0.01286442905664445, G_loss=6.716567910611629\n",
            "Step=49999 D_loss=0.010989274866878979, G_loss=6.768978539109231\n",
            "Reached 50001 epochs for GAN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNGUJwBhj0pQlAGOMSVOWAIwxJk39P9i8sest1Lh9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3K2Iqevjl1i"
      },
      "source": [
        "Above code is pretty self explantory. Steps are: \n",
        "\n",
        "1. create batches\n",
        "2. tcreate an object with the given historical data and batches\n",
        "3. Train this object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFu0U0AlRMP1"
      },
      "source": [
        "#modeling CNN\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "tf.set_random_seed(SEED)\n",
        "class CNN():\n",
        "\n",
        "    def __init__(self, num_features, num_historical_days, is_train=True):\n",
        "      \n",
        "        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n",
        "        self.Y = tf.placeholder(tf.int32, shape=[None, 2])\n",
        "        self.keep_prob = tf.placeholder(tf.float32, shape=[])\n",
        "\n",
        "        with tf.variable_scope(\"cnn\"):\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 16],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b1 = tf.Variable(tf.zeros([16], dtype=tf.float32))\n",
        "\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 16, 32],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b2 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "            conv = tf.nn.conv2d(relu, k2,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b3 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
        "            conv = tf.nn.conv2d(relu, k3, strides=[1, 1, 1, 1], padding='VALID')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob=self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n",
        "            print(flattened_convolution_size)\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n",
        "\n",
        "            if is_train:\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=self.keep_prob)\n",
        "\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*64, 32]))\n",
        "            b4 = tf.Variable(tf.truncated_normal([32]))\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n",
        "\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([32, 2]))\n",
        "            logits = tf.matmul(h1, W2)\n",
        "\n",
        "            #self.accuracy = tf.metrics.accuracy(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.Y, 1), tf.argmax(logits, 1)), tf.float32))\n",
        "            self.confusion_matrix = tf.confusion_matrix(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\n",
        "            tf.summary.scalar('accuracy', self.accuracy)\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]           \n",
        "            \n",
        "            # D_prob = tf.nn.sigmoid(D_logit)\n",
        "\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=logits))\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        # self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n",
        "        # self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n",
        "        # self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n",
        "        # self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.loss)\n",
        "        self.summary = tf.summary.merge_all()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2qVIuzFkE5J"
      },
      "source": [
        "Make the discriminator. \n",
        "Steps: \n",
        "1. Create a flattened convolution object \n",
        "\n",
        "2. Declare the weights and biases.\n",
        "\n",
        "3. make logits and then calculate accuracy and summary\n",
        "\n",
        "4. Use this as a discriminator model against generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSW0oCUNTZ6Z",
        "outputId": "ec406265-f3ae-4957-99c3-19ef161f4483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training CNN\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "#import xgboost as xgb\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "class TrainCNN:\n",
        "\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.test_data = []\n",
        "        self.test_labels = []\n",
        "        self.cnn = CNN(num_features=5, num_historical_days=num_historical_days, is_train=False)\n",
        "#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n",
        "\n",
        "        # Google Drive Method\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n",
        "#         print(files)\n",
        "    \n",
        "    \n",
        "        for file in files:\n",
        "            print(file)\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            df = df[['open','high','low','close','volume']]\n",
        "            # data for new column labels that will use the pct_change of the closing data.\n",
        "            # pct_change measure change between current and prior element. Map these into a 1x2\n",
        "            # array to show if the pct_change > (our desired threshold) or less than.\n",
        "            labels = df.close.pct_change(days).map(lambda x: [int(x > pct_change/100.0), int(x <= pct_change/100.0)])\n",
        "            \n",
        "            # rolling normalization. (df - df.mean) / (df.max - df.min)\n",
        "            df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "            df['labels'] = labels\n",
        "\n",
        "            # doing pct_change will give some rows (like first row) a NaN value. Drop that.\n",
        "            df = df.dropna()\n",
        "\n",
        "            # Do the testing data split\n",
        "            test_df = df[:365]\n",
        "            df = df[400:]\n",
        "\n",
        "            # get the predictors of the dataframe\n",
        "            data = df[['open','high','low','close','volume']].values\n",
        "\n",
        "            # the response value\n",
        "            labels = df['labels'].values\n",
        "\n",
        "            # start at num_historical_days and iterate the full length of the training\n",
        "            # data at intervals of num_historical_days\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                # split the df into arrays of length num_historical_days and append\n",
        "                # to data, i.e. array of df[curr - num_days : curr] -> a batch of values\n",
        "                self.data.append(data[i-num_historical_days:i])\n",
        "\n",
        "                # appending if price went up or down in curr day of \"i\" we are looking\n",
        "                # at\n",
        "                self.labels.append(labels[i-1])\n",
        "            \n",
        "            # do same for test data\n",
        "            data = test_df[['open','high','low','close','volume']].values\n",
        "            labels = test_df['labels'].values\n",
        "            for i in range(num_historical_days, len(test_df), 1):\n",
        "                self.test_data.append(data[i-num_historical_days:i])\n",
        "                self.test_labels.append(labels[i-1])\n",
        "\n",
        "    # a function to get a random_batch of data.\n",
        "    def random_batch(self, batch_size=128):\n",
        "        batch = []\n",
        "        labels = []\n",
        "        # zip concatenates each array index of both arrays together\n",
        "        data = list(zip(self.data, self.labels))\n",
        "        i = 0\n",
        "        while True:\n",
        "            i+= 1\n",
        "            while True:\n",
        "                # pick a random array, i.e. range of days, from data\n",
        "                d = random.choice(data)\n",
        "                # balance the data with equal number of positive pct_change\n",
        "                # and negative pct_change\n",
        "                if(d[1][0]== int(i%2)):\n",
        "                    break\n",
        "            batch.append(d[0])  # append the range of days we got to batch\n",
        "            labels.append(d[1])  # append the label of that range of data we got\n",
        "            if (len(batch) == batch_size):\n",
        "                yield batch, labels\n",
        "                batch = []\n",
        "                labels = []\n",
        "\n",
        "    def train(self, print_steps=100, display_steps=100, save_steps=SAVE_STEPS_AMOUNT, batch_size=128, keep_prob=0.6):\n",
        "        if not os.path.exists(f'{googlepath}cnn_models'):\n",
        "            os.makedirs(f'{googlepath}cnn_models')\n",
        "        if not os.path.exists(f'{googlepath}logs'):\n",
        "            os.makedirs(f'{googlepath}logs')\n",
        "        if os.path.exists(f'{googlepath}logs/train'):\n",
        "            for file in [os.path.join(f'{googlepath}logs/train/', f) for f in os.listdir(f'{googlepath}logs/train/')]:\n",
        "                os.remove(file)\n",
        "        if os.path.exists(f'{googlepath}logs/test'):\n",
        "            for file in [os.path.join(f'{googlepath}logs/test/', f) for f in os.listdir(f'{googlepath}logs/test')]:\n",
        "                os.remove(file)\n",
        "\n",
        "        sess = tf.Session()\n",
        "        loss = 0\n",
        "        l2_loss = 0\n",
        "        accuracy = 0\n",
        "        saver = tf.train.Saver()\n",
        "        train_writer = tf.summary.FileWriter(f'{googlepath}/logs/train')\n",
        "        test_writer = tf.summary.FileWriter(f'{googlepath}/logs/test')\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        test_loss_array = []\n",
        "        test_accuracy_array = []\n",
        "        currentStep = \"0\"\n",
        "        \n",
        "        if os.path.exists(f'{googlepath}cnn_models/checkpoint'):\n",
        "                with open(f'{googlepath}cnn_models/checkpoint', 'rb') as f:\n",
        "                    model_name = next(f).split('\"'.encode())[1]\n",
        "                filename = \"{}cnn_models/{}\".format(googlepath, model_name.decode())\n",
        "                currentStep = filename.split(\"-\")[1]\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\n",
        "\n",
        "        for i, [X, y] in enumerate(self.random_batch(batch_size)):\n",
        "\n",
        "          \n",
        "            _, loss_curr, accuracy_curr = sess.run([self.cnn.optimizer, self.cnn.loss, self.cnn.accuracy], feed_dict=\n",
        "                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\n",
        "            loss += loss_curr\n",
        "            accuracy += accuracy_curr\n",
        "            if (i+1) % print_steps == 0:\n",
        "                print('Step={} loss={}, accuracy={}'.format(i + int(currentStep), loss/print_steps, accuracy/print_steps))\n",
        "                loss = 0\n",
        "                l2_loss = 0\n",
        "                accuracy = 0\n",
        "                test_loss, test_accuracy, confusion_matrix = sess.run([self.cnn.loss, self.cnn.accuracy, self.cnn.confusion_matrix], feed_dict={self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\n",
        "                test_loss_array.append(test_loss)\n",
        "                test_accuracy_array.append(test_accuracy)\n",
        "                print(\"Test loss = {}, Test accuracy = {}\".format(test_loss, test_accuracy))\n",
        "            if (i+1) % save_steps == 0:\n",
        "                saver.save(sess,  f'{googlepath}cnn_models/cnn.ckpt', i)\n",
        "\n",
        "            if (i+1) % display_steps == 0:\n",
        "                summary = sess.run(self.cnn.summary, feed_dict=\n",
        "                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\n",
        "                train_writer.add_summary(summary, i)\n",
        "                summary = sess.run(self.cnn.summary, feed_dict={\n",
        "                    self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\n",
        "                test_writer.add_summary(summary, i)\n",
        "            \n",
        "            # end training at training_amount epochs\n",
        "            if (i + int(currentStep)) > TRAINING_AMOUNT:\n",
        "                print(\"Reached {} epochs for CNN\".format(i + int(currentStep)))\n",
        "                sess.close()\n",
        "                print(confusion_matrix)\n",
        "                plot_confusion_matrix(confusion_matrix, ['Down', 'Up'], normalize=True, title=\"CNN Confusion Matrix\")\n",
        "                \n",
        "                axisA = np.arange(0,len(test_loss_array),1)\n",
        "                axisB = np.arange(0,len(test_accuracy_array),1)\n",
        "                plt.plot(axisA, test_loss_array, label='test accuracy')\n",
        "                plt.plot(axisB, test_accuracy_array, label='test loss')\n",
        "                plt.legend()\n",
        "                plt.title('test loss and accuracy')\n",
        "                plt.show()\n",
        "\n",
        "                break\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "tf.reset_default_graph()\n",
        "cnn = TrainCNN(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\n",
        "cnn.train()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"cnn/Relu:0\", shape=(?, 20, 1, 16), dtype=float32)\n",
            "Tensor(\"cnn/Relu_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"cnn/Relu_2:0\", shape=(?, 18, 1, 64), dtype=float32)\n",
            "1152\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n",
            "Step=99 loss=0.6455005130171776, accuracy=0.702890625\n",
            "Test loss = 0.5633534789085388, Test accuracy = 0.7318840622901917\n",
            "Step=199 loss=0.38534841775894163, accuracy=0.841796875\n",
            "Test loss = 0.4994240701198578, Test accuracy = 0.7739130258560181\n",
            "Step=299 loss=0.28029976472258566, accuracy=0.919453125\n",
            "Test loss = 0.4840500056743622, Test accuracy = 0.782608687877655\n",
            "Step=399 loss=0.2181575807929039, accuracy=0.942734375\n",
            "Test loss = 0.4898371696472168, Test accuracy = 0.7782608866691589\n",
            "Step=499 loss=0.16817410349845885, accuracy=0.95875\n",
            "Test loss = 0.5119732618331909, Test accuracy = 0.7884057760238647\n",
            "Step=599 loss=0.1318838482350111, accuracy=0.97625\n",
            "Test loss = 0.5282639265060425, Test accuracy = 0.7768115997314453\n",
            "Step=699 loss=0.10476633816957474, accuracy=0.984375\n",
            "Test loss = 0.5554772615432739, Test accuracy = 0.7681159377098083\n",
            "Step=799 loss=0.08198465671390295, accuracy=0.991171875\n",
            "Test loss = 0.5908352732658386, Test accuracy = 0.7681159377098083\n",
            "Step=899 loss=0.06370757568627596, accuracy=0.993984375\n",
            "Test loss = 0.6126183271408081, Test accuracy = 0.7666666507720947\n",
            "Step=999 loss=0.053149219676852225, accuracy=0.9946875\n",
            "Test loss = 0.6560640931129456, Test accuracy = 0.7652173638343811\n",
            "Step=1099 loss=0.04316837003454566, accuracy=0.996953125\n",
            "Test loss = 0.7098760008811951, Test accuracy = 0.7637681365013123\n",
            "Step=1199 loss=0.03471717918291688, accuracy=0.998203125\n",
            "Test loss = 0.7251536846160889, Test accuracy = 0.760869562625885\n",
            "Step=1299 loss=0.029514714051038025, accuracy=0.99890625\n",
            "Test loss = 0.7427104711532593, Test accuracy = 0.7652173638343811\n",
            "Step=1399 loss=0.02395401757210493, accuracy=1.0\n",
            "Test loss = 0.7950646281242371, Test accuracy = 0.7637681365013123\n",
            "Step=1499 loss=0.020126780355349184, accuracy=1.0\n",
            "Test loss = 0.8054652214050293, Test accuracy = 0.7579709887504578\n",
            "Step=1599 loss=0.01689181079156697, accuracy=1.0\n",
            "Test loss = 0.8656027317047119, Test accuracy = 0.7623188495635986\n",
            "Step=1699 loss=0.014489036658778787, accuracy=1.0\n",
            "Test loss = 0.8723316192626953, Test accuracy = 0.7594202756881714\n",
            "Step=1799 loss=0.012030724142678082, accuracy=1.0\n",
            "Test loss = 0.9114437103271484, Test accuracy = 0.7565217614173889\n",
            "Step=1899 loss=0.010295248357579112, accuracy=1.0\n",
            "Test loss = 0.934668242931366, Test accuracy = 0.7579709887504578\n",
            "Step=1999 loss=0.009224811932072044, accuracy=1.0\n",
            "Test loss = 0.995672881603241, Test accuracy = 0.760869562625885\n",
            "Step=2099 loss=0.007623512297868729, accuracy=1.0\n",
            "Test loss = 0.9776466488838196, Test accuracy = 0.7579709887504578\n",
            "Step=2199 loss=0.00687622198369354, accuracy=1.0\n",
            "Test loss = 1.0171468257904053, Test accuracy = 0.7594202756881714\n",
            "Step=2299 loss=0.006238719755783677, accuracy=1.0\n",
            "Test loss = 1.0402721166610718, Test accuracy = 0.7565217614173889\n",
            "Step=2399 loss=0.00533247207524255, accuracy=1.0\n",
            "Test loss = 1.0630614757537842, Test accuracy = 0.7565217614173889\n",
            "Step=2499 loss=0.004672179182525724, accuracy=1.0\n",
            "Test loss = 1.073714017868042, Test accuracy = 0.7536231875419617\n",
            "Step=2599 loss=0.004176417370326817, accuracy=1.0\n",
            "Test loss = 1.0960719585418701, Test accuracy = 0.7550724744796753\n",
            "Step=2699 loss=0.0037637158948928117, accuracy=1.0\n",
            "Test loss = 1.1246168613433838, Test accuracy = 0.7536231875419617\n",
            "Step=2799 loss=0.0033668747358024122, accuracy=1.0\n",
            "Test loss = 1.1570963859558105, Test accuracy = 0.7550724744796753\n",
            "Step=2899 loss=0.003052552400622517, accuracy=1.0\n",
            "Test loss = 1.178293228149414, Test accuracy = 0.7536231875419617\n",
            "Step=2999 loss=0.0027770366007462146, accuracy=1.0\n",
            "Test loss = 1.1862610578536987, Test accuracy = 0.752173900604248\n",
            "Step=3099 loss=0.0023982084076851606, accuracy=1.0\n",
            "Test loss = 1.1974427700042725, Test accuracy = 0.752173900604248\n",
            "Step=3199 loss=0.00230134088662453, accuracy=1.0\n",
            "Test loss = 1.2318204641342163, Test accuracy = 0.752173900604248\n",
            "Step=3299 loss=0.002009047253523022, accuracy=1.0\n",
            "Test loss = 1.2403141260147095, Test accuracy = 0.7507246136665344\n",
            "Step=3399 loss=0.0018324128526728601, accuracy=1.0\n",
            "Test loss = 1.2771998643875122, Test accuracy = 0.7507246136665344\n",
            "Step=3499 loss=0.0016740203532390296, accuracy=1.0\n",
            "Test loss = 1.288344383239746, Test accuracy = 0.7492753863334656\n",
            "Step=3599 loss=0.001488404672127217, accuracy=1.0\n",
            "Test loss = 1.326475739479065, Test accuracy = 0.7507246136665344\n",
            "Step=3699 loss=0.0013668786047492177, accuracy=1.0\n",
            "Test loss = 1.3329163789749146, Test accuracy = 0.752173900604248\n",
            "Step=3799 loss=0.0012860607472248375, accuracy=1.0\n",
            "Test loss = 1.3427609205245972, Test accuracy = 0.7507246136665344\n",
            "Step=3899 loss=0.001173119944287464, accuracy=1.0\n",
            "Test loss = 1.3774237632751465, Test accuracy = 0.752173900604248\n",
            "Step=3999 loss=0.0010717943822965027, accuracy=1.0\n",
            "Test loss = 1.3966646194458008, Test accuracy = 0.7536231875419617\n",
            "Step=4099 loss=0.0009482053417013958, accuracy=1.0\n",
            "Test loss = 1.4059785604476929, Test accuracy = 0.752173900604248\n",
            "Step=4199 loss=0.0009044862870359793, accuracy=1.0\n",
            "Test loss = 1.4172710180282593, Test accuracy = 0.7492753863334656\n",
            "Step=4299 loss=0.0008495339599903673, accuracy=1.0\n",
            "Test loss = 1.4441145658493042, Test accuracy = 0.747826099395752\n",
            "Step=4399 loss=0.0007612658990547061, accuracy=1.0\n",
            "Test loss = 1.454982042312622, Test accuracy = 0.747826099395752\n",
            "Step=4499 loss=0.0006944194735842757, accuracy=1.0\n",
            "Test loss = 1.4686086177825928, Test accuracy = 0.747826099395752\n",
            "Step=4599 loss=0.0006731017568381504, accuracy=1.0\n",
            "Test loss = 1.5000734329223633, Test accuracy = 0.7463768124580383\n",
            "Step=4699 loss=0.0006002894716220908, accuracy=1.0\n",
            "Test loss = 1.494809865951538, Test accuracy = 0.7463768124580383\n",
            "Step=4799 loss=0.0005739481301861815, accuracy=1.0\n",
            "Test loss = 1.5140224695205688, Test accuracy = 0.7463768124580383\n",
            "Step=4899 loss=0.0005197670310735702, accuracy=1.0\n",
            "Test loss = 1.5450067520141602, Test accuracy = 0.7420290112495422\n",
            "Step=4999 loss=0.00047513975732726976, accuracy=1.0\n",
            "Test loss = 1.5551984310150146, Test accuracy = 0.7449275255203247\n",
            "Step=5099 loss=0.0004432367006666027, accuracy=1.0\n",
            "Test loss = 1.5600734949111938, Test accuracy = 0.7449275255203247\n",
            "Step=5199 loss=0.0004202781443018466, accuracy=1.0\n",
            "Test loss = 1.6014375686645508, Test accuracy = 0.7420290112495422\n",
            "Step=5299 loss=0.00038840946392156185, accuracy=1.0\n",
            "Test loss = 1.5892130136489868, Test accuracy = 0.7434782385826111\n",
            "Step=5399 loss=0.0003692286994191818, accuracy=1.0\n",
            "Test loss = 1.6115411520004272, Test accuracy = 0.7434782385826111\n",
            "Step=5499 loss=0.00034028590220259503, accuracy=1.0\n",
            "Test loss = 1.630059003829956, Test accuracy = 0.7434782385826111\n",
            "Step=5599 loss=0.00031685155321611093, accuracy=1.0\n",
            "Test loss = 1.6463714838027954, Test accuracy = 0.7434782385826111\n",
            "Step=5699 loss=0.00028172115577035584, accuracy=1.0\n",
            "Test loss = 1.6595641374588013, Test accuracy = 0.7420290112495422\n",
            "Step=5799 loss=0.00026824295346159486, accuracy=1.0\n",
            "Test loss = 1.67706298828125, Test accuracy = 0.7420290112495422\n",
            "Step=5899 loss=0.00024462924819090403, accuracy=1.0\n",
            "Test loss = 1.6772830486297607, Test accuracy = 0.7449275255203247\n",
            "Step=5999 loss=0.0002374729182338342, accuracy=1.0\n",
            "Test loss = 1.7260812520980835, Test accuracy = 0.7405797243118286\n",
            "Step=6099 loss=0.00021731208078563214, accuracy=1.0\n",
            "Test loss = 1.7356977462768555, Test accuracy = 0.7405797243118286\n",
            "Step=6199 loss=0.00020920861097692978, accuracy=1.0\n",
            "Test loss = 1.7414748668670654, Test accuracy = 0.7405797243118286\n",
            "Step=6299 loss=0.00019230973368394188, accuracy=1.0\n",
            "Test loss = 1.7532926797866821, Test accuracy = 0.7405797243118286\n",
            "Step=6399 loss=0.0001773837226210162, accuracy=1.0\n",
            "Test loss = 1.7794904708862305, Test accuracy = 0.7420290112495422\n",
            "Step=6499 loss=0.0001720641873544082, accuracy=1.0\n",
            "Test loss = 1.7605462074279785, Test accuracy = 0.7434782385826111\n",
            "Step=6599 loss=0.00015833684003155213, accuracy=1.0\n",
            "Test loss = 1.7965834140777588, Test accuracy = 0.7405797243118286\n",
            "Step=6699 loss=0.00014623065995692742, accuracy=1.0\n",
            "Test loss = 1.8130075931549072, Test accuracy = 0.7420290112495422\n",
            "Step=6799 loss=0.00013419187249382957, accuracy=1.0\n",
            "Test loss = 1.83470618724823, Test accuracy = 0.7434782385826111\n",
            "Step=6899 loss=0.0001251046661491273, accuracy=1.0\n",
            "Test loss = 1.8584685325622559, Test accuracy = 0.7434782385826111\n",
            "Step=6999 loss=0.00011675868423481006, accuracy=1.0\n",
            "Test loss = 1.8458561897277832, Test accuracy = 0.7420290112495422\n",
            "Step=7099 loss=0.00011386659425625112, accuracy=1.0\n",
            "Test loss = 1.8799095153808594, Test accuracy = 0.7420290112495422\n",
            "Step=7199 loss=0.00010577966822893359, accuracy=1.0\n",
            "Test loss = 1.9052237272262573, Test accuracy = 0.7420290112495422\n",
            "Step=7299 loss=9.608548352844082e-05, accuracy=1.0\n",
            "Test loss = 1.8881547451019287, Test accuracy = 0.7405797243118286\n",
            "Step=7399 loss=8.809736391413026e-05, accuracy=1.0\n",
            "Test loss = 1.9171779155731201, Test accuracy = 0.7420290112495422\n",
            "Step=7499 loss=8.759146447118838e-05, accuracy=1.0\n",
            "Test loss = 1.9388947486877441, Test accuracy = 0.7420290112495422\n",
            "Step=7599 loss=7.735299830528674e-05, accuracy=1.0\n",
            "Test loss = 1.959372878074646, Test accuracy = 0.7420290112495422\n",
            "Step=7699 loss=7.647127771633678e-05, accuracy=1.0\n",
            "Test loss = 1.9546908140182495, Test accuracy = 0.7420290112495422\n",
            "Step=7799 loss=7.055563841277035e-05, accuracy=1.0\n",
            "Test loss = 1.97428560256958, Test accuracy = 0.7434782385826111\n",
            "Step=7899 loss=6.496139747468987e-05, accuracy=1.0\n",
            "Test loss = 1.9913990497589111, Test accuracy = 0.7434782385826111\n",
            "Step=7999 loss=6.118571942351991e-05, accuracy=1.0\n",
            "Test loss = 2.0152642726898193, Test accuracy = 0.7434782385826111\n",
            "Step=8099 loss=5.955694086878793e-05, accuracy=1.0\n",
            "Test loss = 2.0249290466308594, Test accuracy = 0.7434782385826111\n",
            "Step=8199 loss=5.3291604235710113e-05, accuracy=1.0\n",
            "Test loss = 2.0377357006073, Test accuracy = 0.7434782385826111\n",
            "Step=8299 loss=4.930827893986134e-05, accuracy=1.0\n",
            "Test loss = 2.0533204078674316, Test accuracy = 0.7434782385826111\n",
            "Step=8399 loss=4.814094954781467e-05, accuracy=1.0\n",
            "Test loss = 2.0663301944732666, Test accuracy = 0.7434782385826111\n",
            "Step=8499 loss=4.345009296230273e-05, accuracy=1.0\n",
            "Test loss = 2.082120180130005, Test accuracy = 0.7434782385826111\n",
            "Step=8599 loss=4.393060225993395e-05, accuracy=1.0\n",
            "Test loss = 2.096813201904297, Test accuracy = 0.7420290112495422\n",
            "Step=8699 loss=3.931965993615449e-05, accuracy=1.0\n",
            "Test loss = 2.089421033859253, Test accuracy = 0.7434782385826111\n",
            "Step=8799 loss=3.7439632342284314e-05, accuracy=1.0\n",
            "Test loss = 2.111463785171509, Test accuracy = 0.7434782385826111\n",
            "Step=8899 loss=3.601251932195737e-05, accuracy=1.0\n",
            "Test loss = 2.1339540481567383, Test accuracy = 0.7420290112495422\n",
            "Step=8999 loss=3.244342122343369e-05, accuracy=1.0\n",
            "Test loss = 2.1589953899383545, Test accuracy = 0.7420290112495422\n",
            "Step=9099 loss=3.0682506494486e-05, accuracy=1.0\n",
            "Test loss = 2.1643097400665283, Test accuracy = 0.7420290112495422\n",
            "Step=9199 loss=2.9076252449158346e-05, accuracy=1.0\n",
            "Test loss = 2.162238597869873, Test accuracy = 0.7420290112495422\n",
            "Step=9299 loss=2.7499958359840094e-05, accuracy=1.0\n",
            "Test loss = 2.1879866123199463, Test accuracy = 0.7420290112495422\n",
            "Step=9399 loss=2.587783517810749e-05, accuracy=1.0\n",
            "Test loss = 2.2140636444091797, Test accuracy = 0.7420290112495422\n",
            "Step=9499 loss=2.4264521925942972e-05, accuracy=1.0\n",
            "Test loss = 2.2203757762908936, Test accuracy = 0.7420290112495422\n",
            "Step=9599 loss=2.2637555366600283e-05, accuracy=1.0\n",
            "Test loss = 2.224438190460205, Test accuracy = 0.7420290112495422\n",
            "Step=9699 loss=2.119722728821216e-05, accuracy=1.0\n",
            "Test loss = 2.241741418838501, Test accuracy = 0.7420290112495422\n",
            "Step=9799 loss=2.0683036891568917e-05, accuracy=1.0\n",
            "Test loss = 2.2460215091705322, Test accuracy = 0.7420290112495422\n",
            "Step=9899 loss=1.9144765938108323e-05, accuracy=1.0\n",
            "Test loss = 2.2567896842956543, Test accuracy = 0.7420290112495422\n",
            "Step=9999 loss=1.7768654533938388e-05, accuracy=1.0\n",
            "Test loss = 2.2692184448242188, Test accuracy = 0.7420290112495422\n",
            "Step=10099 loss=1.6431240046586025e-05, accuracy=1.0\n",
            "Test loss = 2.290102005004883, Test accuracy = 0.7420290112495422\n",
            "Step=10199 loss=1.5787431939315864e-05, accuracy=1.0\n",
            "Test loss = 2.2979471683502197, Test accuracy = 0.7420290112495422\n",
            "Step=10299 loss=1.4959151085349732e-05, accuracy=1.0\n",
            "Test loss = 2.313241958618164, Test accuracy = 0.7420290112495422\n",
            "Step=10399 loss=1.3966142842036788e-05, accuracy=1.0\n",
            "Test loss = 2.328423023223877, Test accuracy = 0.7420290112495422\n",
            "Step=10499 loss=1.3122152859068592e-05, accuracy=1.0\n",
            "Test loss = 2.3296751976013184, Test accuracy = 0.7420290112495422\n",
            "Step=10599 loss=1.2521056514742668e-05, accuracy=1.0\n",
            "Test loss = 2.3493690490722656, Test accuracy = 0.7420290112495422\n",
            "Step=10699 loss=1.192994607208675e-05, accuracy=1.0\n",
            "Test loss = 2.3667445182800293, Test accuracy = 0.7420290112495422\n",
            "Step=10799 loss=1.1303631035843864e-05, accuracy=1.0\n",
            "Test loss = 2.3787999153137207, Test accuracy = 0.7420290112495422\n",
            "Step=10899 loss=1.0344936886212963e-05, accuracy=1.0\n",
            "Test loss = 2.3721988201141357, Test accuracy = 0.7420290112495422\n",
            "Step=10999 loss=9.895255216179066e-06, accuracy=1.0\n",
            "Test loss = 2.3966126441955566, Test accuracy = 0.7420290112495422\n",
            "Step=11099 loss=9.304628288191451e-06, accuracy=1.0\n",
            "Test loss = 2.413039207458496, Test accuracy = 0.7420290112495422\n",
            "Step=11199 loss=8.642730613246385e-06, accuracy=1.0\n",
            "Test loss = 2.4201595783233643, Test accuracy = 0.7420290112495422\n",
            "Step=11299 loss=8.270050657301908e-06, accuracy=1.0\n",
            "Test loss = 2.4321718215942383, Test accuracy = 0.7420290112495422\n",
            "Step=11399 loss=7.620891447004396e-06, accuracy=1.0\n",
            "Test loss = 2.464155673980713, Test accuracy = 0.7420290112495422\n",
            "Step=11499 loss=7.406509944303252e-06, accuracy=1.0\n",
            "Test loss = 2.478423595428467, Test accuracy = 0.7420290112495422\n",
            "Step=11599 loss=6.900997068441938e-06, accuracy=1.0\n",
            "Test loss = 2.460021734237671, Test accuracy = 0.7420290112495422\n",
            "Step=11699 loss=6.189346631799708e-06, accuracy=1.0\n",
            "Test loss = 2.483280658721924, Test accuracy = 0.7420290112495422\n",
            "Step=11799 loss=6.03757252520154e-06, accuracy=1.0\n",
            "Test loss = 2.5194950103759766, Test accuracy = 0.7420290112495422\n",
            "Step=11899 loss=5.673383318480774e-06, accuracy=1.0\n",
            "Test loss = 2.5252280235290527, Test accuracy = 0.7405797243118286\n",
            "Step=11999 loss=5.308286247327487e-06, accuracy=1.0\n",
            "Test loss = 2.5435330867767334, Test accuracy = 0.7405797243118286\n",
            "Step=12099 loss=5.073820802863338e-06, accuracy=1.0\n",
            "Test loss = 2.5501794815063477, Test accuracy = 0.7405797243118286\n",
            "Step=12199 loss=4.724118020931201e-06, accuracy=1.0\n",
            "Test loss = 2.5685126781463623, Test accuracy = 0.7405797243118286\n",
            "Step=12299 loss=4.361366156899749e-06, accuracy=1.0\n",
            "Test loss = 2.589716672897339, Test accuracy = 0.7376811504364014\n",
            "Step=12399 loss=4.231990358221083e-06, accuracy=1.0\n",
            "Test loss = 2.591461658477783, Test accuracy = 0.739130437374115\n",
            "Step=12499 loss=4.038501670038386e-06, accuracy=1.0\n",
            "Test loss = 2.6019339561462402, Test accuracy = 0.7420290112495422\n",
            "Step=12599 loss=3.7397016171780707e-06, accuracy=1.0\n",
            "Test loss = 2.6245195865631104, Test accuracy = 0.7376811504364014\n",
            "Step=12699 loss=3.521786493365653e-06, accuracy=1.0\n",
            "Test loss = 2.6247763633728027, Test accuracy = 0.7405797243118286\n",
            "Step=12799 loss=3.3478453360658023e-06, accuracy=1.0\n",
            "Test loss = 2.6407535076141357, Test accuracy = 0.739130437374115\n",
            "Step=12899 loss=3.110938978352351e-06, accuracy=1.0\n",
            "Test loss = 2.6339855194091797, Test accuracy = 0.7405797243118286\n",
            "Step=12999 loss=2.8881868536245748e-06, accuracy=1.0\n",
            "Test loss = 2.6640071868896484, Test accuracy = 0.739130437374115\n",
            "Step=13099 loss=2.7726869939215247e-06, accuracy=1.0\n",
            "Test loss = 2.6739306449890137, Test accuracy = 0.7405797243118286\n",
            "Step=13199 loss=2.613377060924904e-06, accuracy=1.0\n",
            "Test loss = 2.6659095287323, Test accuracy = 0.7405797243118286\n",
            "Step=13299 loss=2.4789791757484636e-06, accuracy=1.0\n",
            "Test loss = 2.690565824508667, Test accuracy = 0.7405797243118286\n",
            "Step=13399 loss=2.3645208921152515e-06, accuracy=1.0\n",
            "Test loss = 2.701817274093628, Test accuracy = 0.739130437374115\n",
            "Step=13499 loss=2.1818719869770576e-06, accuracy=1.0\n",
            "Test loss = 2.7407360076904297, Test accuracy = 0.7362318634986877\n",
            "Step=13599 loss=2.11819851870132e-06, accuracy=1.0\n",
            "Test loss = 2.7602055072784424, Test accuracy = 0.7347826361656189\n",
            "Step=13699 loss=1.9484852316509206e-06, accuracy=1.0\n",
            "Test loss = 2.7516746520996094, Test accuracy = 0.739130437374115\n",
            "Step=13799 loss=1.8983711129294534e-06, accuracy=1.0\n",
            "Test loss = 2.7759323120117188, Test accuracy = 0.7362318634986877\n",
            "Step=13899 loss=1.7548088953844853e-06, accuracy=1.0\n",
            "Test loss = 2.777893304824829, Test accuracy = 0.739130437374115\n",
            "Step=13999 loss=1.6930723995756125e-06, accuracy=1.0\n",
            "Test loss = 2.794534206390381, Test accuracy = 0.7362318634986877\n",
            "Step=14099 loss=1.5701480515417643e-06, accuracy=1.0\n",
            "Test loss = 2.805933952331543, Test accuracy = 0.7376811504364014\n",
            "Step=14199 loss=1.4977382568304164e-06, accuracy=1.0\n",
            "Test loss = 2.819359302520752, Test accuracy = 0.7376811504364014\n",
            "Step=14299 loss=1.3901244926728395e-06, accuracy=1.0\n",
            "Test loss = 2.812837839126587, Test accuracy = 0.739130437374115\n",
            "Step=14399 loss=1.3255937165013166e-06, accuracy=1.0\n",
            "Test loss = 2.846116542816162, Test accuracy = 0.7376811504364014\n",
            "Step=14499 loss=1.2628975440520663e-06, accuracy=1.0\n",
            "Test loss = 2.8489644527435303, Test accuracy = 0.7376811504364014\n",
            "Step=14599 loss=1.1525736630346727e-06, accuracy=1.0\n",
            "Test loss = 2.8597710132598877, Test accuracy = 0.7376811504364014\n",
            "Step=14699 loss=1.121802975490027e-06, accuracy=1.0\n",
            "Test loss = 2.8702805042266846, Test accuracy = 0.739130437374115\n",
            "Step=14799 loss=1.0524476761020196e-06, accuracy=1.0\n",
            "Test loss = 2.888728141784668, Test accuracy = 0.7376811504364014\n",
            "Step=14899 loss=9.871530880900537e-07, accuracy=1.0\n",
            "Test loss = 2.9148027896881104, Test accuracy = 0.7376811504364014\n",
            "Step=14999 loss=9.398234112723003e-07, accuracy=1.0\n",
            "Test loss = 2.925208806991577, Test accuracy = 0.7376811504364014\n",
            "Step=15099 loss=9.31860739115109e-07, accuracy=1.0\n",
            "Test loss = 2.9603214263916016, Test accuracy = 0.7333333492279053\n",
            "Step=15199 loss=8.369965621568554e-07, accuracy=1.0\n",
            "Test loss = 2.9332127571105957, Test accuracy = 0.7376811504364014\n",
            "Step=15299 loss=7.864072961183411e-07, accuracy=1.0\n",
            "Test loss = 2.9520490169525146, Test accuracy = 0.7376811504364014\n",
            "Step=15399 loss=7.304070499003502e-07, accuracy=1.0\n",
            "Test loss = 2.9804773330688477, Test accuracy = 0.7362318634986877\n",
            "Step=15499 loss=7.027003164239431e-07, accuracy=1.0\n",
            "Test loss = 3.000761032104492, Test accuracy = 0.7347826361656189\n",
            "Step=15599 loss=6.827328542158284e-07, accuracy=1.0\n",
            "Test loss = 3.015949010848999, Test accuracy = 0.7347826361656189\n",
            "Step=15699 loss=6.351051760589143e-07, accuracy=1.0\n",
            "Test loss = 3.010493755340576, Test accuracy = 0.7362318634986877\n",
            "Step=15799 loss=5.703970325043883e-07, accuracy=1.0\n",
            "Test loss = 3.0036399364471436, Test accuracy = 0.7376811504364014\n",
            "Step=15899 loss=5.499825056176633e-07, accuracy=1.0\n",
            "Test loss = 3.029167652130127, Test accuracy = 0.7376811504364014\n",
            "Step=15999 loss=5.263549189749028e-07, accuracy=1.0\n",
            "Test loss = 3.0467355251312256, Test accuracy = 0.7347826361656189\n",
            "Step=16099 loss=4.887947434895068e-07, accuracy=1.0\n",
            "Test loss = 3.0690536499023438, Test accuracy = 0.7347826361656189\n",
            "Step=16199 loss=4.5634752893874974e-07, accuracy=1.0\n",
            "Test loss = 3.0762040615081787, Test accuracy = 0.7347826361656189\n",
            "Step=16299 loss=4.5545348285713773e-07, accuracy=1.0\n",
            "Test loss = 3.0857903957366943, Test accuracy = 0.7347826361656189\n",
            "Step=16399 loss=4.083472728666493e-07, accuracy=1.0\n",
            "Test loss = 3.1079599857330322, Test accuracy = 0.7347826361656189\n",
            "Step=16499 loss=4.0378381584105226e-07, accuracy=1.0\n",
            "Test loss = 3.104696035385132, Test accuracy = 0.7362318634986877\n",
            "Step=16599 loss=3.693621917477685e-07, accuracy=1.0\n",
            "Test loss = 3.1263046264648438, Test accuracy = 0.7347826361656189\n",
            "Step=16699 loss=3.4405820102278996e-07, accuracy=1.0\n",
            "Test loss = 3.129976511001587, Test accuracy = 0.7347826361656189\n",
            "Step=16799 loss=3.308706990878818e-07, accuracy=1.0\n",
            "Test loss = 3.1447668075561523, Test accuracy = 0.7347826361656189\n",
            "Step=16899 loss=3.0730827973002304e-07, accuracy=1.0\n",
            "Test loss = 3.152613401412964, Test accuracy = 0.7362318634986877\n",
            "Step=16999 loss=2.952010963497287e-07, accuracy=1.0\n",
            "Test loss = 3.1601803302764893, Test accuracy = 0.7362318634986877\n",
            "Step=17099 loss=2.7580168151075666e-07, accuracy=1.0\n",
            "Test loss = 3.185866117477417, Test accuracy = 0.7362318634986877\n",
            "Step=17199 loss=2.6931037723443295e-07, accuracy=1.0\n",
            "Test loss = 3.206892251968384, Test accuracy = 0.7362318634986877\n",
            "Step=17299 loss=2.544930477199614e-07, accuracy=1.0\n",
            "Test loss = 3.201514482498169, Test accuracy = 0.7362318634986877\n",
            "Step=17399 loss=2.401693190279275e-07, accuracy=1.0\n",
            "Test loss = 3.2133870124816895, Test accuracy = 0.7362318634986877\n",
            "Step=17499 loss=2.184881627442792e-07, accuracy=1.0\n",
            "Test loss = 3.2355451583862305, Test accuracy = 0.7362318634986877\n",
            "Step=17599 loss=2.0951021838300222e-07, accuracy=1.0\n",
            "Test loss = 3.247248649597168, Test accuracy = 0.7362318634986877\n",
            "Step=17699 loss=2.013704693837326e-07, accuracy=1.0\n",
            "Test loss = 3.2599828243255615, Test accuracy = 0.7362318634986877\n",
            "Step=17799 loss=1.8906770776538906e-07, accuracy=1.0\n",
            "Test loss = 3.277385950088501, Test accuracy = 0.7362318634986877\n",
            "Step=17899 loss=1.8555663174879556e-07, accuracy=1.0\n",
            "Test loss = 3.287567138671875, Test accuracy = 0.7362318634986877\n",
            "Step=17999 loss=1.7285339971806478e-07, accuracy=1.0\n",
            "Test loss = 3.286471128463745, Test accuracy = 0.7376811504364014\n",
            "Step=18099 loss=1.607927832480982e-07, accuracy=1.0\n",
            "Test loss = 3.3084635734558105, Test accuracy = 0.7347826361656189\n",
            "Step=18199 loss=1.499894498380172e-07, accuracy=1.0\n",
            "Test loss = 3.3236513137817383, Test accuracy = 0.7362318634986877\n",
            "Step=18299 loss=1.4396379448555764e-07, accuracy=1.0\n",
            "Test loss = 3.331388235092163, Test accuracy = 0.7362318634986877\n",
            "Step=18399 loss=1.3470645754409816e-07, accuracy=1.0\n",
            "Test loss = 3.3393394947052, Test accuracy = 0.7362318634986877\n",
            "Step=18499 loss=1.3086941088147342e-07, accuracy=1.0\n",
            "Test loss = 3.3490355014801025, Test accuracy = 0.7362318634986877\n",
            "Step=18599 loss=1.2390312008392358e-07, accuracy=1.0\n",
            "Test loss = 3.3685874938964844, Test accuracy = 0.7362318634986877\n",
            "Step=18699 loss=1.1320222945698787e-07, accuracy=1.0\n",
            "Test loss = 3.3804023265838623, Test accuracy = 0.7362318634986877\n",
            "Step=18799 loss=1.1135821367247445e-07, accuracy=1.0\n",
            "Test loss = 3.4043421745300293, Test accuracy = 0.7362318634986877\n",
            "Step=18899 loss=1.001450933557635e-07, accuracy=1.0\n",
            "Test loss = 3.3975942134857178, Test accuracy = 0.7376811504364014\n",
            "Step=18999 loss=9.944660053662347e-08, accuracy=1.0\n",
            "Test loss = 3.4208507537841797, Test accuracy = 0.7362318634986877\n",
            "Step=19099 loss=9.42311979912347e-08, accuracy=1.0\n",
            "Test loss = 3.4288294315338135, Test accuracy = 0.7362318634986877\n",
            "Step=19199 loss=8.90809858944408e-08, accuracy=1.0\n",
            "Test loss = 3.4391441345214844, Test accuracy = 0.7362318634986877\n",
            "Step=19299 loss=8.21146945639839e-08, accuracy=1.0\n",
            "Test loss = 3.460885524749756, Test accuracy = 0.7362318634986877\n",
            "Step=19399 loss=7.603316014126449e-08, accuracy=1.0\n",
            "Test loss = 3.4545466899871826, Test accuracy = 0.7362318634986877\n",
            "Step=19499 loss=6.981192711918993e-08, accuracy=1.0\n",
            "Test loss = 3.4694464206695557, Test accuracy = 0.7376811504364014\n",
            "Step=19599 loss=6.4922484988017e-08, accuracy=1.0\n",
            "Test loss = 3.500915050506592, Test accuracy = 0.7376811504364014\n",
            "Step=19699 loss=6.14114000896393e-08, accuracy=1.0\n",
            "Test loss = 3.5072202682495117, Test accuracy = 0.739130437374115\n",
            "Step=19799 loss=5.805863910524067e-08, accuracy=1.0\n",
            "Test loss = 3.551452875137329, Test accuracy = 0.7376811504364014\n",
            "Step=19899 loss=5.455686739708199e-08, accuracy=1.0\n",
            "Test loss = 3.5360794067382812, Test accuracy = 0.739130437374115\n",
            "Step=19999 loss=5.314125736788355e-08, accuracy=1.0\n",
            "Test loss = 3.566746711730957, Test accuracy = 0.739130437374115\n",
            "Step=20099 loss=5.022621911265901e-08, accuracy=1.0\n",
            "Test loss = 3.576843738555908, Test accuracy = 0.739130437374115\n",
            "Step=20199 loss=4.5616172776874464e-08, accuracy=1.0\n",
            "Test loss = 3.5807416439056396, Test accuracy = 0.739130437374115\n",
            "Step=20299 loss=4.2729073541636355e-08, accuracy=1.0\n",
            "Test loss = 3.612828016281128, Test accuracy = 0.7376811504364014\n",
            "Step=20399 loss=4.0875741582624415e-08, accuracy=1.0\n",
            "Test loss = 3.622776508331299, Test accuracy = 0.7376811504364014\n",
            "Step=20499 loss=3.939493947058281e-08, accuracy=1.0\n",
            "Test loss = 3.6470236778259277, Test accuracy = 0.7362318634986877\n",
            "Step=20599 loss=3.9422878632677796e-08, accuracy=1.0\n",
            "Test loss = 3.6657655239105225, Test accuracy = 0.7362318634986877\n",
            "Step=20699 loss=3.752298120218711e-08, accuracy=1.0\n",
            "Test loss = 3.6774699687957764, Test accuracy = 0.7362318634986877\n",
            "Step=20799 loss=3.45054968242664e-08, accuracy=1.0\n",
            "Test loss = 3.693948745727539, Test accuracy = 0.7347826361656189\n",
            "Step=20899 loss=3.506429012034573e-08, accuracy=1.0\n",
            "Test loss = 3.712961435317993, Test accuracy = 0.7347826361656189\n",
            "Step=20999 loss=3.213993805317727e-08, accuracy=1.0\n",
            "Test loss = 3.7235546112060547, Test accuracy = 0.7333333492279053\n",
            "Step=21099 loss=3.062188230984475e-08, accuracy=1.0\n",
            "Test loss = 3.7471704483032227, Test accuracy = 0.7333333492279053\n",
            "Step=21199 loss=3.1115483292154524e-08, accuracy=1.0\n",
            "Test loss = 3.7637217044830322, Test accuracy = 0.7333333492279053\n",
            "Step=21299 loss=2.7995552849269244e-08, accuracy=1.0\n",
            "Test loss = 3.7848877906799316, Test accuracy = 0.7333333492279053\n",
            "Step=21399 loss=2.7008351439761214e-08, accuracy=1.0\n",
            "Test loss = 3.819643497467041, Test accuracy = 0.7318840622901917\n",
            "Step=21499 loss=2.561136756895621e-08, accuracy=1.0\n",
            "Test loss = 3.8363664150238037, Test accuracy = 0.7318840622901917\n",
            "Step=21599 loss=2.4316829372317273e-08, accuracy=1.0\n",
            "Test loss = 3.8578896522521973, Test accuracy = 0.7318840622901917\n",
            "Step=21699 loss=2.3087483818073905e-08, accuracy=1.0\n",
            "Test loss = 3.8628082275390625, Test accuracy = 0.7333333492279053\n",
            "Step=21799 loss=2.1681186903599327e-08, accuracy=1.0\n",
            "Test loss = 3.889359951019287, Test accuracy = 0.7333333492279053\n",
            "Step=21899 loss=2.222135401019898e-08, accuracy=1.0\n",
            "Test loss = 3.917635202407837, Test accuracy = 0.7333333492279053\n",
            "Step=21999 loss=2.0926815862409854e-08, accuracy=1.0\n",
            "Test loss = 3.9446804523468018, Test accuracy = 0.7347826361656189\n",
            "Step=22099 loss=2.0060685903544596e-08, accuracy=1.0\n",
            "Test loss = 3.9577884674072266, Test accuracy = 0.7347826361656189\n",
            "Step=22199 loss=2.1308658051388817e-08, accuracy=1.0\n",
            "Test loss = 4.013298034667969, Test accuracy = 0.7333333492279053\n",
            "Step=22299 loss=1.8775460981501623e-08, accuracy=1.0\n",
            "Test loss = 4.03228759765625, Test accuracy = 0.7333333492279053\n",
            "Step=22399 loss=1.8635762666363575e-08, accuracy=1.0\n",
            "Test loss = 4.06177282333374, Test accuracy = 0.7347826361656189\n",
            "Step=22499 loss=1.8561256953653073e-08, accuracy=1.0\n",
            "Test loss = 4.089627742767334, Test accuracy = 0.7362318634986877\n",
            "Step=22599 loss=1.902691815658386e-08, accuracy=1.0\n",
            "Test loss = 4.132661819458008, Test accuracy = 0.7333333492279053\n",
            "Step=22699 loss=1.7564741918718595e-08, accuracy=1.0\n",
            "Test loss = 4.167755126953125, Test accuracy = 0.7318840622901917\n",
            "Step=22799 loss=1.728534505751611e-08, accuracy=1.0\n",
            "Test loss = 4.179065704345703, Test accuracy = 0.7362318634986877\n",
            "Step=22899 loss=1.73412244341975e-08, accuracy=1.0\n",
            "Test loss = 4.234713077545166, Test accuracy = 0.7347826361656189\n",
            "Step=22999 loss=1.6950069077203978e-08, accuracy=1.0\n",
            "Test loss = 4.2806525230407715, Test accuracy = 0.7347826361656189\n",
            "Step=23099 loss=1.702457482544162e-08, accuracy=1.0\n",
            "Test loss = 4.266643524169922, Test accuracy = 0.739130437374115\n",
            "Step=23199 loss=1.659616648552742e-08, accuracy=1.0\n",
            "Test loss = 4.355361461639404, Test accuracy = 0.7362318634986877\n",
            "Step=23299 loss=1.7127020357499134e-08, accuracy=1.0\n",
            "Test loss = 4.368259906768799, Test accuracy = 0.7347826361656189\n",
            "Step=23399 loss=1.681037061551649e-08, accuracy=1.0\n",
            "Test loss = 4.459596157073975, Test accuracy = 0.7333333492279053\n",
            "Step=23499 loss=1.5525145755646008e-08, accuracy=1.0\n",
            "Test loss = 4.526652812957764, Test accuracy = 0.7333333492279053\n",
            "Step=23599 loss=1.5003605100538665e-08, accuracy=1.0\n",
            "Test loss = 4.557858943939209, Test accuracy = 0.7362318634986877\n",
            "Step=23699 loss=1.627020370875698e-08, accuracy=1.0\n",
            "Test loss = 4.586092948913574, Test accuracy = 0.7362318634986877\n",
            "Step=23799 loss=1.5608964774038724e-08, accuracy=1.0\n",
            "Test loss = 4.672569751739502, Test accuracy = 0.7362318634986877\n",
            "Step=23899 loss=1.5255062149055475e-08, accuracy=1.0\n",
            "Test loss = 4.751363754272461, Test accuracy = 0.7362318634986877\n",
            "Step=23999 loss=1.463107611732184e-08, accuracy=1.0\n",
            "Test loss = 4.795843124389648, Test accuracy = 0.739130437374115\n",
            "Step=24099 loss=1.4957038783069977e-08, accuracy=1.0\n",
            "Test loss = 4.876631259918213, Test accuracy = 0.739130437374115\n",
            "Step=24199 loss=1.4733521598309097e-08, accuracy=1.0\n",
            "Test loss = 5.008546352386475, Test accuracy = 0.739130437374115\n",
            "Step=24299 loss=1.3653187446172056e-08, accuracy=1.0\n",
            "Test loss = 5.091900825500488, Test accuracy = 0.7376811504364014\n",
            "Step=24399 loss=1.4109535393380669e-08, accuracy=1.0\n",
            "Test loss = 5.224099636077881, Test accuracy = 0.7376811504364014\n",
            "Step=24499 loss=1.4994291590575414e-08, accuracy=1.0\n",
            "Test loss = 5.278324127197266, Test accuracy = 0.739130437374115\n",
            "Step=24599 loss=1.5888361197813337e-08, accuracy=1.0\n",
            "Test loss = 5.464596271514893, Test accuracy = 0.7376811504364014\n",
            "Step=24699 loss=1.4221293935801072e-08, accuracy=1.0\n",
            "Test loss = 5.5563459396362305, Test accuracy = 0.7405797243118286\n",
            "Step=24799 loss=1.6177070911727043e-08, accuracy=1.0\n",
            "Test loss = 5.702450752258301, Test accuracy = 0.7420290112495422\n",
            "Step=24899 loss=1.61584440294682e-08, accuracy=1.0\n",
            "Test loss = 5.861598968505859, Test accuracy = 0.7420290112495422\n",
            "Step=24999 loss=1.5925612690814716e-08, accuracy=1.0\n",
            "Test loss = 6.027153968811035, Test accuracy = 0.7420290112495422\n",
            "Step=25099 loss=1.707113967963636e-08, accuracy=1.0\n",
            "Test loss = 6.22492790222168, Test accuracy = 0.7449275255203247\n",
            "Step=25199 loss=0.00019801973545651385, accuracy=1.0\n",
            "Test loss = 6.6022725105285645, Test accuracy = 0.7376811504364014\n",
            "Step=25299 loss=6.929128193462475e-06, accuracy=1.0\n",
            "Test loss = 6.693077564239502, Test accuracy = 0.739130437374115\n",
            "Step=25399 loss=1.4296148776793415e-06, accuracy=1.0\n",
            "Test loss = 6.656167984008789, Test accuracy = 0.739130437374115\n",
            "Step=25499 loss=1.0574533068563597e-06, accuracy=1.0\n",
            "Test loss = 6.648642539978027, Test accuracy = 0.739130437374115\n",
            "Step=25599 loss=7.992447226001787e-07, accuracy=1.0\n",
            "Test loss = 6.617820739746094, Test accuracy = 0.739130437374115\n",
            "Step=25699 loss=6.708280807288247e-07, accuracy=1.0\n",
            "Test loss = 6.606771469116211, Test accuracy = 0.7405797243118286\n",
            "Step=25799 loss=5.972851595004159e-07, accuracy=1.0\n",
            "Test loss = 6.597314357757568, Test accuracy = 0.7405797243118286\n",
            "Step=25899 loss=4.173962913966989e-07, accuracy=1.0\n",
            "Test loss = 6.567183017730713, Test accuracy = 0.7405797243118286\n",
            "Step=25999 loss=3.9201653649456604e-07, accuracy=1.0\n",
            "Test loss = 6.559537887573242, Test accuracy = 0.7420290112495422\n",
            "Step=26099 loss=3.506858526236556e-07, accuracy=1.0\n",
            "Test loss = 6.551013469696045, Test accuracy = 0.7420290112495422\n",
            "Step=26199 loss=3.5317234107878905e-07, accuracy=1.0\n",
            "Test loss = 6.5548996925354, Test accuracy = 0.7420290112495422\n",
            "Step=26299 loss=3.25755194978683e-07, accuracy=1.0\n",
            "Test loss = 6.549324989318848, Test accuracy = 0.7420290112495422\n",
            "Step=26399 loss=2.910731422822721e-07, accuracy=1.0\n",
            "Test loss = 6.544185161590576, Test accuracy = 0.7420290112495422\n",
            "Step=26499 loss=2.477491776176066e-07, accuracy=1.0\n",
            "Test loss = 6.529259204864502, Test accuracy = 0.7420290112495422\n",
            "Step=26599 loss=2.5155762870099354e-07, accuracy=1.0\n",
            "Test loss = 6.5319366455078125, Test accuracy = 0.7420290112495422\n",
            "Step=26699 loss=2.09360261962388e-07, accuracy=1.0\n",
            "Test loss = 6.521713733673096, Test accuracy = 0.7434782385826111\n",
            "Step=26799 loss=2.1663353990675205e-07, accuracy=1.0\n",
            "Test loss = 6.524845600128174, Test accuracy = 0.7449275255203247\n",
            "Step=26899 loss=2.0243128449948246e-07, accuracy=1.0\n",
            "Test loss = 6.5181074142456055, Test accuracy = 0.7449275255203247\n",
            "Step=26999 loss=1.8793993591614822e-07, accuracy=1.0\n",
            "Test loss = 6.519003868103027, Test accuracy = 0.7434782385826111\n",
            "Step=27099 loss=1.6094127857968488e-07, accuracy=1.0\n",
            "Test loss = 6.512140274047852, Test accuracy = 0.7434782385826111\n",
            "Step=27199 loss=1.6327892019774026e-07, accuracy=1.0\n",
            "Test loss = 6.503699779510498, Test accuracy = 0.7434782385826111\n",
            "Step=27299 loss=1.6522524152406958e-07, accuracy=1.0\n",
            "Test loss = 6.511398792266846, Test accuracy = 0.7434782385826111\n",
            "Step=27399 loss=1.2497391285393178e-07, accuracy=1.0\n",
            "Test loss = 6.502496719360352, Test accuracy = 0.7434782385826111\n",
            "Step=27499 loss=1.2030796309403513e-07, accuracy=1.0\n",
            "Test loss = 6.496359825134277, Test accuracy = 0.7434782385826111\n",
            "Step=27599 loss=1.1605186859497963e-07, accuracy=1.0\n",
            "Test loss = 6.483262538909912, Test accuracy = 0.7434782385826111\n",
            "Step=27699 loss=1.1796102445060796e-07, accuracy=1.0\n",
            "Test loss = 6.484827995300293, Test accuracy = 0.7434782385826111\n",
            "Step=27799 loss=1.1111582136180687e-07, accuracy=1.0\n",
            "Test loss = 6.488194942474365, Test accuracy = 0.7434782385826111\n",
            "Step=27899 loss=1.1314615782964666e-07, accuracy=1.0\n",
            "Test loss = 6.483829498291016, Test accuracy = 0.7434782385826111\n",
            "Step=27999 loss=9.905529786635725e-08, accuracy=1.0\n",
            "Test loss = 6.4782023429870605, Test accuracy = 0.7420290112495422\n",
            "Step=28099 loss=9.469672363593417e-08, accuracy=1.0\n",
            "Test loss = 6.473404407501221, Test accuracy = 0.7405797243118286\n",
            "Step=28199 loss=8.458261193666772e-08, accuracy=1.0\n",
            "Test loss = 6.465224266052246, Test accuracy = 0.7405797243118286\n",
            "Step=28299 loss=8.403310419069499e-08, accuracy=1.0\n",
            "Test loss = 6.464104175567627, Test accuracy = 0.7405797243118286\n",
            "Step=28399 loss=8.071762087169531e-08, accuracy=1.0\n",
            "Test loss = 6.465060234069824, Test accuracy = 0.739130437374115\n",
            "Step=28499 loss=7.921819522138663e-08, accuracy=1.0\n",
            "Test loss = 6.4601569175720215, Test accuracy = 0.7405797243118286\n",
            "Step=28599 loss=7.923682483479411e-08, accuracy=1.0\n",
            "Test loss = 6.4604010581970215, Test accuracy = 0.739130437374115\n",
            "Step=28699 loss=7.049172809914239e-08, accuracy=1.0\n",
            "Test loss = 6.454024314880371, Test accuracy = 0.7405797243118286\n",
            "Step=28799 loss=7.209359838000751e-08, accuracy=1.0\n",
            "Test loss = 6.459014892578125, Test accuracy = 0.7405797243118286\n",
            "Step=28899 loss=6.966285900134039e-08, accuracy=1.0\n",
            "Test loss = 6.455502510070801, Test accuracy = 0.7405797243118286\n",
            "Step=28999 loss=6.142998980607217e-08, accuracy=1.0\n",
            "Test loss = 6.448233127593994, Test accuracy = 0.7420290112495422\n",
            "Step=29099 loss=5.6689561898082275e-08, accuracy=1.0\n",
            "Test loss = 6.446737766265869, Test accuracy = 0.7420290112495422\n",
            "Step=29199 loss=5.97908596766672e-08, accuracy=1.0\n",
            "Test loss = 6.449211120605469, Test accuracy = 0.7420290112495422\n",
            "Step=29299 loss=5.669887576331689e-08, accuracy=1.0\n",
            "Test loss = 6.4415364265441895, Test accuracy = 0.7420290112495422\n",
            "Step=29399 loss=5.342062685542714e-08, accuracy=1.0\n",
            "Test loss = 6.436738014221191, Test accuracy = 0.7420290112495422\n",
            "Step=29499 loss=5.214471384462627e-08, accuracy=1.0\n",
            "Test loss = 6.434216499328613, Test accuracy = 0.7420290112495422\n",
            "Step=29599 loss=4.99840490419956e-08, accuracy=1.0\n",
            "Test loss = 6.439688205718994, Test accuracy = 0.7420290112495422\n",
            "Step=29699 loss=4.99654250596393e-08, accuracy=1.0\n",
            "Test loss = 6.440844535827637, Test accuracy = 0.7420290112495422\n",
            "Step=29799 loss=4.59048673562279e-08, accuracy=1.0\n",
            "Test loss = 6.4340691566467285, Test accuracy = 0.7420290112495422\n",
            "Step=29899 loss=4.4014282263304946e-08, accuracy=1.0\n",
            "Test loss = 6.428478240966797, Test accuracy = 0.7420290112495422\n",
            "Step=29999 loss=4.458238315407925e-08, accuracy=1.0\n",
            "Test loss = 6.433304309844971, Test accuracy = 0.7420290112495422\n",
            "Step=30099 loss=4.118306366152069e-08, accuracy=1.0\n",
            "Test loss = 6.433096885681152, Test accuracy = 0.7420290112495422\n",
            "Step=30199 loss=4.629602260664001e-08, accuracy=1.0\n",
            "Test loss = 6.439620018005371, Test accuracy = 0.7420290112495422\n",
            "Step=30299 loss=4.2021258011004645e-08, accuracy=1.0\n",
            "Test loss = 6.435423851013184, Test accuracy = 0.7420290112495422\n",
            "Step=30399 loss=3.863124460323775e-08, accuracy=1.0\n",
            "Test loss = 6.431231498718262, Test accuracy = 0.7420290112495422\n",
            "Step=30499 loss=4.08850453492704e-08, accuracy=1.0\n",
            "Test loss = 6.4324798583984375, Test accuracy = 0.7420290112495422\n",
            "Step=30599 loss=3.750434534488534e-08, accuracy=1.0\n",
            "Test loss = 6.431434631347656, Test accuracy = 0.7420290112495422\n",
            "Step=30699 loss=3.695486609167631e-08, accuracy=1.0\n",
            "Test loss = 6.4341349601745605, Test accuracy = 0.7420290112495422\n",
            "Step=30799 loss=3.5213293640978805e-08, accuracy=1.0\n",
            "Test loss = 6.434019565582275, Test accuracy = 0.7420290112495422\n",
            "Step=30899 loss=3.009102415241216e-08, accuracy=1.0\n",
            "Test loss = 6.432461738586426, Test accuracy = 0.7420290112495422\n",
            "Step=30999 loss=2.756714092111423e-08, accuracy=1.0\n",
            "Test loss = 6.426249027252197, Test accuracy = 0.7420290112495422\n",
            "Step=31099 loss=2.662650476992212e-08, accuracy=1.0\n",
            "Test loss = 6.429245948791504, Test accuracy = 0.7420290112495422\n",
            "Step=31199 loss=2.7818597478912465e-08, accuracy=1.0\n",
            "Test loss = 6.4287428855896, Test accuracy = 0.7420290112495422\n",
            "Step=31299 loss=2.6477493619836424e-08, accuracy=1.0\n",
            "Test loss = 6.428232669830322, Test accuracy = 0.7420290112495422\n",
            "Step=31399 loss=2.5471666589282903e-08, accuracy=1.0\n",
            "Test loss = 6.425050735473633, Test accuracy = 0.7420290112495422\n",
            "Step=31499 loss=2.398155068306096e-08, accuracy=1.0\n",
            "Test loss = 6.423828125, Test accuracy = 0.7420290112495422\n",
            "Step=31599 loss=2.265907257159583e-08, accuracy=1.0\n",
            "Test loss = 6.425915718078613, Test accuracy = 0.7420290112495422\n",
            "Step=31699 loss=2.1196896984321256e-08, accuracy=1.0\n",
            "Test loss = 6.428210735321045, Test accuracy = 0.7420290112495422\n",
            "Step=31799 loss=2.2016460703433438e-08, accuracy=1.0\n",
            "Test loss = 6.428754806518555, Test accuracy = 0.7420290112495422\n",
            "Step=31899 loss=1.974403456506124e-08, accuracy=1.0\n",
            "Test loss = 6.422677993774414, Test accuracy = 0.7420290112495422\n",
            "Step=31999 loss=1.971609475237557e-08, accuracy=1.0\n",
            "Test loss = 6.43113374710083, Test accuracy = 0.7434782385826111\n",
            "Step=32099 loss=1.9175928089865123e-08, accuracy=1.0\n",
            "Test loss = 6.42794942855835, Test accuracy = 0.7434782385826111\n",
            "Step=32199 loss=1.7182898464085384e-08, accuracy=1.0\n",
            "Test loss = 6.425497531890869, Test accuracy = 0.7434782385826111\n",
            "Step=32299 loss=1.6205010038294887e-08, accuracy=1.0\n",
            "Test loss = 6.426691055297852, Test accuracy = 0.7434782385826111\n",
            "Step=32399 loss=1.654959920660559e-08, accuracy=1.0\n",
            "Test loss = 6.428429126739502, Test accuracy = 0.7434782385826111\n",
            "Step=32499 loss=1.766718590090477e-08, accuracy=1.0\n",
            "Test loss = 6.432837963104248, Test accuracy = 0.7434782385826111\n",
            "Step=32599 loss=1.7639245843970032e-08, accuracy=1.0\n",
            "Test loss = 6.4352707862854, Test accuracy = 0.7434782385826111\n",
            "Step=32699 loss=1.584179416092013e-08, accuracy=1.0\n",
            "Test loss = 6.436858654022217, Test accuracy = 0.7434782385826111\n",
            "Step=32799 loss=1.6959381170522647e-08, accuracy=1.0\n",
            "Test loss = 6.438867092132568, Test accuracy = 0.7434782385826111\n",
            "Step=32899 loss=1.4649701934876801e-08, accuracy=1.0\n",
            "Test loss = 6.432536602020264, Test accuracy = 0.7434782385826111\n",
            "Step=32999 loss=1.5385446689997196e-08, accuracy=1.0\n",
            "Test loss = 6.436466693878174, Test accuracy = 0.7434782385826111\n",
            "Step=33099 loss=1.4826653108590548e-08, accuracy=1.0\n",
            "Test loss = 6.438150882720947, Test accuracy = 0.7434782385826111\n",
            "Step=33199 loss=1.4090908577735206e-08, accuracy=1.0\n",
            "Test loss = 6.438005447387695, Test accuracy = 0.7434782385826111\n",
            "Step=33299 loss=1.5357507241464674e-08, accuracy=1.0\n",
            "Test loss = 6.443282127380371, Test accuracy = 0.7434782385826111\n",
            "Step=33399 loss=1.1650844058408438e-08, accuracy=1.0\n",
            "Test loss = 6.443132400512695, Test accuracy = 0.7434782385826111\n",
            "Step=33499 loss=1.2451781310485189e-08, accuracy=1.0\n",
            "Test loss = 6.445905685424805, Test accuracy = 0.7434782385826111\n",
            "Step=33599 loss=1.0915099343256074e-08, accuracy=1.0\n",
            "Test loss = 6.447426795959473, Test accuracy = 0.7434782385826111\n",
            "Step=33699 loss=1.036561906753164e-08, accuracy=1.0\n",
            "Test loss = 6.4439921379089355, Test accuracy = 0.7434782385826111\n",
            "Step=33799 loss=1.099891832501143e-08, accuracy=1.0\n",
            "Test loss = 6.452127933502197, Test accuracy = 0.7434782385826111\n",
            "Step=33899 loss=1.0654329150039388e-08, accuracy=1.0\n",
            "Test loss = 6.451867580413818, Test accuracy = 0.7434782385826111\n",
            "Step=33999 loss=1.1343507688721388e-08, accuracy=1.0\n",
            "Test loss = 6.453790187835693, Test accuracy = 0.7434782385826111\n",
            "Step=34099 loss=1.1129303578716333e-08, accuracy=1.0\n",
            "Test loss = 6.45418643951416, Test accuracy = 0.7434782385826111\n",
            "Step=34199 loss=1.0691582053024008e-08, accuracy=1.0\n",
            "Test loss = 6.459614276885986, Test accuracy = 0.7434782385826111\n",
            "Step=34299 loss=9.76025950449877e-09, accuracy=1.0\n",
            "Test loss = 6.462324619293213, Test accuracy = 0.7434782385826111\n",
            "Step=34399 loss=9.86270502378872e-09, accuracy=1.0\n",
            "Test loss = 6.4676008224487305, Test accuracy = 0.7434782385826111\n",
            "Step=34499 loss=9.462236387758339e-09, accuracy=1.0\n",
            "Test loss = 6.471970558166504, Test accuracy = 0.7434782385826111\n",
            "Step=34599 loss=8.75443131764797e-09, accuracy=1.0\n",
            "Test loss = 6.475099563598633, Test accuracy = 0.7434782385826111\n",
            "Step=34699 loss=8.13044524428097e-09, accuracy=1.0\n",
            "Test loss = 6.477118492126465, Test accuracy = 0.7434782385826111\n",
            "Step=34799 loss=8.102505617557654e-09, accuracy=1.0\n",
            "Test loss = 6.4816575050354, Test accuracy = 0.7434782385826111\n",
            "Step=34899 loss=7.795169194579898e-09, accuracy=1.0\n",
            "Test loss = 6.486965179443359, Test accuracy = 0.7434782385826111\n",
            "Step=34999 loss=7.664784070215979e-09, accuracy=1.0\n",
            "Test loss = 6.4885573387146, Test accuracy = 0.7449275255203247\n",
            "Step=35099 loss=7.264315382560227e-09, accuracy=1.0\n",
            "Test loss = 6.492456436157227, Test accuracy = 0.7434782385826111\n",
            "Step=35199 loss=8.13044530978413e-09, accuracy=1.0\n",
            "Test loss = 6.495466232299805, Test accuracy = 0.7434782385826111\n",
            "Step=35299 loss=7.664784035799067e-09, accuracy=1.0\n",
            "Test loss = 6.499462127685547, Test accuracy = 0.7434782385826111\n",
            "Step=35399 loss=8.130445289800114e-09, accuracy=1.0\n",
            "Test loss = 6.506267070770264, Test accuracy = 0.7434782385826111\n",
            "Step=35499 loss=8.055939497153552e-09, accuracy=1.0\n",
            "Test loss = 6.512568473815918, Test accuracy = 0.7434782385826111\n",
            "Step=35599 loss=6.603076420641507e-09, accuracy=1.0\n",
            "Test loss = 6.511785507202148, Test accuracy = 0.7434782385826111\n",
            "Step=35699 loss=7.972120467103494e-09, accuracy=1.0\n",
            "Test loss = 6.5195441246032715, Test accuracy = 0.7434782385826111\n",
            "Step=35799 loss=8.055939491602438e-09, accuracy=1.0\n",
            "Test loss = 6.525608539581299, Test accuracy = 0.7420290112495422\n",
            "Step=35899 loss=7.795169226221254e-09, accuracy=1.0\n",
            "Test loss = 6.533121585845947, Test accuracy = 0.7434782385826111\n",
            "Step=35999 loss=8.093192435110197e-09, accuracy=1.0\n",
            "Test loss = 6.544404029846191, Test accuracy = 0.7420290112495422\n",
            "Step=36099 loss=7.748603131352282e-09, accuracy=1.0\n",
            "Test loss = 6.548117160797119, Test accuracy = 0.7449275255203247\n",
            "Step=36199 loss=7.227062505665849e-09, accuracy=1.0\n",
            "Test loss = 6.549600124359131, Test accuracy = 0.7449275255203247\n",
            "Step=36299 loss=8.78237104817714e-09, accuracy=1.0\n",
            "Test loss = 6.562654495239258, Test accuracy = 0.7449275255203247\n",
            "Step=36399 loss=8.409842094936338e-09, accuracy=1.0\n",
            "Test loss = 6.575530529022217, Test accuracy = 0.7449275255203247\n",
            "Step=36499 loss=7.70203697708638e-09, accuracy=1.0\n",
            "Test loss = 6.577687740325928, Test accuracy = 0.7449275255203247\n",
            "Step=36599 loss=9.03382812889042e-09, accuracy=1.0\n",
            "Test loss = 6.587345123291016, Test accuracy = 0.7449275255203247\n",
            "Step=36699 loss=9.210779374768663e-09, accuracy=1.0\n",
            "Test loss = 6.602047443389893, Test accuracy = 0.7434782385826111\n",
            "Step=36799 loss=8.689238796266707e-09, accuracy=1.0\n",
            "Test loss = 6.603672504425049, Test accuracy = 0.7434782385826111\n",
            "Step=36899 loss=8.32602305766983e-09, accuracy=1.0\n",
            "Test loss = 6.613795757293701, Test accuracy = 0.7434782385826111\n",
            "Step=36999 loss=9.937210891930449e-09, accuracy=1.0\n",
            "Test loss = 6.640927791595459, Test accuracy = 0.7449275255203247\n",
            "Step=37099 loss=9.024514928679395e-09, accuracy=1.0\n",
            "Test loss = 6.641308784484863, Test accuracy = 0.7449275255203247\n",
            "Step=37199 loss=9.760259654933989e-09, accuracy=1.0\n",
            "Test loss = 6.658082962036133, Test accuracy = 0.7434782385826111\n",
            "Step=37299 loss=9.862705103724778e-09, accuracy=1.0\n",
            "Test loss = 6.658726215362549, Test accuracy = 0.7434782385826111\n",
            "Step=37399 loss=9.629874505034941e-09, accuracy=1.0\n",
            "Test loss = 6.68963623046875, Test accuracy = 0.7449275255203247\n",
            "Step=37499 loss=9.23871900204709e-09, accuracy=1.0\n",
            "Test loss = 6.709546089172363, Test accuracy = 0.7449275255203247\n",
            "Step=37599 loss=9.788199242244389e-09, accuracy=1.0\n",
            "Test loss = 6.720250129699707, Test accuracy = 0.7449275255203247\n",
            "Step=37699 loss=1.0039656372917705e-08, accuracy=1.0\n",
            "Test loss = 6.749277591705322, Test accuracy = 0.7463768124580383\n",
            "Step=37799 loss=1.0784714230549497e-08, accuracy=1.0\n",
            "Test loss = 6.755037784576416, Test accuracy = 0.7463768124580383\n",
            "Step=37899 loss=1.0533257239764283e-08, accuracy=1.0\n",
            "Test loss = 6.778374195098877, Test accuracy = 0.7463768124580383\n",
            "Step=37999 loss=1.1371447383723422e-08, accuracy=1.0\n",
            "Test loss = 6.807152271270752, Test accuracy = 0.7463768124580383\n",
            "Step=38099 loss=1.0980291915707596e-08, accuracy=1.0\n",
            "Test loss = 6.826234340667725, Test accuracy = 0.7463768124580383\n",
            "Step=38199 loss=1.1213122486086746e-08, accuracy=1.0\n",
            "Test loss = 6.84716796875, Test accuracy = 0.7449275255203247\n",
            "Step=38299 loss=1.2032686274032756e-08, accuracy=1.0\n",
            "Test loss = 6.882822513580322, Test accuracy = 0.7449275255203247\n",
            "Step=38399 loss=1.1585651584766764e-08, accuracy=1.0\n",
            "Test loss = 6.906591892242432, Test accuracy = 0.7449275255203247\n",
            "Step=38499 loss=1.1613591178738503e-08, accuracy=1.0\n",
            "Test loss = 6.915470600128174, Test accuracy = 0.7449275255203247\n",
            "Step=38599 loss=1.1557711917520308e-08, accuracy=1.0\n",
            "Test loss = 6.923529624938965, Test accuracy = 0.7449275255203247\n",
            "Step=38699 loss=1.2852250097505902e-08, accuracy=1.0\n",
            "Test loss = 6.9701433181762695, Test accuracy = 0.7434782385826111\n",
            "Step=38799 loss=1.1166556431740914e-08, accuracy=1.0\n",
            "Test loss = 6.97700834274292, Test accuracy = 0.7449275255203247\n",
            "Step=38899 loss=1.0496004282378735e-08, accuracy=1.0\n",
            "Test loss = 6.9973249435424805, Test accuracy = 0.7463768124580383\n",
            "Step=38999 loss=1.1241062175537663e-08, accuracy=1.0\n",
            "Test loss = 7.0531392097473145, Test accuracy = 0.7463768124580383\n",
            "Step=39099 loss=1.0086222462790673e-08, accuracy=1.0\n",
            "Test loss = 7.066290855407715, Test accuracy = 0.7463768124580383\n",
            "Step=39199 loss=1.0719521532087662e-08, accuracy=1.0\n",
            "Test loss = 7.096092224121094, Test accuracy = 0.7463768124580383\n",
            "Step=39299 loss=1.0952352213489113e-08, accuracy=1.0\n",
            "Test loss = 7.129877090454102, Test accuracy = 0.7463768124580383\n",
            "Step=39399 loss=1.1371447318220263e-08, accuracy=1.0\n",
            "Test loss = 7.156691551208496, Test accuracy = 0.7463768124580383\n",
            "Step=39499 loss=1.1408700304471608e-08, accuracy=1.0\n",
            "Test loss = 7.232802391052246, Test accuracy = 0.7449275255203247\n",
            "Step=39599 loss=1.0626389377321744e-08, accuracy=1.0\n",
            "Test loss = 7.26681661605835, Test accuracy = 0.7463768124580383\n",
            "Step=39699 loss=1.1110677073933318e-08, accuracy=1.0\n",
            "Test loss = 7.304901123046875, Test accuracy = 0.747826099395752\n",
            "Step=39799 loss=1.1697409953992377e-08, accuracy=1.0\n",
            "Test loss = 7.451737880706787, Test accuracy = 0.747826099395752\n",
            "Step=39899 loss=1.247040777030417e-08, accuracy=1.0\n",
            "Test loss = 7.434568881988525, Test accuracy = 0.7463768124580383\n",
            "Step=39999 loss=1.358799465944749e-08, accuracy=1.0\n",
            "Test loss = 7.481443405151367, Test accuracy = 0.7463768124580383\n",
            "Step=40099 loss=1.3895330637780923e-08, accuracy=1.0\n",
            "Test loss = 7.556329727172852, Test accuracy = 0.7463768124580383\n",
            "Step=40199 loss=1.3513488283933838e-08, accuracy=1.0\n",
            "Test loss = 7.693355083465576, Test accuracy = 0.7449275255203247\n",
            "Step=40299 loss=1.2954695144951067e-08, accuracy=1.0\n",
            "Test loss = 7.7513628005981445, Test accuracy = 0.7449275255203247\n",
            "Step=40399 loss=1.3457609033817875e-08, accuracy=1.0\n",
            "Test loss = 7.822971820831299, Test accuracy = 0.7463768124580383\n",
            "Step=40499 loss=1.2749804116918284e-08, accuracy=1.0\n",
            "Test loss = 7.9029412269592285, Test accuracy = 0.747826099395752\n",
            "Step=40599 loss=1.1790541977196866e-08, accuracy=1.0\n",
            "Test loss = 8.029817581176758, Test accuracy = 0.747826099395752\n",
            "Step=40699 loss=1.4184040293407918e-08, accuracy=1.0\n",
            "Test loss = 8.165783882141113, Test accuracy = 0.747826099395752\n",
            "Step=40799 loss=1.3373790257453777e-08, accuracy=1.0\n",
            "Test loss = 8.270328521728516, Test accuracy = 0.7507246136665344\n",
            "Step=40899 loss=1.494772454702087e-08, accuracy=1.0\n",
            "Test loss = 8.563406944274902, Test accuracy = 0.7492753863334656\n",
            "Step=40999 loss=1.6344707876037034e-08, accuracy=1.0\n",
            "Test loss = 8.592260360717773, Test accuracy = 0.7507246136665344\n",
            "Step=41099 loss=0.007492369532311918, accuracy=0.998046875\n",
            "Test loss = 8.004533767700195, Test accuracy = 0.7492753863334656\n",
            "Step=41199 loss=0.00024307505169815613, accuracy=0.999921875\n",
            "Test loss = 8.442400932312012, Test accuracy = 0.7536231875419617\n",
            "Step=41299 loss=3.8019948448209106e-06, accuracy=1.0\n",
            "Test loss = 8.463301658630371, Test accuracy = 0.7550724744796753\n",
            "Step=41399 loss=2.4119328760718872e-06, accuracy=1.0\n",
            "Test loss = 8.454305648803711, Test accuracy = 0.7550724744796753\n",
            "Step=41499 loss=2.447367618287899e-06, accuracy=1.0\n",
            "Test loss = 8.454728126525879, Test accuracy = 0.7565217614173889\n",
            "Step=41599 loss=1.904721606464932e-06, accuracy=1.0\n",
            "Test loss = 8.451254844665527, Test accuracy = 0.7565217614173889\n",
            "Step=41699 loss=1.718209762486822e-06, accuracy=1.0\n",
            "Test loss = 8.454851150512695, Test accuracy = 0.7565217614173889\n",
            "Step=41799 loss=1.5121655559369173e-06, accuracy=1.0\n",
            "Test loss = 8.455538749694824, Test accuracy = 0.7565217614173889\n",
            "Step=41899 loss=1.408304884691347e-06, accuracy=1.0\n",
            "Test loss = 8.4592866897583, Test accuracy = 0.7550724744796753\n",
            "Step=41999 loss=1.232696877693229e-06, accuracy=1.0\n",
            "Test loss = 8.461243629455566, Test accuracy = 0.7565217614173889\n",
            "Step=42099 loss=1.1952087747602037e-06, accuracy=1.0\n",
            "Test loss = 8.461732864379883, Test accuracy = 0.7550724744796753\n",
            "Step=42199 loss=1.1102470041635115e-06, accuracy=1.0\n",
            "Test loss = 8.464170455932617, Test accuracy = 0.7550724744796753\n",
            "Step=42299 loss=9.666922931472754e-07, accuracy=1.0\n",
            "Test loss = 8.463323593139648, Test accuracy = 0.7550724744796753\n",
            "Step=42399 loss=8.419630614042717e-07, accuracy=1.0\n",
            "Test loss = 8.46562385559082, Test accuracy = 0.7536231875419617\n",
            "Step=42499 loss=8.06940025981362e-07, accuracy=1.0\n",
            "Test loss = 8.465144157409668, Test accuracy = 0.7536231875419617\n",
            "Step=42599 loss=8.307810020369288e-07, accuracy=1.0\n",
            "Test loss = 8.467039108276367, Test accuracy = 0.7536231875419617\n",
            "Step=42699 loss=7.721367103386001e-07, accuracy=1.0\n",
            "Test loss = 8.470203399658203, Test accuracy = 0.7536231875419617\n",
            "Step=42799 loss=7.235980436703926e-07, accuracy=1.0\n",
            "Test loss = 8.475994110107422, Test accuracy = 0.7536231875419617\n",
            "Step=42899 loss=6.170864842047763e-07, accuracy=1.0\n",
            "Test loss = 8.475194931030273, Test accuracy = 0.7536231875419617\n",
            "Step=42999 loss=6.816533900178002e-07, accuracy=1.0\n",
            "Test loss = 8.482321739196777, Test accuracy = 0.7550724744796753\n",
            "Step=43099 loss=5.844915634156677e-07, accuracy=1.0\n",
            "Test loss = 8.481714248657227, Test accuracy = 0.7550724744796753\n",
            "Step=43199 loss=5.165713307064834e-07, accuracy=1.0\n",
            "Test loss = 8.481770515441895, Test accuracy = 0.7550724744796753\n",
            "Step=43299 loss=4.991561760903096e-07, accuracy=1.0\n",
            "Test loss = 8.480249404907227, Test accuracy = 0.7550724744796753\n",
            "Step=43399 loss=4.771492981348047e-07, accuracy=1.0\n",
            "Test loss = 8.48075008392334, Test accuracy = 0.7550724744796753\n",
            "Step=43499 loss=4.744858874516922e-07, accuracy=1.0\n",
            "Test loss = 8.482579231262207, Test accuracy = 0.7550724744796753\n",
            "Step=43599 loss=4.3687021153537844e-07, accuracy=1.0\n",
            "Test loss = 8.48604965209961, Test accuracy = 0.7550724744796753\n",
            "Step=43699 loss=4.0626762402240504e-07, accuracy=1.0\n",
            "Test loss = 8.486351013183594, Test accuracy = 0.7550724744796753\n",
            "Step=43799 loss=4.477669020275243e-07, accuracy=1.0\n",
            "Test loss = 8.48757553100586, Test accuracy = 0.7550724744796753\n",
            "Step=43899 loss=4.4538310429942384e-07, accuracy=1.0\n",
            "Test loss = 8.488809585571289, Test accuracy = 0.7550724744796753\n",
            "Step=43999 loss=3.8752068970637765e-07, accuracy=1.0\n",
            "Test loss = 8.489336013793945, Test accuracy = 0.7550724744796753\n",
            "Step=44099 loss=3.491228074636865e-07, accuracy=1.0\n",
            "Test loss = 8.488656044006348, Test accuracy = 0.7550724744796753\n",
            "Step=44199 loss=3.4697154262630646e-07, accuracy=1.0\n",
            "Test loss = 8.489947319030762, Test accuracy = 0.7550724744796753\n",
            "Step=44299 loss=3.3004943105652274e-07, accuracy=1.0\n",
            "Test loss = 8.491239547729492, Test accuracy = 0.7550724744796753\n",
            "Step=44399 loss=3.3217297122689616e-07, accuracy=1.0\n",
            "Test loss = 8.489309310913086, Test accuracy = 0.7550724744796753\n",
            "Step=44499 loss=3.4737191988654104e-07, accuracy=1.0\n",
            "Test loss = 8.490498542785645, Test accuracy = 0.7550724744796753\n",
            "Step=44599 loss=2.8785196150238333e-07, accuracy=1.0\n",
            "Test loss = 8.487874984741211, Test accuracy = 0.7550724744796753\n",
            "Step=44699 loss=2.96978573146589e-07, accuracy=1.0\n",
            "Test loss = 8.488306999206543, Test accuracy = 0.7550724744796753\n",
            "Step=44799 loss=2.913256263070707e-07, accuracy=1.0\n",
            "Test loss = 8.490106582641602, Test accuracy = 0.7550724744796753\n",
            "Step=44899 loss=2.6314402614247e-07, accuracy=1.0\n",
            "Test loss = 8.490775108337402, Test accuracy = 0.7550724744796753\n",
            "Step=44999 loss=2.4143508980500883e-07, accuracy=1.0\n",
            "Test loss = 8.48715591430664, Test accuracy = 0.7550724744796753\n",
            "Step=45099 loss=2.431486859677534e-07, accuracy=1.0\n",
            "Test loss = 8.485658645629883, Test accuracy = 0.7550724744796753\n",
            "Step=45199 loss=2.487086904423563e-07, accuracy=1.0\n",
            "Test loss = 8.487300872802734, Test accuracy = 0.7550724744796753\n",
            "Step=45299 loss=2.5166100179774277e-07, accuracy=1.0\n",
            "Test loss = 8.486906051635742, Test accuracy = 0.7550724744796753\n",
            "Step=45399 loss=2.388180608470236e-07, accuracy=1.0\n",
            "Test loss = 8.491495132446289, Test accuracy = 0.7550724744796753\n",
            "Step=45499 loss=2.3363073339055519e-07, accuracy=1.0\n",
            "Test loss = 8.490803718566895, Test accuracy = 0.7550724744796753\n",
            "Step=45599 loss=2.0871801179822568e-07, accuracy=1.0\n",
            "Test loss = 8.488835334777832, Test accuracy = 0.7550724744796753\n",
            "Step=45699 loss=2.1235944743125402e-07, accuracy=1.0\n",
            "Test loss = 8.490151405334473, Test accuracy = 0.7550724744796753\n",
            "Step=45799 loss=2.0793571316346516e-07, accuracy=1.0\n",
            "Test loss = 8.490303993225098, Test accuracy = 0.7550724744796753\n",
            "Step=45899 loss=2.083734551305838e-07, accuracy=1.0\n",
            "Test loss = 8.492161750793457, Test accuracy = 0.7550724744796753\n",
            "Step=45999 loss=1.7830123969275746e-07, accuracy=1.0\n",
            "Test loss = 8.487593650817871, Test accuracy = 0.7550724744796753\n",
            "Step=46099 loss=1.9027791100967306e-07, accuracy=1.0\n",
            "Test loss = 8.488734245300293, Test accuracy = 0.7550724744796753\n",
            "Step=46199 loss=1.8872264842428876e-07, accuracy=1.0\n",
            "Test loss = 8.489635467529297, Test accuracy = 0.7550724744796753\n",
            "Step=46299 loss=2.00140622013123e-07, accuracy=1.0\n",
            "Test loss = 8.493082046508789, Test accuracy = 0.7550724744796753\n",
            "Step=46399 loss=1.8302301558748012e-07, accuracy=1.0\n",
            "Test loss = 8.493247985839844, Test accuracy = 0.7550724744796753\n",
            "Step=46499 loss=1.722942365844915e-07, accuracy=1.0\n",
            "Test loss = 8.492146492004395, Test accuracy = 0.7550724744796753\n",
            "Step=46599 loss=1.6402411027627294e-07, accuracy=1.0\n",
            "Test loss = 8.49303913116455, Test accuracy = 0.7550724744796753\n",
            "Step=46699 loss=1.5069697225200685e-07, accuracy=1.0\n",
            "Test loss = 8.488093376159668, Test accuracy = 0.7550724744796753\n",
            "Step=46799 loss=1.6852241245146614e-07, accuracy=1.0\n",
            "Test loss = 8.492450714111328, Test accuracy = 0.7550724744796753\n",
            "Step=46899 loss=1.4696239176714697e-07, accuracy=1.0\n",
            "Test loss = 8.48793888092041, Test accuracy = 0.7550724744796753\n",
            "Step=46999 loss=1.498588201087614e-07, accuracy=1.0\n",
            "Test loss = 8.487539291381836, Test accuracy = 0.7550724744796753\n",
            "Step=47099 loss=1.3758400651653347e-07, accuracy=1.0\n",
            "Test loss = 8.480515480041504, Test accuracy = 0.7550724744796753\n",
            "Step=47199 loss=1.4415911213205844e-07, accuracy=1.0\n",
            "Test loss = 8.482763290405273, Test accuracy = 0.7550724744796753\n",
            "Step=47299 loss=1.3660611138277546e-07, accuracy=1.0\n",
            "Test loss = 8.485410690307617, Test accuracy = 0.7550724744796753\n",
            "Step=47399 loss=1.3791928989093093e-07, accuracy=1.0\n",
            "Test loss = 8.484467506408691, Test accuracy = 0.7550724744796753\n",
            "Step=47499 loss=1.2727430444670064e-07, accuracy=1.0\n",
            "Test loss = 8.477313041687012, Test accuracy = 0.7550724744796753\n",
            "Step=47599 loss=1.22478003472537e-07, accuracy=1.0\n",
            "Test loss = 8.479338645935059, Test accuracy = 0.7550724744796753\n",
            "Step=47699 loss=1.1586563486432055e-07, accuracy=1.0\n",
            "Test loss = 8.483918190002441, Test accuracy = 0.7550724744796753\n",
            "Step=47799 loss=1.15101971296383e-07, accuracy=1.0\n",
            "Test loss = 8.484722137451172, Test accuracy = 0.7550724744796753\n",
            "Step=47899 loss=1.1285748044542743e-07, accuracy=1.0\n",
            "Test loss = 8.485040664672852, Test accuracy = 0.7550724744796753\n",
            "Step=47999 loss=9.9101881652075e-08, accuracy=1.0\n",
            "Test loss = 8.478204727172852, Test accuracy = 0.7550724744796753\n",
            "Step=48099 loss=1.0661764394015449e-07, accuracy=1.0\n",
            "Test loss = 8.478203773498535, Test accuracy = 0.7550724744796753\n",
            "Step=48199 loss=1.028178553852399e-07, accuracy=1.0\n",
            "Test loss = 8.479150772094727, Test accuracy = 0.7550724744796753\n",
            "Step=48299 loss=9.614961010484535e-08, accuracy=1.0\n",
            "Test loss = 8.477448463439941, Test accuracy = 0.7550724744796753\n",
            "Step=48399 loss=1.0213800582192789e-07, accuracy=1.0\n",
            "Test loss = 8.478102684020996, Test accuracy = 0.7550724744796753\n",
            "Step=48499 loss=9.751865155394057e-08, accuracy=1.0\n",
            "Test loss = 8.481559753417969, Test accuracy = 0.7550724744796753\n",
            "Step=48599 loss=8.58585226382047e-08, accuracy=1.0\n",
            "Test loss = 8.479191780090332, Test accuracy = 0.7536231875419617\n",
            "Step=48699 loss=8.197491926154043e-08, accuracy=1.0\n",
            "Test loss = 8.472281455993652, Test accuracy = 0.7536231875419617\n",
            "Step=48799 loss=7.856627899371915e-08, accuracy=1.0\n",
            "Test loss = 8.473176002502441, Test accuracy = 0.7536231875419617\n",
            "Step=48899 loss=8.354884986516709e-08, accuracy=1.0\n",
            "Test loss = 8.471511840820312, Test accuracy = 0.7536231875419617\n",
            "Step=48999 loss=7.671295374933607e-08, accuracy=1.0\n",
            "Test loss = 8.469552040100098, Test accuracy = 0.7536231875419617\n",
            "Step=49099 loss=7.855697242931114e-08, accuracy=1.0\n",
            "Test loss = 8.472240447998047, Test accuracy = 0.752173900604248\n",
            "Step=49199 loss=7.641493633414598e-08, accuracy=1.0\n",
            "Test loss = 8.475176811218262, Test accuracy = 0.752173900604248\n",
            "Step=49299 loss=7.500864857235001e-08, accuracy=1.0\n",
            "Test loss = 8.471128463745117, Test accuracy = 0.752173900604248\n",
            "Step=49399 loss=7.693648248263684e-08, accuracy=1.0\n",
            "Test loss = 8.469654083251953, Test accuracy = 0.752173900604248\n",
            "Step=49499 loss=6.77070927768142e-08, accuracy=1.0\n",
            "Test loss = 8.466496467590332, Test accuracy = 0.752173900604248\n",
            "Step=49599 loss=5.759294092300138e-08, accuracy=1.0\n",
            "Test loss = 8.465062141418457, Test accuracy = 0.752173900604248\n",
            "Step=49699 loss=6.023789335785068e-08, accuracy=1.0\n",
            "Test loss = 8.460494041442871, Test accuracy = 0.752173900604248\n",
            "Step=49799 loss=6.039621819553531e-08, accuracy=1.0\n",
            "Test loss = 8.4658842086792, Test accuracy = 0.7507246136665344\n",
            "Step=49899 loss=5.6438106277312273e-08, accuracy=1.0\n",
            "Test loss = 8.462470054626465, Test accuracy = 0.7507246136665344\n",
            "Step=49999 loss=5.5124941704676186e-08, accuracy=1.0\n",
            "Test loss = 8.461015701293945, Test accuracy = 0.7507246136665344\n",
            "Reached 50001 epochs for CNN\n",
            "[[103 151]\n",
            " [ 21 415]]\n",
            "Normalized confusion matrix\n",
            "[[0.40551181 0.59448819]\n",
            " [0.04816514 0.95183486]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEmCAYAAADbUaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nssGCqDHGZIMFUWOMyYb/A+Rem2ulvz4PAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VOW9x/HPb7InZCMJGAirIFtY\nDYjgAlJExIq4VUWtXBV7W7darVptrVVvcbnV2qpVW/UKFhcUF3CvKK5g2GTfA1mAbGTfZnnuH+eA\nMQKZQCaz/d6v17wyZ5k5vzOZfOfJM+ecR4wxKKWUCh4OfxeglFKqbTS4lVIqyGhwK6VUkNHgVkqp\nIKPBrZRSQUaDWymlgowGt2pXIvKpiFzj7zraSkR6i4gRkUh/16JUazS4w4CI5InIT9rhea4SkS/a\noyal1NHT4FYqwIlF/1bVQfpmCHEiMhfoCbwjIjUi8lt7/lgR+UpEKkRkjYhMaPaYq0Rkh4hUi8hO\nEZkpIoOAfwAn289T4cW2HSJyt4jsEpFiEXlRRJLtZbEiMk9EyuwavhWRrofb/mGef4yIfG0/fo+I\n/F1EopstNyLyCxHZaq/zhIiIvSxCRB4RkVIR2QFMa2Vf7hCR7XZNG0RkRovl14rIxmbLR9nze4jI\nGyJSYu/r3+35fxSRec0e/4OuGrvL6QER+RKoA/qKyKxm29ghIte1qGG6iKwWkSq71rNE5CIRWdFi\nvVtE5K0j/vJUYDPG6C3Eb0Ae8JNm092BMuBsrA/vyfZ0BpAAVAED7HUzgSH2/auAL1rZ1qfANfb9\n/wK2AX2BTsAbwFx72XXAO0A8EAGcCCQdafuH2NaJwFggEugNbARubrbcAIuAFKwPrxLgLHvZL4BN\nQA+gM7DEXj/yMNu6COhmv14/A2qBzGbLCoHRgAD9gF72fq0BHrX3KxY4xX7MH4F5zZ6/d/Pt26/j\nbmCIvX9RWB8ux9vbOB0r0EfZ648BKu3fpcP+HQ8EYoByYFCzba0CLvD3+1JvR3/TFnd4uhx41xjz\nrjHGY4z5CMjFCnIAD5AtInHGmD3GmPVHuZ2ZwF+MMTuMMTXAncAldqvSCaQB/YwxbmPMCmNMVVu2\nbz/mG2OMyxiTBzyNFWjNzTHGVBhjdmOF8wh7/sXAY8aYfGNMOfDnI+2IMeY1Y0yR/Xq9AmzFCkuA\na4CHjDHfGss2Y8wue3k34DZjTK0xpsEY05bvCF4wxqy3989pjFlsjNlub+Mz4EPgVHvdq4HnjDEf\n2TUWGmM2GWMagVewfueIyBCsD4lFbahDBRgN7vDUC7jI7j6osLs9TsFqQdZitSh/AewRkcUiMvAo\nt9MN2NVsehdW67ErMBf4AHhZRIpE5CERiWrL9kXkBBFZJCJ7RaQK+B8gvcVqe5vdr8Nq+R+oLb9F\nbYclIlfa3RAHXq/sZtvqAWw/xMN6ALuMMa4jPfcRNK8PEZkqIt+ISLldw9le1ADwf8BldjfRFcCr\ndqCrIKXBHR5aXgIyH6vLIqXZLcEYMwfAGPOBMWYyVjfFJuDZwzxPa4qwPiQO6Am4gH12C/JeY8xg\nYBxwDnBlK9tv6Sl7eX9jTBLwO6xuBG/swQq75rUdkoj0smu4HkgzxqQA65ptKx+rC6OlfKDnYQ4x\nrMXqJjrguEOsc/D1FpEY4HXgEaCrXcO7XtSAMeYboAmrdX4Z1oemCmIa3OFhH1Y/8wHzgJ+KyBT7\nS7pYEZkgIlki0tX+kisBaARqsLouDjxPVvMvAFsxH/i1iPQRkU5YLeJXjDEuEZkoIkNFJAKrT9sJ\neFrZfkuJ9mNr7Fb5f3v7ggCvAjfa+5wK3HGEdROwQrQEQERmYbW4D/gncKuInCiWfnbYL8f6gJgj\nIgn26zzefsxq4DQR6SnWF7Z3tlJvNFZ/dQngEpGpwJnNlv8LmCUik8T6Urh7i/9UXgT+Djjb2F2j\nApAGd3j4M3C3/W/+rcaYfGA6Vgu1BKu1dhvW+8EB3ILVWi7H6jM+EIifAOuBvSJS6sV2n8Nq3S0F\ndgINwA32suOABVjBuxH4zF73SNtv6VasFmQ1Vov4FS9qOuBZrK6aNcBKrC9OD8kYswH4X+BrrA+v\nocCXzZa/BjwA/Nuu5U2gszHGDfwU68vK3UABVjcQ9vcKrwDfAStopc/ZGFMN3Ij1gbPf3u+3my1f\nDszC+iK0Euv1bP7fzlysD5t5qKAnxuhACkqFOhGJA4qxjkLZ6u961LHRFrdS4eG/gW81tEODV9dl\nEJGbgGuxvgh51hjzmE+rUkq1GxHJw/rbPc/Ppah20mpXiYhkAy9jHZPaBLwP/MIYs8335SmllGrJ\nm66SQcAyY0ydfTzqZ8D5vi1LKaXU4XjTVbIOeEBE0oB6rIP+c1uuJCKzgdkACQkJJw4ceLTnbCil\nVPhZsWJFqTEmw5t1vTqqRESuBn6JddLAeqDRGHPz4dbPyckxubk/ynallFKHISIrjDE53qzr1VEl\nxph/GWNONMachnUM6ZZjKVAppdTR8/aoki7GmGIR6YnVvz3Wt2UppZQ6HG+HaXrd7uN2Ar8yxrR6\nLWallFK+4VVwG2NObX2tI3M6nRQUFNDQ0HCsT6XaUWxsLFlZWURFRfm7FKWUlzpsYNSCggISExPp\n3bs31tUllb8ZYygrK6OgoIA+ffr4uxyllJc67JT3hoYG0tLSNLQDiIiQlpam/wUpFWQ69FolGtqB\nR38nSgUfvciUUqrD5JfX8faaIn+XEfTCJrgrKip48sknj/rxjz32GHV1de1YkVLhpcnl4dSHlnDj\n/FU0uQ43Nobyhga3lwIhuF2uox26UCn/K9j//d9PfZPbp9vyeAxOt4cGp5vaRheV9U4aXdY2nW4P\nVQ1OKuud1De5CcYxCTrsqBJ/u+OOO9i+fTsjRoxg8uTJPPzwwzz88MO8+uqrNDY2MmPGDO69915q\na2u5+OKLKSgowO128/vf/559+/ZRVFTExIkTSU9PZ8mSJT947j/96U+888471NfXM27cOJ5++mlE\nhG3btvGLX/yCkpISIiIieO211zj++ON58MEHmTdvHg6Hg6lTpzJnzhwmTJjAI488Qk5ODqWlpeTk\n5JCXl8cLL7zAG2+8QU1NDW63m8WLFzN9+nT279+P0+nk/vvvZ/r06QC8+OKLPPLII4gIw4YN48kn\nn2TYsGFs2bKFqKgoqqqqGD58+MFppTpSWW3TwfvbS2twujwkxUVR1+SmvslNXZOLuiY3NY0uGpxu\nquqdFFc3UlzdSGlNIx5jaHJ5qGlwUd3goqbJxYHMjXAIEQ7B4zG4PIcOYodY3+m4WyzvFBNJVmoc\nTreHCIeQEhfN/rom3B5DZkosxoDHGBwi9E5PIC4qApfbQ02jm+OSY8joFEN8dCRx0REkxUVx+gle\nXW7kmPgluO99Zz0biqra9TkHd0vinp8OOezyOXPmsG7dOlavXg3Ahx9+yNatW1m+fDnGGM4991yW\nLl1KSUkJ3bp1Y/HixQBUVlaSnJzMX/7yF5YsWUJ6estBxOH666/nD3/4AwBXXHEFixYt4qc//Skz\nZ87kjjvuYMaMGTQ0NODxeHjvvfd46623WLZsGfHx8ZSXl7e6bytXruS7776jc+fOuFwuFi5cSFJS\nEqWlpYwdO5Zzzz2XDRs2cP/99/PVV1+Rnp5OeXk5iYmJTJgwgcWLF3Peeefx8ssvc/7552toK78o\nq/l+YPnzn/yq1fVFIC0hhq5JMaR3iiHCIURHOOgUG0libCQJ0ZE4HNaX626PB5fHEOkQIkRwOIRI\nR7OfIlQ3uHB7DLFRDmKjInB7rA+Cstom8svriI2KoLbJRX2Tm35dOuHyGMprmw4Gfr3TzXtr99Do\n8hAhQqfYSIqrG3/wQZDeKYbcu3/S/i9eC2HT4m7pww8/5MMPP2TkyJEA1NTUsHXrVk499VR+85vf\ncPvtt3POOedw6qmtn3u0ZMkSHnroIerq6igvL2fIkCFMmDCBwsJCZsyYAVgnugB8/PHHzJo1i/h4\na4Dvzp07t/r8kydPPrieMYbf/e53LF26FIfDQWFhIfv27eOTTz7hoosuOvjBcmD9a665hoceeojz\nzjuP559/nmefPdyA6Ur5VmlN0w+mrz6lDzm9UomLjiA+OpL46AjioiNIjIkkJiqChOgIIiMCuzfX\n7TFU1jupswPf6e6Ybhe/BPeRWsYdxRjDnXfeyXXXXfejZStXruTdd9/l7rvvZtKkSQdb04fS0NDA\nL3/5S3Jzc+nRowd//OMfj+q46MjISDwez8HnbC4hIeHg/ZdeeomSkhJWrFhBVFQUvXv3PuL2xo8f\nT15eHp9++ilut5vs7OzDrquUL5W1CO4rxvaid3rCYdYODhEOoXNCNJ0Tojt0u4H9cdaOEhMTqa6u\nPjg9ZcoUnnvuOWpqagAoLCykuLiYoqIi4uPjufzyy7nttttYuXLlIR9/wIHQTE9Pp6amhgULFhxc\nPysrizfffBOAxsZG6urqmDx5Ms8///zBLzoPdJX07t2bFStWABx8jkOprKykS5cuREVFsWTJEnbt\n2gXAGWecwWuvvUZZWdkPnhfgyiuv5LLLLmPWrFltfdmUajdltY0/mE6MDdt/+I9Z2AR3Wloa48eP\nJzs7m9tuu40zzzyTyy67jJNPPpmhQ4dy4YUXUl1dzdq1axkzZgwjRozg3nvv5e677wZg9uzZnHXW\nWUycOPEHz5uSksK1115LdnY2U6ZMYfTo0QeXzZ07l8cff5xhw4Yxbtw49u7dy1lnncW5555LTk4O\nI0aM4JFHHgHg1ltv5amnnmLkyJGUlpYedj9mzpxJbm4uQ4cO5cUXX+TAgBVDhgzhrrvu4vTTT2f4\n8OHccsstP3jM/v37ufTSS9vt9VSqrVq2uDtpcB81rwZSaKtDDaSwceNGBg0a1O7bUq1bsGABb731\nFnPnzj3kcv3dqI4w9a+fU9voYne59d9m3pxpfq4osLRlIAX9yAtxN9xwA++99x7vvvuuv0tRYazB\n6WbLvmrOH9n9YHCro6fBHeL+9re/+bsEpdiwpwq3x3BS3zReW1Hg73KCXtj0cSul/Oe7fGvslZP6\ntH74q2qdV8EtIr8WkfUisk5E5otIrK8LU0qFju8KK0nvFENWapy/SwkJrQa3iHQHbgRyjDHZQARw\nia8LU0qFjrUFlQzPStbLCLcTb7tKIoE4EYkE4gG9LqNSyiu1jS62ldQwNCv54Lz0Th17wkqoaTW4\njTGFwCPAbmAPUGmM+bDleiIyW0RyRSS3pKSk/Ss9Rr68OuCECRNoefijUsqyrrASY2CYHdyf/3Yi\nH99yup+rCm7edJWkAtOBPkA3IEFELm+5njHmGWNMjjEmJyPD91fHaqtQuKyrUsHou4JKAIZ2TwGg\nR+d4UuK1xX0svOkq+Qmw0xhTYoxxAm8A43xbVvtrflnX2267DYCHH36Y0aNHM2zYMO655x4Aamtr\nmTZtGsOHDyc7O5tXXnmFxx9//OBlXVueOdnS/PnzGTp0KNnZ2dx+++0AuN1urrrqKrKzsxk6dCiP\nPvooAI8//jiDBw9m2LBhXHKJfm2gQtNnW0rok55ARmKMv0sJGd4cx70bGCsi8UA9MAk4tn6B9+6A\nvWuP6Sl+5LihMHXOYRf78rKuBxQVFXH77bezYsUKUlNTOfPMM3nzzTfp0aMHhYWFrFu3DrBa/wdq\n2rlzJzExMQfnKRVKymub+HpHGded1tffpYQUb/q4lwELgJXAWvsxz/i4Lp9rflnXUaNGsWnTJrZu\n3crQoUP56KOPuP322/n8889JTk5u/cls3377LRMmTCAjI4PIyEhmzpzJ0qVL6du3Lzt27OCGG27g\n/fffJykpCYBhw4Yxc+ZM5s2bR2SkngulQs+H6/fi9hjOHprp71JCildpYYy5B7in3bZ6hJZxR2mv\ny7p6IzU1lTVr1vDBBx/wj3/8g1dffZXnnnuOxYsXs3TpUt555x0eeOAB1q5dqwGuQso73xXRs3M8\nQ7ol+buUkBI2Z0766rKuzY0ZM4bPPvuM0tJS3G438+fP5/TTT6e0tBSPx8MFF1zA/fffz8qVK/F4\nPOTn5zNx4kQefPBBKisrD9aiVCjYVlzNl9vKuOjELD1+u52FTfOu+WVdp06dysMPP8zGjRs5+eST\nAejUqRPz5s1j27Zt3HbbbTgcDqKionjqqaeA7y/r2q1btx+NOXlAZmYmc+bMYeLEiRhjmDZtGtOn\nT2fNmjXMmjXr4EAJf/7zn3G73Vx++eVUVlZijOHGG28kJSWlY14MpTrAp5utw4IvHt3Dz5WEHr2s\nq9LfjfKJW15dzRdbS1l+l+/HYAwFbbmsa9h0lSilOtbGPdUMzNS+bV/Q4FZKtbvqBifbiqv1S0kf\n6dDg9kW3jDo2+jtRvvCfjcU43YZJA7v4u5SQ1GHBHRsbS1lZmQZFADHGUFZWRmysXqVXtZ9Xvt3N\nza+spntKHKN6pvq7nJDUYUeVZGVlUVBQQCBegCqcxcbGkpWV5e8yVAi5/XXrrOibf9Ifh0MPA/SF\nDgvuqKgo+vTp01GbU0r5QUWdNZL7VeN6c1GOHgboK/rlpFKq3WzZZ51ENmFA4F0hNJRocCul2s2n\nm4sBGHBcop8rCW0a3EqpdrEmv4Knl+7gvBHdyEzWsSV9SYNbKdUu/vejLaQlRHPv9Gx/lxLyNLiV\nUsesoq6Jr7aVcv6oLJLjovxdTsjT4FZKHROPx/DbBd/hMYbpI7r5u5yw4M2YkwNEZHWzW5WI3NwR\nxSmlApsxhgc/2MSHG/Zx97TBDNJrk3SIVo/jNsZsBkYAiEgEUAgs9HFdSqkg8Pu31jHvm91cOqYH\ns8b39nc5YaOtXSWTgO3GmF2+KEYpFTy27Kvm5eX5nD+yO/efN1QHS+hAbQ3uS4D5h1ogIrNFJFdE\ncvW0dqVC27xvdjHlsaV0io3k1ikDiNBT2zuU18EtItHAucBrh1pujHnGGJNjjMnJyNCzppQKRTWN\nLv72n608+N4mhnZPZvGNp9ItRY/Z7mhtuVbJVGClMWafr4pRSgWuRpebK/61jFW7KxjTpzMPXziM\n7hraftGW4L6Uw3STKKVCm9tjuGvhOlbtruCJy0YxbVimv0sKa14Ft4gkAJOB63xbjlIq0DQ43dz8\n8mreX7+Xmyb119AOAF4FtzGmFkjzcS1KqQCzfGc59y3awNrCSn5/zmCuPkUvzRwIOux63EoFij2V\n9XoRpFZ4PIZ/fbGT//1oMwnRkfz1khFMH9Hd32Upm57yrsLKNzvKOPnPn/D3T7bqMHqHsbusjuvn\nr+SBdzcypk8a7910qoZ2gNEWtworH2+wDop65MMtFFU28IdzBhMbFeHnqgJDSXUjN85fxdc7ygD4\nr/F9+P05g/TEmgCkwa3Cytc7yhjZM4Uh3ZKY981u3lu7h2euzGF0787+Ls0v9lU1sGRTMa/k5rNq\ndwWRDuGqcb2ZeVJP+nfVwRAClQa3Cit7KxuYkn0c903PZmp2Jne/uY6fPf01/bskkpoQxcl90xnX\nL40+6QmkJUSHXGvTGMP2khqWbCph2c5ylm4tocnloVtyLDdO6s+0oZk6ek0Q0OBWYcMYQ2W9k+S4\nKESE8f3SeXn2WOZ9s4vV+RVU1Dl59OMtPPqxtX5MpIPjMzqR1imaId2SOW9kN1Ljo4mOcJCaEO3f\nnWmFMYay2iZ2ldWxu7yWrftqyN21n4LyOooqGwDok57A+SO78/NxvTmha6Keth5ENLhV2KhrcuPy\nmB9c6L9rUiy/OXPAwekt+6rZvLeaoop69lU1srawgl1ldXy1vYx/fLa92eNiyO6WTOeEaDISY0iK\niyI+OoJeaQn0SUugW0osEQ7B6TZER1rHABhjjqkFf+DxLreHPZUNVDe4cHk8FOyvp77JTVltI5v2\nVrNlXzV5pXXUNLoOPjbCIWR3T2Zkz1R+1S+NiQO66KnqQUyDW4WNynonwBFHaDmhayInHKJvt7i6\ngU83l+ByG+qaXGwoqmJ9URVrCyspq23C7fnhESqRDiEqwkGjy02f9ATqm9zsq26kW0oskQ4HTS4P\nkRFCVmqcFe4RDmKjHMREReAQYXdZLcnx0VTVOympbmR/XRPGQFSEUNXg+lF9B3RNimHAcUnk9OpM\nz87x9EqLp1daAlmpcfolbAjR4FZhw5vgPpwuibFcnNPjkMtcbg/VDS6cbg95ZXXkldaSV1ZLg9ND\nbJSDTXurSYmLIq1TNHurGgGIjnDQ4HJTuL+emEgHdU0uyms9NLjcNDo9ZKXGUVHXRFJsFH3SE+ic\nEI3T7QEgJS6K7qlxJMVGYYCs1DiS46JIjosiJT6wu3BU+9DgVmHjWIL7SCKb9Xl3SYplTJ/wPEJF\ndRw9AUeFDV8Ft1IdTYNbhQ0NbhUqNLhV2KiygztJg1sFOQ1uFTb2VjYQE+kgKVa/2lHBTYNbhY2C\n/fVkpcaF3NmQKvx4FdwikiIiC0Rkk4hsFJGTfV2YUu2toKKOrNR4f5eh1DHztsX9V+B9Y8xAYDiw\n0XclKeUb+eX19OisZwuq4NdqZ5+IJAOnAVcBGGOagCbflqVU+6pqcFJZ76R7ira4VfDzpsXdBygB\nnheRVSLyT3sMyh8QkdkikisiuSUlJe1eqFLHIq+0FoA+6RrcKvh5E9yRwCjgKWPMSKAWuKPlSsaY\nZ4wxOcaYnIyMjHYuU6ljs9MO7r4ZnfxciVLHzpvgLgAKjDHL7OkFWEGuVNDYXlKLCPRK0xa3Cn6t\nBrcxZi+QLyIHrn05Cdjg06qUamc7S2vJSo0jJlKvkKeCn7dnItwAvCQi0cAOYJbvSlKq/W0rrqGf\ndpOoEOFVcBtjVgM5Pq5FKZ9we6zhuk7tn+7vUpRqF3rmpAp5+eV1NLk89OuiLW4VGjS4VcjbWlwD\nQH8NbhUiNLhVyNtaXA2gLW4VMjS4Vcjbtq+GzORYEmP1cq4qNGhwq5C3tbhGW9sqpGhwq5BW3+Rm\n875qBmUm+bsUpdqNBrcKacvzymlyeRjfTw8FVKFDg1uFtOU7y4h0CGN668jrKnRocKuQlldaR4/O\n8cRF66nuKnRocKuQtrvcCm6lQokGtwppu8pq6aXBrUKMBrcKWftrm6hqcNFTg1uFGA1uFbLeXlME\nwKheqX6uRKn2pcGtQtabqwsZlpXMiRrcKsRocKuQ5PYYNu6pYrQeBqhCkAa3Ckk7S2tocHoYrGdM\nqhDk1UAKIpIHVANuwGWM0UEVVEBbnV8JwJDuGtwq9Hg7dBnARGNMqc8qUaodLdlUTEZiDCd0SfR3\nKUq1O+0qUSHH4zEs3VrCGQO64HCIv8tRqt15G9wG+FBEVojI7EOtICKzRSRXRHJLSkrar0Kl2mhX\neR3VDS49mkSFLG+D+xRjzChgKvArETmt5QrGmGeMMTnGmJyMjIx2LVKptthQVAXA4G7av61Ck1fB\nbYwptH8WAwuBMb4sSqljsa6okkiH0L+rDp6gQlOrwS0iCSKSeOA+cCawzteFKXW0Pt1cwsieKcRE\n6hUBVWjypsXdFfhCRNYAy4HFxpj3fVuWUkdnW3ENG/dUMWXIcf4uRSmfafVwQGPMDmB4B9Si1DF7\n9KMtJERHMH1Ed3+XopTP6OGAKmTUNrr4aMM+Lh7dg4zEGH+Xo5TPaHCrkPH51lKa3B4mD+7q71KU\n8ikNbhUyPt64j6TYSL2wlAp5GtwqJFTWO/lkUzETBnQhKkLf1iq06TtcBT2X28P1/15JVb2Tn4/r\n5e9ylPK5tlxkSqmA9P76vXy+tZQHZmRzYi/tJlGhT1vcKui9vqKAzORYLhnd09+lKNUhNLhVUKtv\ncvPltjLOHppJhF4JUIUJDW4V1N5ZU0ST28PpJ+iFzVT40OBWQau0ppG731rHcUmxjOmjfdsqfGhw\nq6Dkcnu45631NLk8PDFzFLFRekEpFT40uFVQ+r+vd7F47R4cAiN7pPi7HKU6lAa3CjoNTjf/+Gw7\ncVERLPzleB2eTIUdPY5bBZVGl5vr/72SkupG5l87luHa2lZhSFvcKqg8uWQ7H28s5lcTj2dsX/1C\nUoUnr4NbRCJEZJWILPJlQUodzrrCSp78dBvnjejGbVMGIqJdJCo8taXFfROw0VeFKHUkRRX1XPN/\nuaQlxPD7cwb7uxyl/Mqr4BaRLGAa8E/flqPUj5VUNzL9iS+paXTx/KzRpHXSQRJUePO2xf0Y8FvA\nc7gVRGS2iOSKSG5JSUm7FKeUx2P4y0dbKK1p5KVrTmJQZpK/S1LK77wZ5f0coNgYs+JI6xljnjHG\n5BhjcjIy9PRjdeyMMdz15lrmL9/N+SOz9AgSpWzeHA44HjhXRM4GYoEkEZlnjLnct6WpcNbocvPI\nB5uZvzyf/55wPL+dMsDfJSkVMLwZ5f1O4E4AEZkA3KqhrXxpdX4Fv3ppJYUV9Vx2Uk9+O2WAHkGi\nVDN6Ao4KKBv3VPGzp78mIzGGp2aO4swhx2loK9VCm4LbGPMp8KlPKlFhb8mmYm5+ZTVJcVEs/OV4\nMhL16BGlDkVb3Mrv6pvcPPrxFv75+Q4GHpfEkzNHaWgrdQQa3MqvPtqwjz8tWk/B/npmjOzO/edl\nEx+tb0uljkT/QpRfNDjdPLFkG3/7ZBsDuiby0jUnMe74dH+XpVRQ0OBWHW7T3ipufnk1m/ZWc+GJ\nWfzPjKFER+r1zpTylga36jBvryniz+9uZE9lAw6Bx342gukjuulRI0q1kQa36hCLv9vDjfNXAXDJ\n6B78amI/enSO93NVSgUnDW7lU98VVPDB+r08sWQ7XZNi+MvFIxh3fJq2spU6BhrcyifWF1Xy7NId\nvLWmCGPglH7p/PPnOTqor1LtQINbtav1RZX8+pXVbNlXQ2JsJDNP6skFo7IYnpWiY0Mq1U40uNUx\nM8awpqCShSsLmPvNLmKjIrhibC9unNRfT6RRygc0uNVRc3sMq3bv58H3N/Ft3n6iIoQLT8zirrMH\nkxwf5e/ylApZGtyqzYwxfLqlhD++vZ5dZXV0ionkvvOyOXd4N5LjNLCV8jUNbuW1BqebzXurmfPe\nJr7eUUbvtHgevGAo445P10P7lOpAGtzKKwX767jiX8vZWVpLdISD+6YP4Weje+oZj0r5gQa3OiyP\nx/Dl9lL+vWw3SzYXExXh4LatNP4UAAAPyklEQVQpAzhnWCa90hL8XZ5SYavV4BaRWGApEGOvv8AY\nc4+vC1P+43J7mL98N08v3UHB/nqSYiP5WU4Prji5N/26dPJ3eUqFPW9a3I3AGcaYGhGJAr4QkfeM\nMd/4uDblBwtXFXDfoo2U1zbRs3M8d08bxMWje5AUq186KhUovBlz0gA19mSUfTO+LEp1rM17q/lg\n/V5Kaxp58etd5PRK5c/nD+Ung7oSoSfNKBVwvOrjFpEIYAXQD3jCGLPMp1Upn/N4DOV1TXy2uYQ7\nF66lyeUBrAtA3XdeNlER+qWjUoHKq+A2xriBESKSAiwUkWxjzLrm64jIbGA2QM+ePdu9UNV+nG4P\nd7y+ltdXFgAw7vg0Hr5oOI1ON33SE/QCUEoFuLYOFlwhIkuAs4B1LZY9AzwDkJOTo10pAaqyzsms\nF5azcncFANOGZvLQhcNIiNEDjJQKFt4cVZIBOO3QjgMmAw/6vDLVrjbtrWLhqkJeX1FARZ2TRy4a\nzgWjumvrWqkg5E0zKxP4P7uf2wG8aoxZ5NuyVHvZuKeKv32ylXfX7gVg4oAMbv7JCQzvkeLnypRS\nR8ubo0q+A0Z2QC2qnZTVNPLu2j18sH4fX24vJSbSwaDMJP526Ug9DlupEKAdmyFmZ2ktFzz1FeW1\nTaTGR3HtqX355YTjSYmP9ndpSql2osEdIirqmvg2bz8PLN6A0+3hT9OHMG1oJmmd9HrYSoUaDe4g\nZoyhYH89L3yVx9xvdtHk8nBcUix/v2wUp5+Q4e/ylFI+osEdhFxuD2sLK3no/c18vaMMgIHHJfKz\n0T2Ymp3Jccmxfq5QKeVLGtxBJq+0lpteXsWagkqiIx1cd3pfTumXzvAeKXo9EaXChAZ3kMgrreWv\n/9nKwlWF1ogz04dwVnamjumoVBjS4A5w+eV1fLKpmGc/ty6xetW43lxzah+yUnXEGaXClQZ3AFtb\nUMnFT39NvdMNwCuzx3JS3zQ/V6WU8jcN7gBUWFHPnW+s5fOtJWR0iuGOqQPJ7p7Mib1S/V2aUioA\naHAHkG/zyrlr4Vq27KshMSaSG8/oz8yxPemSqEeJKKW+p8EdAL7eXsajH2/h27xyslLjuGpcb342\nugeDMpP8XZpSKgBpcPtRXmktty1Yw7d5++mWHMuscX349eT+JOphfUqpI9Dg7mB1TS4+31rK3K93\n8dX2UhKiI7nr7EFcPrYXcdER/i5PKRUENLg70Afr93Lj/FU0ujx0T4lj9mnH81/je9MlSfuwlVLe\n0+DuAAtWFPDSsl2s2l3BiB4pXD+xH6f0Tyc2SlvYSqm20+D2oYq6Jl74Ko/HPt5Kvy6dOH9kd+45\ndwjJcdqHrZQ6et4MXdYDeBHoChjgGWPMX31dWDArq2nk4Q828/K3+QDMGNmdhy4cpiOnK6XahTct\nbhfwG2PMShFJBFaIyEfGmA0+ri3oVNY5mbdsF09/tp26JjcXnZjF2L5pzBjZHYdDx3ZUSrUPb4Yu\n2wPsse9Xi8hGoDugwd3Mqt37uW7uCoqrG5kwIIO7pw2iX5dEf5ellApBberjFpHeWONPLjvEstnA\nbICePXu2Q2nB4buCCh5YvJFv88rpnhrH29ePZ1iWDsSrlPIdr4NbRDoBrwM3G2OqWi43xjwDPAOQ\nk5Nj2q3CALWusJJXc/N5NTcfQTh3eDfu+ekQUhN0bEellG95FdwiEoUV2i8ZY97wbUmBrarByYLc\nAu5bvAFjrJFn5l59kl4XWynVYbw5qkSAfwEbjTF/8X1JgWtXWS3XvpjLln01dE6I5qZJ/Zk2LJN0\nHZBXKdWBvGlxjweuANaKyGp73u+MMe/6rqzAsq+qgQUrCvjHp9txOIRZ43tz2Zie9O+qXz4qpTqe\nN0eVfAGE5bFsZTWNvLOmiCc/3U5xdSOp8VG8ff0p9Oiso88opfxHz5w8jMp6J1MeW0ppTRMDj0vk\nzCFdmXlSLw1tpZTfaXC34PEYXsnN5w9vrcPpNvz72pMYd3y6v8tSSqmDNLhtbo+htKaRG+avYvnO\nckb2TOGiE3toaCulAo4GN9DocnPu375k875q4qIieOiCYVyUk4V1QI1SSgWWsA/uV3PzuW/RBqob\nXPRJT+DJmaN0yDClVEAL2+BeW1DJP7/YwTtrihjZM5VrTunD1KGZ/i5LKaVaFXbB7fEY3lhVyJz3\nNlJa08SkgV14/NKRJMSE3UuhlApSYZVW+eV1PPzBZt5eU8TgzCRenj1Wr+CnlAo6YRPc877ZxR/f\nXo/HGG6a1J+bf9Jfv3xUSgWlkA/uvZUNPP/VTp5duoPTT8jggRlD6ZYS5++ylFLqqIV0cJfWNHLl\nc8vYXlLLGQO78NglI+mkfdlKqSAXsilWXNXAjCe/Yk9lPc9emcOkQV39XZJSSrWLkAzu7SU1zHji\nS2qb3My7Rk9ZV0qFlpAKbpfbw+/fWs+CFfnER0fy+n+PY0QPHUZMKRVaQia4jTH8buFaXs0tYOZJ\nPbn21L70Tk/wd1lKKdXuvBkB5zngHKDYGJPt+5Lazu0x/M+7G3k1t4Abz+jHLWcO8HdJSinlMw4v\n1nkBOMvHdRy1gv11TP3rUv71xU5+fnIvfj35BH+XpJRSPuXNCDhLRaS370tpG7fHsHRrCXe+vpba\nJhd/v2wk04Zm6kk1SqmQ12593CIyG5gN0LNnz/Z62sN6/sud3L94IwCLbjiF7O7JPt+mUkoFAm+6\nSrxijHnGGJNjjMnJyMhor6c9pPLaJp7/Mg+AJy4bpaGtlAorQXdUSW2ji+vm5lJa08jLs8cytm+a\nv0tSSqkOFVTBbYzhsn8uY01+BQ/MyNbQVkqFpVa7SkRkPvA1MEBECkTkap9W5GqEqiIo3QrG/GDR\nC1/lsSa/gpsm9WfmSb18WoZSSgUqb44qubQjCgEg70uYfwk0VlnTGYMgqRtEx7PfFU3XTfk8m9iJ\niQ194aMEcDZAdIJ1i4gCR5T9M/L7n45I6wPA3QhuJ0TFQXQniOkE0YkQkwgi4Ky3HiMHPsvso1N+\ncJTKoea1ts5hpr1Z57DPK+CIsPYnNgncTeBxW7eoOOv1cDut9T0ua7+QQz9/MB+F4/FY+xcZ/cP5\nbic4675/TYz90+ME44HETGsdt9NaBi0aCaZt834w39t5h3tOb9fzxbYPM+/A62w81rQxXv7k8Mvr\nyr7/OwcO+/485DStLD/SdFvWbWstApGxMPBsfC1wukpcTbDoZitUT7vV+kPb+RnU76e+bDfOynIG\nRMTSJ05wbFgFTbXWi9RU+/0fn7JIxPd/ZG17oP2j2Zu01Td/Wx/jxXbEYX3YGGN9KLntwAXrAysi\nGhoqrOUep/UBHBFph3GTdVPKHxK6hFlwe5zQdyL0mwQnTLHmnXoLNY0uxs/5hMTYSP73ouEc37Jf\n2xire8Xjsp7DfeCns1krAeuPPSLKalk31UBjNTTWWPeNx2qpup183zLgx9ux7hx62pt1jti6aePz\nupusgHbWWvvmsP9bcNZZLRmJsP7biIy2XpNDPf/h7h9c72gec4ia27od47ZqFvn+9yYR1noel/U7\njO9srRudCPXl9n8W0dY+RydYv09HlBX0jojvXw9XAzRUfv+8jojvaz7Sf0Otzms239t5R72dDt62\nIxIcDvt3cGAdOYqffD8dlwqxKdb9Q77XaDHt5d/LkaaP+rFe1HLg/g/eT74TOMEdnQBnP/Sj2XO/\n3kVlvZMXZo1mZM/UHz9OBKJiO6BApZQKDO12HLcvrNy9n79/spUzBnY5dGgrpVQYCpjgrml0cdXz\ny/n3st0A7KmsZ/aLuaQnxnDfeQF5bSullPKLgAnuhOgI9lU18tKyXWzcU8WMJ76iwenh6StOpLuO\nEamUUgcFTHCLCJeN6cH6oiqm/vVzDIZXrzuZgccl+bs0pZQKKIHz5SRw6ZieNDg9FFXWM/u0vmQm\na0tbKaVaCqjgjoxwcO1pff1dhlJKBbSA6SpRSinlHQ1upZQKMhrcSikVZDS4lVIqyGhwK6VUkNHg\nVkqpIKPBrZRSQUaDWymlgoyYQ117+lifVKQE2HWUD08HStuxnGCg+xwedJ/Dw9Hucy9jTIY3K/ok\nuI+FiOQaY3L8XUdH0n0OD7rP4aEj9lm7SpRSKshocCulVJAJxOB+xt8F+IHuc3jQfQ4PPt/ngOvj\nVkopdWSB2OJWSil1BBrcSikVZAImuEXkLBHZLCLbROQOf9fTXkTkOREpFpF1zeZ1FpGPRGSr/TPV\nni8i8rj9GnwnIqP8V/nRE5EeIrJERDaIyHoRucmeH7L7LSKxIrJcRNbY+3yvPb+PiCyz9+0VEYm2\n58fY09vs5b39Wf+xEJEIEVklIovs6ZDeZxHJE5G1IrJaRHLteR363g6I4BaRCOAJYCowGLhURAb7\nt6p28wJwVot5dwD/Mcb0B/5jT4O1//3t22zgqQ6qsb25gN8YYwYDY4Ff2b/PUN7vRuAMY8xwYARw\nloiMBR4EHjXG9AP2A1fb618N7LfnP2qvF6xuAjY2mw6HfZ5ojBnR7Hjtjn1vG2P8fgNOBj5oNn0n\ncKe/62rH/esNrGs2vRnItO9nApvt+08Dlx5qvWC+AW8Bk8Nlv4F4YCVwEtYZdJH2/IPvc+AD4GT7\nfqS9nvi79qPY1yysoDoDWARIGOxzHpDeYl6HvrcDosUNdAfym00X2PNCVVdjzB77/l6gq30/5F4H\n+9/hkcAyQny/7S6D1UAx8BGwHagwxrjsVZrv18F9tpdXAmkdW3G7eAz4LeCxp9MI/X02wIciskJE\nZtvzOvS9HVCDBYcjY4wRkZA8JlNEOgGvAzcbY6pE5OCyUNxvY4wbGCEiKcBCYKCfS/IpETkHKDbG\nrBCRCf6upwOdYowpFJEuwEcisqn5wo54bwdKi7sQ6NFsOsueF6r2iUgmgP2z2J4fMq+DiERhhfZL\nxpg37Nkhv98AxpgKYAlWN0GKiBxoIDXfr4P7bC9PBso6uNRjNR44V0TygJexukv+SmjvM8aYQvtn\nMdYH9Bg6+L0dKMH9LdDf/jY6GrgEeNvPNfnS28DP7fs/x+oDPjD/Svub6LFAZbN/v4KGWE3rfwEb\njTF/abYoZPdbRDLsljYiEofVp78RK8AvtFdruc8HXosLgU+M3QkaLIwxdxpjsowxvbH+Zj8xxswk\nhPdZRBJEJPHAfeBMYB0d/d72d0d/s077s4EtWP2Cd/m7nnbcr/nAHsCJ1b91NVa/3n+ArcDHQGd7\nXcE6umY7sBbI8Xf9R7nPp2D1A34HrLZvZ4fyfgPDgFX2Pq8D/mDP7wssB7YBrwEx9vxYe3qbvbyv\nv/fhGPd/ArAo1PfZ3rc19m39gazq6Pe2nvKulFJBJlC6SpRSSnlJg1sppYKMBrdSSgUZDW6llAoy\nGtxKKRVkNLiVUirIaHArpVSQ+X95MM91WwdqgQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6tnNqWFqTqW",
        "outputId": "32ddf869-4c6a-49b8-ae45-ba6818e91da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Training XGBoost\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "class TrainXGBBoost:\n",
        "\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.test_data = []\n",
        "        self.test_labels = []\n",
        "    #    assert os.path.exists('./models/checkpoint')\n",
        "        gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200, is_train=False)\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver = tf.train.Saver()\n",
        "            with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "              model_name = next(f).split('\"'.encode())[1]\n",
        "    #          saver.restore(sess, \"{}models/{}\".format(googlepath, model_name.decode()))\n",
        "\n",
        "         #   files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n",
        "            files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]      \n",
        "            for file in files:\n",
        "                print(file)\n",
        "                #Read in file -- note that parse_dates will be need later\n",
        "                df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "                df = df[['open','high','low','close','volume']]\n",
        "                # #Create new index with missing days\n",
        "                # idx = pd.date_range(df.index[-1], df.index[0])\n",
        "                # #Reindex and fill the missing day with the value from the day before\n",
        "                # df = df.reindex(idx, method='bfill').sort_index(ascending=False)\n",
        "                #Normilize using a of size num_historical_days\n",
        "                labels = df.close.pct_change(days).map(lambda x: int(x > pct_change/100.0))\n",
        "                df = ((df -\n",
        "                df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "                /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "                -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "                df['labels'] = labels\n",
        "                #Drop the last 10 day that we don't have data for\n",
        "                df = df.dropna()\n",
        "                #Hold out the last year of trading for testing\n",
        "                test_df = df[:365]\n",
        "                #Padding to keep labels from bleeding\n",
        "                df = df[400:]\n",
        "                #This may not create good samples if num_historical_days is a\n",
        "                #mutliple of 7\n",
        "                data = df[['open', 'high', 'low', 'close', 'volume']].values\n",
        "                labels = df['labels'].values\n",
        "                for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\n",
        "                    self.data.append(features[0])\n",
        "                    print(features[0])\n",
        "                    self.labels.append(labels[i-1])\n",
        "                data = test_df[['open', 'high', 'low', 'close', 'volume']].values\n",
        "                labels = test_df['labels'].values\n",
        "                for i in range(num_historical_days, len(test_df), 1):\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\n",
        "                    self.test_data.append(features[0])\n",
        "                    self.test_labels.append(labels[i-1])\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        params = {}\n",
        "        params['objective'] = 'multi:softprob'\n",
        "        params['eta'] = 0.01\n",
        "        params['num_class'] = 2\n",
        "        params['max_depth'] = 20\n",
        "        params['subsample'] = 0.05\n",
        "        params['colsample_bytree'] = 0.05\n",
        "        params['eval_metric'] = 'mlogloss'\n",
        "        #params['scale_pos_weight'] = 10\n",
        "        #params['silent'] = True\n",
        "        #params['gpu_id'] = 0\n",
        "        #params['max_bin'] = 16\n",
        "        #params['tree_method'] = 'gpu_hist'\n",
        "\n",
        "        train = xgb.DMatrix(self.data, self.labels)\n",
        "        test = xgb.DMatrix(self.test_data, self.test_labels)\n",
        "\n",
        "        watchlist = [(train, 'train'), (test, 'test')]\n",
        "        clf = xgb.train(params, train, 1000, evals=watchlist, early_stopping_rounds=100)\n",
        "        joblib.dump(clf, f'{googlepath}models/clf.pkl')\n",
        "        #cm = confusion_matrix(self.test_labels, map(lambda x: int(x[1] > .5), clf.predict(test)))\n",
        "        #print(cm)\n",
        "        #plot_confusion_matrix(cm, ['Down', 'Up'], normalize=True, title=\"Confusion Matrix\")\n",
        "\n",
        "\n",
        "boost_model = TrainXGBBoost(num_historical_days=20, days=10, pct_change=10)\n",
        "boost_model.train()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Relu:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_2:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"Relu_4:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_5:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_6:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "[0.03233599 0.         0.01741412 ... 0.06604493 0.         0.02162729]\n",
            "[0.         0.         0.04114368 ... 0.         0.         0.09578311]\n",
            "[0.0116357  0.         0.         ... 0.02863285 0.01337457 0.        ]\n",
            "[0.02551394 0.         0.02559086 ... 0.         0.00045254 0.00536522]\n",
            "[0.05187207 0.         0.01009213 ... 0.02371604 0.         0.02022137]\n",
            "[0.00233749 0.         0.         ... 0.         0.         0.03052939]\n",
            "[0.03253358 0.         0.02724711 ... 0.         0.0513097  0.        ]\n",
            "[0.02173332 0.         0.03193788 ... 0.03082949 0.         0.00505348]\n",
            "[0.03203054 0.         0.         ... 0.         0.         0.12238848]\n",
            "[0.02897845 0.         0.00126023 ... 0.00473168 0.         0.02439099]\n",
            "[0.00849279 0.00026431 0.05640072 ... 0.02302236 0.         0.00842175]\n",
            "[0.         0.         0.         ... 0.04616554 0.         0.03461901]\n",
            "[0.01943982 0.         0.         ... 0.         0.01595057 0.01924629]\n",
            "[0.         0.         0.04995041 ... 0.         0.02847041 0.        ]\n",
            "[0.00585243 0.         0.         ... 0.02510902 0.00599416 0.        ]\n",
            "[0.02055603 0.         0.01969388 ... 0.03634372 0.         0.        ]\n",
            "[0.00877874 0.         0.         ... 0.00702079 0.         0.01744582]\n",
            "[0.02979372 0.         0.         ... 0.         0.         0.02275095]\n",
            "[0.         0.         0.03700828 ... 0.         0.         0.00198227]\n",
            "[0.0123596  0.         0.         ... 0.00572683 0.         0.01730301]\n",
            "[0.04116255 0.         0.         ... 0.         0.01695333 0.00865426]\n",
            "[0.05198548 0.         0.08310337 ... 0.00557486 0.         0.01244999]\n",
            "[0.         0.         0.00263595 ... 0.03231228 0.00075452 0.        ]\n",
            "[0.00312032 0.         0.00699464 ... 0.         0.         0.04322532]\n",
            "[0.02029279 0.         0.01502894 ... 0.         0.00342801 0.01007181]\n",
            "[0.01889507 0.         0.         ... 0.01919246 0.00855669 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.01869115 0.        ]\n",
            "[0.05873467 0.         0.02223    ... 0.         0.06248006 0.        ]\n",
            "[0.02840839 0.         0.01115491 ... 0.         0.01330079 0.        ]\n",
            "[0.04508822 0.         0.07666495 ... 0.         0.00750973 0.        ]\n",
            "[0.02185839 0.         0.00807505 ... 0.         0.01719755 0.06242901]\n",
            "[0.01986287 0.         0.         ... 0.         0.01248006 0.01737616]\n",
            "[0.03163689 0.         0.06651136 ... 0.         0.00837331 0.        ]\n",
            "[0.03646839 0.         0.05426225 ... 0.         0.01710617 0.01902495]\n",
            "[0.04141288 0.         0.10640321 ... 0.         0.         0.01304535]\n",
            "[0.04016317 0.         0.         ... 0.02588347 0.00814638 0.        ]\n",
            "[0.00804248 0.         0.         ... 0.02281105 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.00217864 0.01091263]\n",
            "[0.04059281 0.         0.02927712 ... 0.         0.         0.04436439]\n",
            "[0.02160432 0.         0.00238964 ... 0.         0.03270011 0.01829128]\n",
            "[0.04167442 0.         0.00823497 ... 0.         0.01039178 0.02024985]\n",
            "[0.0264501  0.         0.05513189 ... 0.01324194 0.0028067  0.02192087]\n",
            "[0.00592875 0.         0.         ... 0.         0.0036668  0.04101306]\n",
            "[0.0546109  0.         0.04577173 ... 0.00877607 0.         0.00493077]\n",
            "[0.00541042 0.01683906 0.04449337 ... 0.         0.02353319 0.017979  ]\n",
            "[0.08305394 0.         0.07596244 ... 0.         0.00733309 0.02081158]\n",
            "[0.00387132 0.         0.01056988 ... 0.         0.01707689 0.02849261]\n",
            "[0.         0.         0.03971162 ... 0.         0.00842235 0.02031129]\n",
            "[0.         0.         0.00827606 ... 0.         0.         0.01389867]\n",
            "[0.03713074 0.         0.0524836  ... 0.         0.         0.01682168]\n",
            "[0.         0.         0.02043329 ... 0.         0.01887003 0.03311637]\n",
            "[0.0281094  0.         0.01935472 ... 0.         0.01882476 0.01981032]\n",
            "[0.01817363 0.         0.03585138 ... 0.09094594 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.04588864 0.        ]\n",
            "[0.0329507  0.         0.         ... 0.         0.00653354 0.        ]\n",
            "[0.06106513 0.         0.0190639  ... 0.01271575 0.         0.01120603]\n",
            "[0.01180927 0.         0.         ... 0.04371996 0.02648022 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.01571358 0.00824756]\n",
            "[0.00872294 0.         0.01628811 ... 0.02287889 0.02265853 0.        ]\n",
            "[0.00287066 0.         0.         ... 0.         0.00493212 0.03092926]\n",
            "[0.03921797 0.         0.03485425 ... 0.00298851 0.0189442  0.01249392]\n",
            "[0.05034023 0.         0.         ... 0.02145164 0.         0.00035759]\n",
            "[0.        0.        0.        ... 0.        0.0172122 0.       ]\n",
            "[0.05456283 0.         0.06442782 ... 0.         0.03212506 0.02439392]\n",
            "[0.10011574 0.         0.01415259 ... 0.01650868 0.         0.00941179]\n",
            "[0.02534829 0.         0.         ... 0.         0.02586757 0.03667621]\n",
            "[0.01207137 0.         0.05584094 ... 0.00070435 0.00645363 0.0161165 ]\n",
            "[0.0273695  0.         0.02381421 ... 0.         0.         0.04237887]\n",
            "[0.01908408 0.         0.06197788 ... 0.02153911 0.         0.00735407]\n",
            "[0.06096233 0.         0.17646992 ... 0.         0.00501697 0.02417862]\n",
            "[0.03031772 0.         0.         ... 0.         0.         0.03685243]\n",
            "[0.0161506  0.         0.         ... 0.         0.00918541 0.        ]\n",
            "[0.01302022 0.         0.02604075 ... 0.02515278 0.01539197 0.01919353]\n",
            "[0.01986793 0.         0.         ... 0.         0.         0.00508339]\n",
            "[0.01613768 0.         0.0106147  ... 0.         0.00841034 0.01812228]\n",
            "[0.15486881 0.         0.         ... 0.01634412 0.         0.        ]\n",
            "[0.01981451 0.         0.00151529 ... 0.         0.         0.00246712]\n",
            "[0.04494622 0.         0.04075374 ... 0.         0.         0.00284143]\n",
            "[0.         0.         0.         ... 0.         0.01946606 0.02715731]\n",
            "[0.06504534 0.         0.05425583 ... 0.         0.         0.00830844]\n",
            "[0.         0.         0.         ... 0.01159712 0.00623921 0.00561332]\n",
            "[0.00815349 0.         0.         ... 0.         0.         0.02827167]\n",
            "[0.01937865 0.         0.         ... 0.         0.02465645 0.        ]\n",
            "[0.01608262 0.         0.         ... 0.0097806  0.         0.05543784]\n",
            "[0.01088654 0.         0.         ... 0.00766145 0.         0.00832766]\n",
            "[0.0202572  0.         0.03917966 ... 0.         0.02318826 0.00512298]\n",
            "[0.02733448 0.         0.03414948 ... 0.02410158 0.         0.00545148]\n",
            "[0.00406091 0.         0.         ... 0.         0.04991772 0.00387683]\n",
            "[0.02424027 0.         0.01325167 ... 0.         0.         0.05401356]\n",
            "[0.00556263 0.         0.00662452 ... 0.         0.         0.01178954]\n",
            "[0.0420806  0.         0.02059857 ... 0.         0.0630495  0.12683682]\n",
            "[0.02544167 0.         0.04069626 ... 0.0312716  0.         0.        ]\n",
            "[0.         0.         0.         ... 0.01067493 0.         0.00978282]\n",
            "[0.         0.         0.         ... 0.         0.00894721 0.02710042]\n",
            "[0.02824558 0.         0.03980035 ... 0.06548882 0.00475983 0.        ]\n",
            "[0.01878639 0.01731727 0.05641023 ... 0.         0.01662828 0.02094373]\n",
            "[0.01763493 0.         0.02669388 ... 0.         0.0202297  0.03202713]\n",
            "[0.05413127 0.         0.         ... 0.         0.01562841 0.03292376]\n",
            "[0.0494098  0.         0.00269506 ... 0.02201922 0.02212195 0.        ]\n",
            "[0.00561404 0.         0.01578497 ... 0.         0.01285986 0.01775395]\n",
            "[0.09053072 0.         0.07715586 ... 0.         0.01015262 0.        ]\n",
            "[0.03395234 0.         0.01934754 ... 0.         0.00957151 0.03180169]\n",
            "[0.         0.         0.05895447 ... 0.02856    0.         0.        ]\n",
            "[0.01658997 0.         0.         ... 0.02895785 0.05134712 0.        ]\n",
            "[0.00090698 0.         0.03442383 ... 0.00329121 0.         0.00316926]\n",
            "[0.01903404 0.         0.         ... 0.         0.         0.        ]\n",
            "[0.00977595 0.         0.         ... 0.         0.0189244  0.        ]\n",
            "[0.         0.         0.         ... 0.         0.01872745 0.00881012]\n",
            "[0.00130863 0.         0.05268558 ... 0.         0.00031222 0.00064051]\n",
            "[0.01029988 0.         0.01443024 ... 0.0110559  0.00685795 0.02054773]\n",
            "[0.02290939 0.         0.01949588 ... 0.00853302 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.0251752  0.01238646]\n",
            "[0.02631606 0.         0.00111554 ... 0.         0.01291171 0.02462921]\n",
            "[0.04612001 0.         0.0332527  ... 0.04110891 0.         0.        ]\n",
            "[0.02045361 0.         0.         ... 0.02017194 0.01856228 0.        ]\n",
            "[0.02344232 0.         0.         ... 0.01171274 0.         0.        ]\n",
            "[0.01337152 0.         0.         ... 0.         0.03949264 0.01700458]\n",
            "[0.         0.         0.03872992 ... 0.         0.         0.        ]\n",
            "[0.06628618 0.         0.02193442 ... 0.         0.00826694 0.01857967]\n",
            "[0.05009132 0.         0.05020239 ... 0.         0.01122896 0.02017515]\n",
            "[0.02776828 0.         0.02837316 ... 0.         0.         0.06192888]\n",
            "[0.00750249 0.         0.04099512 ... 0.         0.         0.00450692]\n",
            "[0.03380159 0.         0.         ... 0.         0.02924261 0.        ]\n",
            "[0.         0.         0.04632598 ... 0.02668218 0.         0.        ]\n",
            "[0.0069066  0.         0.01337972 ... 0.         0.02993844 0.03924646]\n",
            "[0.02121455 0.         0.01982272 ... 0.         0.         0.03386411]\n",
            "[0.00979767 0.         0.         ... 0.         0.02532334 0.02298265]\n",
            "[0.03676795 0.         0.06763291 ... 0.         0.0187361  0.01429662]\n",
            "[0.04906013 0.         0.04019697 ... 0.         0.01016948 0.03927539]\n",
            "[0.         0.         0.06053547 ... 0.03820995 0.         0.        ]\n",
            "[0.05138538 0.         0.         ... 0.         0.         0.        ]\n",
            "[0.00577836 0.         0.         ... 0.12774548 0.16487207 0.        ]\n",
            "[0.01018727 0.         0.00486046 ... 0.         0.         0.0682854 ]\n",
            "[0.00095951 0.         0.06073928 ... 0.         0.01315786 0.02761148]\n",
            "[0.06476961 0.         0.04546735 ... 0.         0.0249339  0.        ]\n",
            "[0.02075612 0.         0.01433679 ... 0.         0.         0.01044505]\n",
            "[0.01925255 0.         0.00921738 ... 0.01195926 0.01415193 0.        ]\n",
            "[0.02676802 0.         0.02441519 ... 0.11461452 0.04196643 0.05674665]\n",
            "[0.01470936 0.         0.03693815 ... 0.         0.0053405  0.02341842]\n",
            "[0.03712017 0.         0.03912461 ... 0.         0.         0.03063076]\n",
            "[0.02780405 0.         0.0261923  ... 0.         0.         0.04658634]\n",
            "[0.         0.         0.07590067 ... 0.         0.05480747 0.        ]\n",
            "[0.04403766 0.         0.07876477 ... 0.         0.04849903 0.03171191]\n",
            "[0.0156808  0.         0.02198143 ... 0.0158324  0.         0.00158657]\n",
            "[0.         0.         0.         ... 0.         0.         0.03613229]\n",
            "[0.01685303 0.         0.         ... 0.         0.00676605 0.0105032 ]\n",
            "[0.         0.         0.01374955 ... 0.         0.00778754 0.00616567]\n",
            "[0.04398501 0.         0.03042394 ... 0.00640732 0.00044783 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.00374037 0.0064164 ]\n",
            "[0.03686294 0.         0.0312672  ... 0.         0.         0.04684773]\n",
            "[0.01523246 0.         0.00364126 ... 0.00651994 0.04923081 0.        ]\n",
            "[0.01517252 0.         0.04404831 ... 0.         0.         0.03235473]\n",
            "[0.05175441 0.         0.03330164 ... 0.0195087  0.03958657 0.02290933]\n",
            "[0.03542838 0.         0.0101089  ... 0.         0.10646248 0.        ]\n",
            "[0.02425664 0.         0.         ... 0.         0.00076602 0.00182727]\n",
            "[0.02379285 0.         0.         ... 0.00319091 0.         0.0031556 ]\n",
            "[0.00150943 0.         0.03124    ... 0.         0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.00953026 0.        ]\n",
            "[0.         0.         0.03863643 ... 0.         0.         0.016269  ]\n",
            "[0.08511925 0.         0.01465238 ... 0.         0.02342273 0.02410582]\n",
            "[0.14366227 0.         0.17556414 ... 0.0528503  0.06021139 0.        ]\n",
            "[0.02461795 0.         0.01376404 ... 0.         0.         0.05740502]\n",
            "[0.01417852 0.         0.0238101  ... 0.         0.01595934 0.01992056]\n",
            "[0.         0.         0.03968648 ... 0.         0.02033293 0.0115725 ]\n",
            "[0.04218094 0.         0.03566211 ... 0.         0.00874712 0.        ]\n",
            "[0.0123555  0.         0.         ... 0.01755642 0.0043269  0.01195756]\n",
            "[0.0017421  0.         0.         ... 0.         0.02359868 0.01116483]\n",
            "[0.01211253 0.         0.04272701 ... 0.         0.00604188 0.        ]\n",
            "[0.0486707  0.         0.02981563 ... 0.         0.         0.0130724 ]\n",
            "[0.02160983 0.         0.00702691 ... 0.         0.01343819 0.        ]\n",
            "[0.06196589 0.         0.         ... 0.00061921 0.         0.00074717]\n",
            "[0.02477437 0.         0.01267113 ... 0.         0.         0.02443136]\n",
            "[0.00643462 0.         0.         ... 0.         0.02751488 0.06563296]\n",
            "[0.0282714  0.         0.04510571 ... 0.         0.01591577 0.02354555]\n",
            "[0.         0.         0.06597841 ... 0.         0.01758391 0.03762879]\n",
            "[0.04056057 0.         0.07530426 ... 0.         0.05045594 0.        ]\n",
            "[0.02599858 0.         0.         ... 0.         0.02274963 0.01267209]\n",
            "[0.02094459 0.         0.06424572 ... 0.         0.00057144 0.00536278]\n",
            "[0.00911812 0.         0.0164877  ... 0.         0.         0.        ]\n",
            "[0.03169618 0.         0.         ... 0.         0.029058   0.03637337]\n",
            "[0.06642033 0.         0.02707847 ... 0.         0.         0.06551056]\n",
            "[0.         0.         0.00787012 ... 0.02815068 0.         0.01852888]\n",
            "[0.03577844 0.         0.         ... 0.         0.03421685 0.02170212]\n",
            "[0.01821276 0.         0.06744662 ... 0.04266193 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.01911861 0.         0.01432394]\n",
            "[0.01083609 0.         0.         ... 0.         0.0183577  0.01299314]\n",
            "[0.05968973 0.         0.05273461 ... 0.         0.03114075 0.0284461 ]\n",
            "[0.01335823 0.         0.01936522 ... 0.0143905  0.         0.01393793]\n",
            "[0.00486357 0.         0.         ... 0.         0.04528583 0.01528834]\n",
            "[0.00998056 0.         0.03636318 ... 0.03203826 0.05954814 0.        ]\n",
            "[0.01084231 0.         0.0181575  ... 0.         0.00977973 0.        ]\n",
            "[0.         0.         0.01908671 ... 0.         0.01183975 0.00625766]\n",
            "[0.07288403 0.         0.04814524 ... 0.00559294 0.04158313 0.03227574]\n",
            "[0.         0.         0.01042706 ... 0.00260166 0.01165236 0.        ]\n",
            "[0.01904767 0.         0.02727132 ... 0.         0.         0.06983504]\n",
            "[0.07051919 0.         0.02775039 ... 0.05374311 0.         0.        ]\n",
            "[0.02981235 0.         0.04751161 ... 0.         0.01573711 0.00875037]\n",
            "[0.03709409 0.         0.0358091  ... 0.         0.05465985 0.00166207]\n",
            "[0.07681216 0.         0.03253013 ... 0.         0.         0.01420362]\n",
            "[0.         0.         0.02813943 ... 0.         0.01736421 0.01334187]\n",
            "[0.02562751 0.         0.03925825 ... 0.         0.10347769 0.13818713]\n",
            "[0.         0.         0.         ... 0.         0.0660085  0.00223077]\n",
            "[0.01242944 0.         0.06368364 ... 0.0296938  0.         0.03426418]\n",
            "[0.00227138 0.         0.02936188 ... 0.         0.0196474  0.02472726]\n",
            "[0.         0.         0.107438   ... 0.         0.         0.00055401]\n",
            "[0.00533192 0.         0.         ... 0.         0.         0.01436153]\n",
            "[0.         0.         0.         ... 0.00016861 0.01384168 0.        ]\n",
            "[0.01528307 0.         0.         ... 0.         0.00633849 0.05351022]\n",
            "[0.09806021 0.         0.00718872 ... 0.         0.         0.02074349]\n",
            "[0.01310702 0.         0.01310125 ... 0.         0.00189174 0.03107855]\n",
            "[0.03430204 0.         0.04137512 ... 0.         0.         0.07490242]\n",
            "[0.01780717 0.         0.03200669 ... 0.         0.         0.        ]\n",
            "[0.03667384 0.         0.         ... 0.         0.         0.02525538]\n",
            "[0.02784122 0.         0.01104652 ... 0.         0.         0.07566206]\n",
            "[0.02220942 0.         0.         ... 0.         0.         0.00411833]\n",
            "[0.04150303 0.         0.02108875 ... 0.         0.         0.01518332]\n",
            "[0.00711759 0.         0.         ... 0.         0.05107029 0.        ]\n",
            "[0.01902612 0.         0.02045238 ... 0.         0.04415692 0.01996882]\n",
            "[0.07686717 0.         0.01873942 ... 0.         0.00609459 0.01739209]\n",
            "[0.04555644 0.         0.05110098 ... 0.         0.         0.        ]\n",
            "[0.         0.         0.02760388 ... 0.         0.01941288 0.01055028]\n",
            "[0.01704332 0.         0.01572453 ... 0.         0.02923068 0.02100705]\n",
            "[0.02409224 0.         0.06933599 ... 0.         0.02618606 0.03266309]\n",
            "[0.         0.         0.04759179 ... 0.         0.         0.04001036]\n",
            "[0.01603167 0.         0.         ... 0.         0.01961826 0.03656138]\n",
            "[0.0087697  0.01798616 0.02666622 ... 0.00380419 0.         0.00419467]\n",
            "[0.         0.         0.         ... 0.01836355 0.02457711 0.        ]\n",
            "[0.00264407 0.         0.01759922 ... 0.00216466 0.03340209 0.01355772]\n",
            "[0.02185441 0.         0.01628545 ... 0.         0.009993   0.01757675]\n",
            "[0.03684964 0.         0.0419652  ... 0.         0.01659225 0.01572197]\n",
            "[0.02669015 0.         0.0312502  ... 0.         0.01366324 0.02342924]\n",
            "[0.00951214 0.         0.0197684  ... 0.         0.02998288 0.03470701]\n",
            "[0.01876515 0.         0.06691442 ... 0.         0.01227602 0.        ]\n",
            "[0.01222536 0.         0.01946588 ... 0.01436892 0.         0.00046705]\n",
            "[0.00081419 0.         0.         ... 0.         0.01694769 0.0078781 ]\n",
            "[0.0517093  0.         0.06561907 ... 0.         0.         0.06920026]\n",
            "[0.03737197 0.         0.00319589 ... 0.         0.         0.0532384 ]\n",
            "[0.08157392 0.         0.01214673 ... 0.         0.02037652 0.02425771]\n",
            "[0.05240062 0.         0.07987957 ... 0.         0.00886864 0.05067249]\n",
            "[0.06111144 0.         0.03384409 ... 0.         0.         0.01541448]\n",
            "[0.         0.         0.02170504 ... 0.02383878 0.         0.00375508]\n",
            "[0.00336487 0.         0.00397463 ... 0.01755755 0.04112507 0.        ]\n",
            "[0.07824761 0.         0.02778552 ... 0.         0.         0.04967343]\n",
            "[0.00991023 0.         0.         ... 0.         0.05743932 0.        ]\n",
            "[0.         0.         0.04097289 ... 0.         0.01381644 0.01717604]\n",
            "[0.02251221 0.         0.         ... 0.         0.02136572 0.02301485]\n",
            "[0.03614128 0.         0.05761099 ... 0.         0.04231169 0.        ]\n",
            "[0.04607297 0.         0.02743782 ... 0.         0.         0.02667458]\n",
            "[0.0128391  0.         0.00339659 ... 0.00175657 0.01514887 0.        ]\n",
            "[0.01335325 0.         0.01367166 ... 0.         0.00489081 0.03978428]\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n",
            "[0.01146383 0.         0.06350367 ... 0.         0.         0.02786175]\n",
            "[0.01149759 0.         0.         ... 0.         0.01189811 0.        ]\n",
            "[0.05225091 0.         0.03375757 ... 0.         0.01385402 0.01636475]\n",
            "[0.04691485 0.         0.04242539 ... 0.         0.01433781 0.02696579]\n",
            "[0.01477476 0.         0.02355579 ... 0.         0.         0.00146262]\n",
            "[0.05327479 0.         0.02167445 ... 0.         0.         0.        ]\n",
            "[0.0021928  0.         0.0043436  ... 0.         0.00128685 0.01933449]\n",
            "[0.04801124 0.         0.02967048 ... 0.00280443 0.         0.        ]\n",
            "[0.03034114 0.         0.         ... 0.02012099 0.         0.01142795]\n",
            "[0.0153071  0.         0.         ... 0.03321892 0.         0.        ]\n",
            "[0.02738561 0.         0.03683434 ... 0.02313738 0.         0.00032912]\n",
            "[0.04915681 0.22370513 0.         ... 0.00317115 0.         0.03024298]\n",
            "[0.03646562 0.         0.04527417 ... 0.         0.00392241 0.02940872]\n",
            "[0.07333788 0.         0.05548294 ... 0.01924945 0.         0.00074724]\n",
            "[0.0063535  0.         0.         ... 0.         0.00071618 0.        ]\n",
            "[0.01152978 0.         0.         ... 0.01417664 0.         0.02679821]\n",
            "[0.01000124 0.         0.00408799 ... 0.01928779 0.01113134 0.        ]\n",
            "[0.01741982 0.         0.011543   ... 0.         0.00175243 0.03744774]\n",
            "[0.02540374 0.         0.05826969 ... 0.         0.         0.        ]\n",
            "[0.         0.         0.01647599 ... 0.         0.         0.02194128]\n",
            "[0.03242398 0.         0.         ... 0.         0.00787627 0.01125674]\n",
            "[0.0373374  0.         0.06046296 ... 0.         0.0173111  0.01403163]\n",
            "[0.         0.         0.         ... 0.07822526 0.06166431 0.        ]\n",
            "[0.02557666 0.         0.00124541 ... 0.01334292 0.         0.        ]\n",
            "[0.00638963 0.         0.         ... 0.         0.03292673 0.        ]\n",
            "[0.02660638 0.         0.         ... 0.02360938 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.02188338 0.00969425]\n",
            "[0.03863481 0.         0.03482354 ... 0.         0.         0.02752543]\n",
            "[0.03487697 0.         0.02872977 ... 0.         0.         0.        ]\n",
            "[0.01806534 0.         0.01209724 ... 0.         0.01586152 0.02679521]\n",
            "[0.04685846 0.         0.03810712 ... 0.         0.00749358 0.02859906]\n",
            "[0.04770282 0.         0.03416815 ... 0.         0.00385218 0.01080394]\n",
            "[0.         0.         0.03862585 ... 0.06341102 0.02399666 0.00469243]\n",
            "[0.09280751 0.         0.03974205 ... 0.         0.01738092 0.01929045]\n",
            "[0.00267418 0.         0.0149343  ... 0.         0.         0.00047295]\n",
            "[0.00841252 0.         0.         ... 0.         0.         0.03456545]\n",
            "[0.01843097 0.         0.         ... 0.11238498 0.         0.        ]\n",
            "[0.0476548 0.        0.        ... 0.        0.        0.       ]\n",
            "[0.01787563 0.         0.         ... 0.         0.         0.01727494]\n",
            "[0.00911378 0.         0.         ... 0.         0.02841089 0.        ]\n",
            "[0.02366251 0.         0.00573609 ... 0.03536507 0.         0.        ]\n",
            "[0.02369043 0.         0.03948246 ... 0.         0.03253796 0.05004992]\n",
            "[0.00861293 0.         0.         ... 0.02249015 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.00481036 0.01913797 0.08850263]\n",
            "[0.         0.         0.02991934 ... 0.         0.00226567 0.        ]\n",
            "[0.03728379 0.         0.02402226 ... 0.00268332 0.         0.        ]\n",
            "[0.00180677 0.         0.01891073 ... 0.         0.00302201 0.0218269 ]\n",
            "[0.01179531 0.         0.0330775  ... 0.         0.02064208 0.        ]\n",
            "[0.03410661 0.         0.02318545 ... 0.         0.00433885 0.02374694]\n",
            "[0.02005723 0.         0.0589229  ... 0.02758054 0.         0.01069092]\n",
            "[0.0243612  0.0099522  0.02356779 ... 0.         0.05286396 0.03577047]\n",
            "[0.00057152 0.         0.03862415 ... 0.         0.01467793 0.01493244]\n",
            "[0.03598636 0.         0.02059992 ... 0.         0.01494838 0.05303331]\n",
            "[0.02646581 0.         0.03108174 ... 0.         0.02136335 0.        ]\n",
            "[0.01293094 0.         0.         ... 0.         0.00942195 0.        ]\n",
            "[0.0454023  0.         0.03304471 ... 0.         0.21102059 0.12431385]\n",
            "[0.03173437 0.         0.01977345 ... 0.         0.02417562 0.01043612]\n",
            "[0.05470956 0.         0.03563274 ... 0.         0.03629642 0.01004627]\n",
            "[0.01699783 0.         0.03639168 ... 0.01776386 0.         0.        ]\n",
            "[0.09863628 0.         0.08740339 ... 0.         0.01374682 0.04765174]\n",
            "[0.01829936 0.         0.0332786  ... 0.         0.00259067 0.04827265]\n",
            "[0.         0.         0.00252932 ... 0.         0.01544882 0.01210354]\n",
            "[0.04770474 0.         0.13601501 ... 0.         0.01865133 0.        ]\n",
            "[0.04116196 0.         0.03210953 ... 0.         0.01694346 0.0412706 ]\n",
            "[0.06407974 0.         0.02319946 ... 0.         0.01792786 0.01996067]\n",
            "[0.04849358 0.         0.04715814 ... 0.         0.00944815 0.0076781 ]\n",
            "[0.00752936 0.         0.01957627 ... 0.         0.00248535 0.01753115]\n",
            "[0.         0.00626802 0.04149278 ... 0.         0.01606156 0.03793444]\n",
            "[0.03447483 0.         0.05503355 ... 0.         0.         0.02932414]\n",
            "[0.00919168 0.         0.00305636 ... 0.6253895  0.         0.        ]\n",
            "[0.02436204 0.         0.05545722 ... 0.02704848 0.         0.        ]\n",
            "[0.00667903 0.         0.         ... 0.         0.         0.00522104]\n",
            "[0.01067216 0.         0.00868886 ... 0.         0.         0.04812699]\n",
            "[0.00262926 0.         0.         ... 0.         0.00112598 0.00779396]\n",
            "[0.02327765 0.         0.01766879 ... 0.         0.         0.01935077]\n",
            "[0.02560622 0.         0.00448873 ... 0.         0.01391634 0.01378522]\n",
            "[0.05069305 0.         0.03506082 ... 0.         0.         0.02411814]\n",
            "[0.03087425 0.         0.01647157 ... 0.         0.00945051 0.08396055]\n",
            "[0.02511087 0.         0.         ... 0.         0.01923528 0.03657846]\n",
            "[0.02741435 0.         0.07237168 ... 0.         0.         0.00183527]\n",
            "[0.03359342 0.         0.01896401 ... 0.         0.         0.03916289]\n",
            "[0.       0.       0.       ... 0.       0.009299 0.      ]\n",
            "[0.02442591 0.         0.         ... 0.02067395 0.01457328 0.        ]\n",
            "[0.         0.         0.01232046 ... 0.         0.01693477 0.1452141 ]\n",
            "[0.00986743 0.         0.02083878 ... 0.01022913 0.         0.01187987]\n",
            "[0.01516714 0.         0.         ... 0.         0.01156213 0.        ]\n",
            "[0.00399004 0.         0.00531962 ... 0.00175268 0.00272026 0.00750275]\n",
            "[0.03939056 0.         0.02855126 ... 0.01299652 0.         0.01022448]\n",
            "[0.02857863 0.         0.04351543 ... 0.         0.02229033 0.01190551]\n",
            "[3.9612412e-02 0.0000000e+00 6.8756782e-02 ... 7.6465537e-03 7.5364478e-05\n",
            " 1.5045108e-02]\n",
            "[0.00785612 0.         0.00493806 ... 0.         0.00850222 0.0256843 ]\n",
            "[0.03060693 0.         0.03951436 ... 0.         0.         0.        ]\n",
            "[0.01090962 0.         0.         ... 0.         0.01986085 0.01373829]\n",
            "[0.05106937 0.         0.02110493 ... 0.         0.01641227 0.02948445]\n",
            "[0.01987385 0.         0.         ... 0.         0.         0.02469543]\n",
            "[0.06020167 0.         0.         ... 0.02057048 0.         0.        ]\n",
            "[0.01906173 0.         0.03408056 ... 0.         0.02000717 0.02616411]\n",
            "[0.         0.         0.07692695 ... 0.         0.01984931 0.0452653 ]\n",
            "[0.05214159 0.         0.04596562 ... 0.         0.00101011 0.02094259]\n",
            "[0.03156199 0.         0.02486791 ... 0.0125321  0.         0.        ]\n",
            "[0.0091159  0.         0.         ... 0.         0.01437566 0.00161827]\n",
            "[0.06806869 0.         0.04227637 ... 0.         0.01742897 0.04321893]\n",
            "[0.01737578 0.         0.05361851 ... 0.         0.         0.        ]\n",
            "[0.07127926 0.         0.02268276 ... 0.05124416 0.07170796 0.        ]\n",
            "[0.02633713 0.         0.02047893 ... 0.         0.         0.03876675]\n",
            "[0.01020369 0.         0.         ... 0.01580416 0.00509609 0.        ]\n",
            "[0.00569597 0.         0.         ... 0.0585163  0.06235906 0.        ]\n",
            "[0.0071502  0.         0.         ... 0.         0.         0.02705843]\n",
            "[0.         0.         0.04456166 ... 0.         0.00580411 0.01815948]\n",
            "[0.01647031 0.         0.01860983 ... 0.         0.         0.04733704]\n",
            "[0.00968735 0.         0.01272919 ... 0.02291047 0.         0.        ]\n",
            "[0.       0.       0.       ... 0.       0.000586 0.      ]\n",
            "[0.03573078 0.         0.         ... 0.         0.01099007 0.01543781]\n",
            "[0.04784025 0.         0.05751191 ... 0.03400581 0.         0.        ]\n",
            "[0.01247875 0.         0.01647493 ... 0.02720534 0.01244507 0.        ]\n",
            "[0.02816796 0.         0.07104119 ... 0.         0.         0.01326169]\n",
            "[0.         0.         0.         ... 0.         0.03374625 0.02844546]\n",
            "[0.01587123 0.         0.03645783 ... 0.         0.04265554 0.00355959]\n",
            "[0.04885468 0.00208155 0.03047684 ... 0.         0.03468746 0.00701393]\n",
            "[0.10914848 0.         0.         ... 0.         0.01782827 0.        ]\n",
            "[0.         0.         0.00024502 ... 0.         0.00036104 0.06743185]\n",
            "[0.0031517  0.         0.02778248 ... 0.         0.         0.02438352]\n",
            "[0.04527878 0.         0.         ... 0.         0.02503919 0.01517856]\n",
            "[0.0362579  0.         0.06599494 ... 0.         0.02692348 0.        ]\n",
            "[0.00434544 0.         0.02447897 ... 0.         0.02678899 0.03080748]\n",
            "[0.053745   0.         0.04324755 ... 0.         0.00748554 0.0215166 ]\n",
            "[0.         0.         0.02590831 ... 0.         0.00330695 0.03482275]\n",
            "[0.06270395 0.         0.06320254 ... 0.         0.         0.03216867]\n",
            "[0.07150101 0.         0.03001158 ... 0.         0.05610526 0.00099785]\n",
            "[0.05888148 0.         0.01893453 ... 0.04960471 0.02467493 0.        ]\n",
            "[0.05602319 0.         0.00286573 ... 0.01480941 0.         0.        ]\n",
            "[0.0096476  0.         0.         ... 0.         0.08140045 0.        ]\n",
            "[0.00668899 0.         0.00319753 ... 0.         0.00827938 0.04870152]\n",
            "[0.         0.         0.06860169 ... 0.         0.00292205 0.02946443]\n",
            "[0.02014155 0.         0.03582316 ... 0.02197981 0.02048041 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.         0.01270804]\n",
            "[0.0174198  0.         0.00895441 ... 0.01201276 0.         0.01197836]\n",
            "[0.01087749 0.         0.08048919 ... 0.         0.         0.03085423]\n",
            "[0.01787171 0.         0.         ... 0.         0.01420576 0.01765955]\n",
            "[0.01744836 0.         0.02933604 ... 0.         0.         0.03270859]\n",
            "[0.0196903  0.         0.02969815 ... 0.         0.         0.01118036]\n",
            "[0.01639894 0.         0.01401161 ... 0.         0.0547975  0.        ]\n",
            "[0.01103859 0.         0.07432772 ... 0.         0.02416245 0.        ]\n",
            "[0.03284923 0.         0.02426719 ... 0.         0.         0.00857584]\n",
            "[0.02043529 0.         0.07250071 ... 0.05238705 0.01149624 0.01130369]\n",
            "[0.01918135 0.         0.         ... 0.         0.02463805 0.        ]\n",
            "[0.00703376 0.         0.04289199 ... 0.         0.00658736 0.00251815]\n",
            "[0.02166874 0.         0.05880818 ... 0.00467409 0.         0.00442148]\n",
            "[0.         0.         0.         ... 0.05341747 0.04756927 0.00336739]\n",
            "[0.01275044 0.         0.033689   ... 0.         0.0243108  0.03853539]\n",
            "[0.08300696 0.         0.06751304 ... 0.00347858 0.0224728  0.        ]\n",
            "[0.06434632 0.         0.02357264 ... 0.01599712 0.00312751 0.0193977 ]\n",
            "[0.07659694 0.         0.02776517 ... 0.02115697 0.         0.        ]\n",
            "[0.         0.         0.01023917 ... 0.01230787 0.         0.0189568 ]\n",
            "[0.02795636 0.         0.01209179 ... 0.         0.02412885 0.01897517]\n",
            "[0.         0.         0.02405839 ... 0.         0.00174272 0.04307966]\n",
            "[0.04823538 0.         0.04262261 ... 0.         0.         0.02341812]\n",
            "[0.05149346 0.         0.05297683 ... 0.         0.0228755  0.02503202]\n",
            "[0.0250263  0.         0.03732691 ... 0.         0.00386369 0.04676916]\n",
            "[0.02545305 0.         0.01219013 ... 0.0078487  0.04130351 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.03248493 0.        ]\n",
            "[0.0559209  0.         0.0204931  ... 0.         0.00945896 0.02943683]\n",
            "[0.01963069 0.         0.02091757 ... 0.0020931  0.         0.00531833]\n",
            "[0.         0.00050224 0.01597012 ... 0.         0.01798789 0.04444467]\n",
            "[0.02542624 0.         0.04480236 ... 0.00870773 0.         0.        ]\n",
            "[0.00162547 0.         0.         ... 0.         0.01676946 0.02565628]\n",
            "[0.01026241 0.         0.         ... 0.         0.0404442  0.00522849]\n",
            "[0.01318894 0.         0.01135347 ... 0.         0.03774828 0.01039342]\n",
            "[0.01729298 0.         0.06789724 ... 0.01803958 0.         0.00251514]\n",
            "[0.01337534 0.         0.03141376 ... 0.01420568 0.         0.00271673]\n",
            "[0.0371738  0.         0.         ... 0.         0.         0.02241135]\n",
            "[0.04805255 0.         0.02375326 ... 0.         0.00594712 0.        ]\n",
            "[0.01691422 0.         0.00364515 ... 0.00076125 0.         0.0099933 ]\n",
            "[0.00995327 0.         0.         ... 0.00244759 0.         0.00927475]\n",
            "[0.02873012 0.         0.         ... 0.         0.         0.08374023]\n",
            "[0.01273118 0.         0.01875207 ... 0.         0.01794355 0.02407452]\n",
            "[0.02704236 0.         0.05616629 ... 0.00863645 0.03405377 0.        ]\n",
            "[0.01205669 0.         0.         ... 0.         0.04150751 0.07079556]\n",
            "[0.01035079 0.         0.0491557  ... 0.         0.         0.04184373]\n",
            "[0.05011509 0.         0.10571219 ... 0.         0.03336497 0.03375823]\n",
            "[0.08159982 0.         0.05920709 ... 0.         0.         0.        ]\n",
            "[0.         0.         0.01312564 ... 0.00905734 0.         0.        ]\n",
            "[0.01170027 0.         0.05923089 ... 0.00934517 0.07819778 0.00135499]\n",
            "[0.03049691 0.         0.02325227 ... 0.         0.         0.        ]\n",
            "[0.00926892 0.         0.         ... 0.         0.02938912 0.02920052]\n",
            "[0.09641589 0.         0.03426075 ... 0.0186465  0.         0.        ]\n",
            "[0.00147428 0.         0.         ... 0.         0.02240575 0.00664225]\n",
            "[0.01833867 0.         0.01275131 ... 0.00633827 0.         0.        ]\n",
            "[0.05438004 0.         0.03719785 ... 0.         0.01600906 0.04593163]\n",
            "[0.07860286 0.         0.02528056 ... 0.         0.0325969  0.07185465]\n",
            "[0.01387924 0.         0.03142842 ... 0.         0.         0.        ]\n",
            "[0.         0.         0.00525584 ... 0.         0.00935434 0.02042551]\n",
            "[0.08258209 0.         0.05735731 ... 0.         0.         0.02582543]\n",
            "[0.         0.         0.01327145 ... 0.         0.01863198 0.        ]\n",
            "[0.007384   0.         0.04519038 ... 0.         0.00326097 0.02856142]\n",
            "[0.06035341 0.         0.05211063 ... 0.         0.         0.        ]\n",
            "[0.00510138 0.00207019 0.06608419 ... 0.         0.00581754 0.00823286]\n",
            "[0.01464228 0.         0.03065132 ... 0.         0.02002661 0.        ]\n",
            "[0.05101571 0.         0.00123483 ... 0.02026843 0.02092637 0.00212351]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "[0.00830235 0.         0.         ... 0.01211706 0.03405437 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.09346437 0.0289403 ]\n",
            "[0.00819724 0.         0.04983412 ... 0.01215971 0.         0.00195487]\n",
            "[0.         0.         0.02166284 ... 0.         0.01335913 0.03533376]\n",
            "[0.         0.         0.02874352 ... 0.         0.00128435 0.024255  ]\n",
            "[0.0019868  0.         0.         ... 0.00302347 0.0005124  0.00171291]\n",
            "[0.01530833 0.         0.         ... 0.00313256 0.01760116 0.00185664]\n",
            "[0.0466646  0.         0.         ... 0.         0.         0.04229993]\n",
            "[0.03314197 0.         0.02096726 ... 0.         0.         0.02614068]\n",
            "[0.         0.         0.02708267 ... 0.02615436 0.         0.00601204]\n",
            "[0.02122114 0.         0.         ... 0.         0.00838374 0.00477657]\n",
            "[0.         0.         0.05459138 ... 0.02631597 0.00817283 0.00175002]\n",
            "[0.0157997  0.         0.         ... 0.         0.01556693 0.02895399]\n",
            "[0.03918988 0.         0.03278473 ... 0.         0.         0.04825875]\n",
            "[0.0195004  0.         0.         ... 0.         0.         0.02693213]\n",
            "[0.01224664 0.         0.02801099 ... 0.         0.02987409 0.02996885]\n",
            "[0.02666644 0.         0.03710134 ... 0.         0.         0.00627531]\n",
            "[0.03106626 0.         0.05161072 ... 0.         0.00966027 0.        ]\n",
            "[0.05358423 0.         0.04517249 ... 0.         0.         0.04138222]\n",
            "[0.02625293 0.         0.04520035 ... 0.         0.01921367 0.02402524]\n",
            "[0.06292044 0.         0.1796135  ... 0.         0.01321171 0.00409255]\n",
            "[0.00347004 0.         0.02845057 ... 0.         0.         0.        ]\n",
            "[0.00923871 0.         0.         ... 0.         0.02341306 0.03978307]\n",
            "[0.06030478 0.         0.05542228 ... 0.00449563 0.         0.00870448]\n",
            "[0.00330244 0.         0.         ... 0.         0.02068589 0.04951934]\n",
            "[0.0102163  0.         0.02446584 ... 0.         0.00639712 0.02299238]\n",
            "[0.00462641 0.         0.03110694 ... 0.00671375 0.02110481 0.00280274]\n",
            "[0.04446025 0.         0.05368697 ... 0.         0.04704791 0.00421992]\n",
            "[0.0215892  0.00056734 0.07578114 ... 0.         0.01608511 0.02135985]\n",
            "[0.0507982  0.         0.03537752 ... 0.01713884 0.         0.01001193]\n",
            "[0.05902748 0.         0.01835041 ... 0.         0.00918256 0.04272408]\n",
            "[0.01570076 0.         0.         ... 0.02098102 0.         0.        ]\n",
            "[0.00182104 0.         0.         ... 0.         0.03987264 0.0248328 ]\n",
            "[0.06544055 0.         0.0495452  ... 0.03916381 0.         0.00124169]\n",
            "[0.00963566 0.         0.         ... 0.         0.00977695 0.        ]\n",
            "[0.022529   0.         0.00859899 ... 0.         0.         0.15288909]\n",
            "[0.         0.         0.05044074 ... 0.0118391  0.         0.01654438]\n",
            "[0.02029813 0.         0.00434164 ... 0.         0.00106244 0.02607713]\n",
            "[0.00821449 0.         0.04486218 ... 0.         0.         0.        ]\n",
            "[0.02618473 0.         0.02367129 ... 0.         0.         0.01744074]\n",
            "[0.         0.         0.         ... 0.         0.         0.00615102]\n",
            "[0.01639017 0.         0.         ... 0.01666133 0.         0.        ]\n",
            "[0.02011167 0.         0.         ... 0.         0.         0.00580388]\n",
            "[0.00090657 0.         0.         ... 0.         0.0544578  0.        ]\n",
            "[0.01918503 0.         0.         ... 0.         0.02280155 0.01810979]\n",
            "[0.07113001 0.         0.         ... 0.         0.01916966 0.02901005]\n",
            "[0.02765809 0.         0.04548378 ... 0.         0.00947601 0.03146039]\n",
            "[0.0829112  0.         0.02744785 ... 0.         0.         0.        ]\n",
            "[0.01617967 0.         0.00675321 ... 0.03937702 0.01321123 0.        ]\n",
            "[0.00466256 0.         0.01759823 ... 0.         0.         0.05023537]\n",
            "[0]\ttrain-mlogloss:0.687517\ttest-mlogloss:0.689974\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 100 rounds.\n",
            "[1]\ttrain-mlogloss:0.681814\ttest-mlogloss:0.687445\n",
            "[2]\ttrain-mlogloss:0.677472\ttest-mlogloss:0.685085\n",
            "[3]\ttrain-mlogloss:0.671069\ttest-mlogloss:0.682362\n",
            "[4]\ttrain-mlogloss:0.665495\ttest-mlogloss:0.680186\n",
            "[5]\ttrain-mlogloss:0.659572\ttest-mlogloss:0.67729\n",
            "[6]\ttrain-mlogloss:0.654022\ttest-mlogloss:0.674863\n",
            "[7]\ttrain-mlogloss:0.648595\ttest-mlogloss:0.672531\n",
            "[8]\ttrain-mlogloss:0.64482\ttest-mlogloss:0.669327\n",
            "[9]\ttrain-mlogloss:0.640558\ttest-mlogloss:0.667331\n",
            "[10]\ttrain-mlogloss:0.635307\ttest-mlogloss:0.66512\n",
            "[11]\ttrain-mlogloss:0.630093\ttest-mlogloss:0.662087\n",
            "[12]\ttrain-mlogloss:0.625782\ttest-mlogloss:0.660339\n",
            "[13]\ttrain-mlogloss:0.621976\ttest-mlogloss:0.658615\n",
            "[14]\ttrain-mlogloss:0.617293\ttest-mlogloss:0.656818\n",
            "[15]\ttrain-mlogloss:0.612705\ttest-mlogloss:0.654095\n",
            "[16]\ttrain-mlogloss:0.608533\ttest-mlogloss:0.652222\n",
            "[17]\ttrain-mlogloss:0.603944\ttest-mlogloss:0.649995\n",
            "[18]\ttrain-mlogloss:0.599779\ttest-mlogloss:0.648558\n",
            "[19]\ttrain-mlogloss:0.595175\ttest-mlogloss:0.6457\n",
            "[20]\ttrain-mlogloss:0.590959\ttest-mlogloss:0.6442\n",
            "[21]\ttrain-mlogloss:0.587186\ttest-mlogloss:0.640602\n",
            "[22]\ttrain-mlogloss:0.583855\ttest-mlogloss:0.639288\n",
            "[23]\ttrain-mlogloss:0.580124\ttest-mlogloss:0.637363\n",
            "[24]\ttrain-mlogloss:0.576458\ttest-mlogloss:0.636318\n",
            "[25]\ttrain-mlogloss:0.573096\ttest-mlogloss:0.634717\n",
            "[26]\ttrain-mlogloss:0.569265\ttest-mlogloss:0.632635\n",
            "[27]\ttrain-mlogloss:0.565733\ttest-mlogloss:0.631646\n",
            "[28]\ttrain-mlogloss:0.562024\ttest-mlogloss:0.629991\n",
            "[29]\ttrain-mlogloss:0.558685\ttest-mlogloss:0.628522\n",
            "[30]\ttrain-mlogloss:0.555519\ttest-mlogloss:0.627056\n",
            "[31]\ttrain-mlogloss:0.552172\ttest-mlogloss:0.625079\n",
            "[32]\ttrain-mlogloss:0.548496\ttest-mlogloss:0.623811\n",
            "[33]\ttrain-mlogloss:0.544746\ttest-mlogloss:0.622898\n",
            "[34]\ttrain-mlogloss:0.541049\ttest-mlogloss:0.62137\n",
            "[35]\ttrain-mlogloss:0.537321\ttest-mlogloss:0.620472\n",
            "[36]\ttrain-mlogloss:0.53391\ttest-mlogloss:0.618546\n",
            "[37]\ttrain-mlogloss:0.530813\ttest-mlogloss:0.61776\n",
            "[38]\ttrain-mlogloss:0.529123\ttest-mlogloss:0.615222\n",
            "[39]\ttrain-mlogloss:0.525353\ttest-mlogloss:0.613757\n",
            "[40]\ttrain-mlogloss:0.522448\ttest-mlogloss:0.612878\n",
            "[41]\ttrain-mlogloss:0.518942\ttest-mlogloss:0.612178\n",
            "[42]\ttrain-mlogloss:0.51549\ttest-mlogloss:0.610133\n",
            "[43]\ttrain-mlogloss:0.51261\ttest-mlogloss:0.608794\n",
            "[44]\ttrain-mlogloss:0.509647\ttest-mlogloss:0.607865\n",
            "[45]\ttrain-mlogloss:0.50711\ttest-mlogloss:0.607825\n",
            "[46]\ttrain-mlogloss:0.504175\ttest-mlogloss:0.606322\n",
            "[47]\ttrain-mlogloss:0.502119\ttest-mlogloss:0.605236\n",
            "[48]\ttrain-mlogloss:0.499838\ttest-mlogloss:0.604853\n",
            "[49]\ttrain-mlogloss:0.497555\ttest-mlogloss:0.604917\n",
            "[50]\ttrain-mlogloss:0.494387\ttest-mlogloss:0.603427\n",
            "[51]\ttrain-mlogloss:0.491682\ttest-mlogloss:0.601356\n",
            "[52]\ttrain-mlogloss:0.489209\ttest-mlogloss:0.600982\n",
            "[53]\ttrain-mlogloss:0.486433\ttest-mlogloss:0.599506\n",
            "[54]\ttrain-mlogloss:0.483734\ttest-mlogloss:0.598225\n",
            "[55]\ttrain-mlogloss:0.481044\ttest-mlogloss:0.597952\n",
            "[56]\ttrain-mlogloss:0.47881\ttest-mlogloss:0.596135\n",
            "[57]\ttrain-mlogloss:0.476266\ttest-mlogloss:0.594494\n",
            "[58]\ttrain-mlogloss:0.473649\ttest-mlogloss:0.593189\n",
            "[59]\ttrain-mlogloss:0.473013\ttest-mlogloss:0.592285\n",
            "[60]\ttrain-mlogloss:0.470573\ttest-mlogloss:0.592188\n",
            "[61]\ttrain-mlogloss:0.470156\ttest-mlogloss:0.592245\n",
            "[62]\ttrain-mlogloss:0.46773\ttest-mlogloss:0.590534\n",
            "[63]\ttrain-mlogloss:0.466063\ttest-mlogloss:0.59049\n",
            "[64]\ttrain-mlogloss:0.4643\ttest-mlogloss:0.591061\n",
            "[65]\ttrain-mlogloss:0.461951\ttest-mlogloss:0.589826\n",
            "[66]\ttrain-mlogloss:0.459516\ttest-mlogloss:0.589832\n",
            "[67]\ttrain-mlogloss:0.457743\ttest-mlogloss:0.589708\n",
            "[68]\ttrain-mlogloss:0.45569\ttest-mlogloss:0.589322\n",
            "[69]\ttrain-mlogloss:0.453345\ttest-mlogloss:0.58896\n",
            "[70]\ttrain-mlogloss:0.451076\ttest-mlogloss:0.588519\n",
            "[71]\ttrain-mlogloss:0.448964\ttest-mlogloss:0.589044\n",
            "[72]\ttrain-mlogloss:0.446987\ttest-mlogloss:0.587524\n",
            "[73]\ttrain-mlogloss:0.444687\ttest-mlogloss:0.586989\n",
            "[74]\ttrain-mlogloss:0.443089\ttest-mlogloss:0.586331\n",
            "[75]\ttrain-mlogloss:0.44137\ttest-mlogloss:0.586144\n",
            "[76]\ttrain-mlogloss:0.439455\ttest-mlogloss:0.585225\n",
            "[77]\ttrain-mlogloss:0.437034\ttest-mlogloss:0.584944\n",
            "[78]\ttrain-mlogloss:0.43467\ttest-mlogloss:0.58435\n",
            "[79]\ttrain-mlogloss:0.432572\ttest-mlogloss:0.584388\n",
            "[80]\ttrain-mlogloss:0.430557\ttest-mlogloss:0.584283\n",
            "[81]\ttrain-mlogloss:0.429036\ttest-mlogloss:0.584835\n",
            "[82]\ttrain-mlogloss:0.427366\ttest-mlogloss:0.583846\n",
            "[83]\ttrain-mlogloss:0.425565\ttest-mlogloss:0.583617\n",
            "[84]\ttrain-mlogloss:0.424056\ttest-mlogloss:0.583516\n",
            "[85]\ttrain-mlogloss:0.422286\ttest-mlogloss:0.583416\n",
            "[86]\ttrain-mlogloss:0.420392\ttest-mlogloss:0.583698\n",
            "[87]\ttrain-mlogloss:0.41831\ttest-mlogloss:0.58408\n",
            "[88]\ttrain-mlogloss:0.416327\ttest-mlogloss:0.583206\n",
            "[89]\ttrain-mlogloss:0.414486\ttest-mlogloss:0.583409\n",
            "[90]\ttrain-mlogloss:0.412738\ttest-mlogloss:0.583644\n",
            "[91]\ttrain-mlogloss:0.411136\ttest-mlogloss:0.583859\n",
            "[92]\ttrain-mlogloss:0.409247\ttest-mlogloss:0.58268\n",
            "[93]\ttrain-mlogloss:0.407705\ttest-mlogloss:0.582849\n",
            "[94]\ttrain-mlogloss:0.406323\ttest-mlogloss:0.583264\n",
            "[95]\ttrain-mlogloss:0.404725\ttest-mlogloss:0.583366\n",
            "[96]\ttrain-mlogloss:0.403387\ttest-mlogloss:0.583228\n",
            "[97]\ttrain-mlogloss:0.401886\ttest-mlogloss:0.583112\n",
            "[98]\ttrain-mlogloss:0.400515\ttest-mlogloss:0.583303\n",
            "[99]\ttrain-mlogloss:0.399354\ttest-mlogloss:0.581364\n",
            "[100]\ttrain-mlogloss:0.397973\ttest-mlogloss:0.580072\n",
            "[101]\ttrain-mlogloss:0.396661\ttest-mlogloss:0.5807\n",
            "[102]\ttrain-mlogloss:0.395044\ttest-mlogloss:0.580103\n",
            "[103]\ttrain-mlogloss:0.393396\ttest-mlogloss:0.580154\n",
            "[104]\ttrain-mlogloss:0.39176\ttest-mlogloss:0.580507\n",
            "[105]\ttrain-mlogloss:0.39001\ttest-mlogloss:0.580282\n",
            "[106]\ttrain-mlogloss:0.388917\ttest-mlogloss:0.580581\n",
            "[107]\ttrain-mlogloss:0.387674\ttest-mlogloss:0.580912\n",
            "[108]\ttrain-mlogloss:0.386158\ttest-mlogloss:0.580778\n",
            "[109]\ttrain-mlogloss:0.384719\ttest-mlogloss:0.581081\n",
            "[110]\ttrain-mlogloss:0.383261\ttest-mlogloss:0.580013\n",
            "[111]\ttrain-mlogloss:0.381837\ttest-mlogloss:0.579931\n",
            "[112]\ttrain-mlogloss:0.380915\ttest-mlogloss:0.578286\n",
            "[113]\ttrain-mlogloss:0.379721\ttest-mlogloss:0.578845\n",
            "[114]\ttrain-mlogloss:0.378376\ttest-mlogloss:0.579106\n",
            "[115]\ttrain-mlogloss:0.37768\ttest-mlogloss:0.580287\n",
            "[116]\ttrain-mlogloss:0.376493\ttest-mlogloss:0.580713\n",
            "[117]\ttrain-mlogloss:0.37564\ttest-mlogloss:0.580846\n",
            "[118]\ttrain-mlogloss:0.374602\ttest-mlogloss:0.581067\n",
            "[119]\ttrain-mlogloss:0.373636\ttest-mlogloss:0.580341\n",
            "[120]\ttrain-mlogloss:0.373058\ttest-mlogloss:0.580798\n",
            "[121]\ttrain-mlogloss:0.371681\ttest-mlogloss:0.580787\n",
            "[122]\ttrain-mlogloss:0.370135\ttest-mlogloss:0.580821\n",
            "[123]\ttrain-mlogloss:0.369664\ttest-mlogloss:0.581163\n",
            "[124]\ttrain-mlogloss:0.368823\ttest-mlogloss:0.580768\n",
            "[125]\ttrain-mlogloss:0.367751\ttest-mlogloss:0.581635\n",
            "[126]\ttrain-mlogloss:0.366426\ttest-mlogloss:0.581148\n",
            "[127]\ttrain-mlogloss:0.365027\ttest-mlogloss:0.579514\n",
            "[128]\ttrain-mlogloss:0.363921\ttest-mlogloss:0.579957\n",
            "[129]\ttrain-mlogloss:0.362661\ttest-mlogloss:0.580355\n",
            "[130]\ttrain-mlogloss:0.361724\ttest-mlogloss:0.581296\n",
            "[131]\ttrain-mlogloss:0.360626\ttest-mlogloss:0.581069\n",
            "[132]\ttrain-mlogloss:0.359662\ttest-mlogloss:0.580107\n",
            "[133]\ttrain-mlogloss:0.359174\ttest-mlogloss:0.580152\n",
            "[134]\ttrain-mlogloss:0.358121\ttest-mlogloss:0.579846\n",
            "[135]\ttrain-mlogloss:0.35699\ttest-mlogloss:0.579798\n",
            "[136]\ttrain-mlogloss:0.356376\ttest-mlogloss:0.579566\n",
            "[137]\ttrain-mlogloss:0.356029\ttest-mlogloss:0.579345\n",
            "[138]\ttrain-mlogloss:0.355259\ttest-mlogloss:0.577636\n",
            "[139]\ttrain-mlogloss:0.354392\ttest-mlogloss:0.577267\n",
            "[140]\ttrain-mlogloss:0.353577\ttest-mlogloss:0.576955\n",
            "[141]\ttrain-mlogloss:0.352497\ttest-mlogloss:0.576805\n",
            "[142]\ttrain-mlogloss:0.351499\ttest-mlogloss:0.577308\n",
            "[143]\ttrain-mlogloss:0.350836\ttest-mlogloss:0.57789\n",
            "[144]\ttrain-mlogloss:0.349874\ttest-mlogloss:0.578004\n",
            "[145]\ttrain-mlogloss:0.349157\ttest-mlogloss:0.578492\n",
            "[146]\ttrain-mlogloss:0.348145\ttest-mlogloss:0.578742\n",
            "[147]\ttrain-mlogloss:0.347254\ttest-mlogloss:0.579268\n",
            "[148]\ttrain-mlogloss:0.346401\ttest-mlogloss:0.579616\n",
            "[149]\ttrain-mlogloss:0.34551\ttest-mlogloss:0.578578\n",
            "[150]\ttrain-mlogloss:0.344569\ttest-mlogloss:0.578473\n",
            "[151]\ttrain-mlogloss:0.343826\ttest-mlogloss:0.579521\n",
            "[152]\ttrain-mlogloss:0.342914\ttest-mlogloss:0.57815\n",
            "[153]\ttrain-mlogloss:0.342033\ttest-mlogloss:0.579138\n",
            "[154]\ttrain-mlogloss:0.341307\ttest-mlogloss:0.579155\n",
            "[155]\ttrain-mlogloss:0.340542\ttest-mlogloss:0.58034\n",
            "[156]\ttrain-mlogloss:0.339631\ttest-mlogloss:0.580318\n",
            "[157]\ttrain-mlogloss:0.338982\ttest-mlogloss:0.579647\n",
            "[158]\ttrain-mlogloss:0.338155\ttest-mlogloss:0.579422\n",
            "[159]\ttrain-mlogloss:0.33757\ttest-mlogloss:0.578781\n",
            "[160]\ttrain-mlogloss:0.336522\ttest-mlogloss:0.579114\n",
            "[161]\ttrain-mlogloss:0.335724\ttest-mlogloss:0.579957\n",
            "[162]\ttrain-mlogloss:0.334995\ttest-mlogloss:0.580649\n",
            "[163]\ttrain-mlogloss:0.333935\ttest-mlogloss:0.581099\n",
            "[164]\ttrain-mlogloss:0.332949\ttest-mlogloss:0.582114\n",
            "[165]\ttrain-mlogloss:0.332182\ttest-mlogloss:0.583074\n",
            "[166]\ttrain-mlogloss:0.331406\ttest-mlogloss:0.583725\n",
            "[167]\ttrain-mlogloss:0.330842\ttest-mlogloss:0.584537\n",
            "[168]\ttrain-mlogloss:0.329983\ttest-mlogloss:0.584462\n",
            "[169]\ttrain-mlogloss:0.329168\ttest-mlogloss:0.584267\n",
            "[170]\ttrain-mlogloss:0.328666\ttest-mlogloss:0.585027\n",
            "[171]\ttrain-mlogloss:0.327928\ttest-mlogloss:0.586079\n",
            "[172]\ttrain-mlogloss:0.327461\ttest-mlogloss:0.586073\n",
            "[173]\ttrain-mlogloss:0.326964\ttest-mlogloss:0.586642\n",
            "[174]\ttrain-mlogloss:0.325954\ttest-mlogloss:0.586453\n",
            "[175]\ttrain-mlogloss:0.325601\ttest-mlogloss:0.586335\n",
            "[176]\ttrain-mlogloss:0.324982\ttest-mlogloss:0.586799\n",
            "[177]\ttrain-mlogloss:0.324389\ttest-mlogloss:0.586391\n",
            "[178]\ttrain-mlogloss:0.323714\ttest-mlogloss:0.586295\n",
            "[179]\ttrain-mlogloss:0.323218\ttest-mlogloss:0.586714\n",
            "[180]\ttrain-mlogloss:0.322499\ttest-mlogloss:0.587024\n",
            "[181]\ttrain-mlogloss:0.322169\ttest-mlogloss:0.587038\n",
            "[182]\ttrain-mlogloss:0.321548\ttest-mlogloss:0.585783\n",
            "[183]\ttrain-mlogloss:0.321129\ttest-mlogloss:0.585193\n",
            "[184]\ttrain-mlogloss:0.320458\ttest-mlogloss:0.586418\n",
            "[185]\ttrain-mlogloss:0.319872\ttest-mlogloss:0.587244\n",
            "[186]\ttrain-mlogloss:0.319459\ttest-mlogloss:0.587296\n",
            "[187]\ttrain-mlogloss:0.318772\ttest-mlogloss:0.585649\n",
            "[188]\ttrain-mlogloss:0.317999\ttest-mlogloss:0.585452\n",
            "[189]\ttrain-mlogloss:0.317534\ttest-mlogloss:0.586336\n",
            "[190]\ttrain-mlogloss:0.316739\ttest-mlogloss:0.585653\n",
            "[191]\ttrain-mlogloss:0.316227\ttest-mlogloss:0.586639\n",
            "[192]\ttrain-mlogloss:0.315907\ttest-mlogloss:0.587472\n",
            "[193]\ttrain-mlogloss:0.315107\ttest-mlogloss:0.58626\n",
            "[194]\ttrain-mlogloss:0.314577\ttest-mlogloss:0.585901\n",
            "[195]\ttrain-mlogloss:0.313882\ttest-mlogloss:0.586493\n",
            "[196]\ttrain-mlogloss:0.313415\ttest-mlogloss:0.58654\n",
            "[197]\ttrain-mlogloss:0.312971\ttest-mlogloss:0.586423\n",
            "[198]\ttrain-mlogloss:0.312403\ttest-mlogloss:0.58621\n",
            "[199]\ttrain-mlogloss:0.311859\ttest-mlogloss:0.58676\n",
            "[200]\ttrain-mlogloss:0.311508\ttest-mlogloss:0.586786\n",
            "[201]\ttrain-mlogloss:0.310973\ttest-mlogloss:0.588329\n",
            "[202]\ttrain-mlogloss:0.310337\ttest-mlogloss:0.589154\n",
            "[203]\ttrain-mlogloss:0.309935\ttest-mlogloss:0.5903\n",
            "[204]\ttrain-mlogloss:0.309328\ttest-mlogloss:0.590963\n",
            "[205]\ttrain-mlogloss:0.30888\ttest-mlogloss:0.592179\n",
            "[206]\ttrain-mlogloss:0.308463\ttest-mlogloss:0.592622\n",
            "[207]\ttrain-mlogloss:0.307989\ttest-mlogloss:0.593507\n",
            "[208]\ttrain-mlogloss:0.307438\ttest-mlogloss:0.593375\n",
            "[209]\ttrain-mlogloss:0.306977\ttest-mlogloss:0.594405\n",
            "[210]\ttrain-mlogloss:0.306511\ttest-mlogloss:0.594281\n",
            "[211]\ttrain-mlogloss:0.306019\ttest-mlogloss:0.594893\n",
            "[212]\ttrain-mlogloss:0.305327\ttest-mlogloss:0.59557\n",
            "[213]\ttrain-mlogloss:0.305052\ttest-mlogloss:0.596236\n",
            "[214]\ttrain-mlogloss:0.30495\ttest-mlogloss:0.597678\n",
            "[215]\ttrain-mlogloss:0.304868\ttest-mlogloss:0.598577\n",
            "[216]\ttrain-mlogloss:0.304291\ttest-mlogloss:0.599714\n",
            "[217]\ttrain-mlogloss:0.304021\ttest-mlogloss:0.600175\n",
            "[218]\ttrain-mlogloss:0.303299\ttest-mlogloss:0.600813\n",
            "[219]\ttrain-mlogloss:0.303\ttest-mlogloss:0.601388\n",
            "[220]\ttrain-mlogloss:0.302367\ttest-mlogloss:0.599711\n",
            "[221]\ttrain-mlogloss:0.301968\ttest-mlogloss:0.600585\n",
            "[222]\ttrain-mlogloss:0.301372\ttest-mlogloss:0.600841\n",
            "[223]\ttrain-mlogloss:0.301003\ttest-mlogloss:0.600031\n",
            "[224]\ttrain-mlogloss:0.300295\ttest-mlogloss:0.600347\n",
            "[225]\ttrain-mlogloss:0.299861\ttest-mlogloss:0.600565\n",
            "[226]\ttrain-mlogloss:0.299644\ttest-mlogloss:0.600994\n",
            "[227]\ttrain-mlogloss:0.299222\ttest-mlogloss:0.60206\n",
            "[228]\ttrain-mlogloss:0.298971\ttest-mlogloss:0.603504\n",
            "[229]\ttrain-mlogloss:0.29873\ttest-mlogloss:0.603848\n",
            "[230]\ttrain-mlogloss:0.298493\ttest-mlogloss:0.60468\n",
            "[231]\ttrain-mlogloss:0.297914\ttest-mlogloss:0.604818\n",
            "[232]\ttrain-mlogloss:0.297451\ttest-mlogloss:0.605683\n",
            "[233]\ttrain-mlogloss:0.297385\ttest-mlogloss:0.606851\n",
            "[234]\ttrain-mlogloss:0.297198\ttest-mlogloss:0.607988\n",
            "[235]\ttrain-mlogloss:0.297119\ttest-mlogloss:0.608724\n",
            "[236]\ttrain-mlogloss:0.296818\ttest-mlogloss:0.609851\n",
            "[237]\ttrain-mlogloss:0.296314\ttest-mlogloss:0.610454\n",
            "[238]\ttrain-mlogloss:0.296126\ttest-mlogloss:0.61174\n",
            "[239]\ttrain-mlogloss:0.295583\ttest-mlogloss:0.611321\n",
            "[240]\ttrain-mlogloss:0.295213\ttest-mlogloss:0.611155\n",
            "[241]\ttrain-mlogloss:0.29496\ttest-mlogloss:0.610752\n",
            "Stopping. Best iteration:\n",
            "[141]\ttrain-mlogloss:0.352497\ttest-mlogloss:0.576805\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OnAYG3DETzi",
        "outputId": "875549eb-51f6-4c0f-c3c0-6a306c0fc92e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "# Create GAN estimator.\n",
        "\n",
        "import tensorflow as tf\n",
        "tfgan = tf.contrib.gan\n",
        "\n",
        "gan_estimator = tfgan.estimator.GANEstimator(\n",
        "         model_dir = '{googlepath}/checkpoint',\n",
        "         generator_fn=GAN(num_features=5, num_historical_days=20,generator_input_size=200),\n",
        "         discriminator_fn=TrainCNN(num_historical_days=20, days=5, pct_change=5),\n",
        "         generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
        "         discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
        "         generator_optimizer=tf.compat.v1.train.AdamOptimizer(0.1, 0.5),\n",
        "         discriminator_optimizer=tf.compat.v1.train.AdamOptimizer(0.1, 0.5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"dropout/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_1/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_2/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"dropout_4/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_5/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_6/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"cnn_1/Relu:0\", shape=(?, 20, 1, 16), dtype=float32)\n",
            "Tensor(\"cnn_1/Relu_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"cnn_1/Relu_2:0\", shape=(?, 18, 1, 64), dtype=float32)\n",
            "1152\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQj9QRwNxiDX",
        "outputId": "8ed9fb7b-d340-4cf8-e954-3bc9266b9550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "##Predict mode\n",
        "#PREDICTING THE MODEL\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "class Predict:\n",
        "  def __init__(self, num_historical_days=20, days=10, pct_change=0, gan_model=f'{googlepath}/deployed_models/gan', cnn_modle=f'{googlepath}/deployed_models/cnn', xgb_model=f'{googlepath}/deployed_models/xgb'):\n",
        "    self.data = []\n",
        "    self.num_historical_days = num_historical_days\n",
        "    self.gan_model = gan_model\n",
        "    self.cnn_modle = cnn_modle\n",
        "    self.xgb_model = xgb_model\n",
        "    \n",
        "    files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")] \n",
        "    for file in files:\n",
        "      \n",
        "      print(file)\n",
        "      df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "      df = df[['open','high','low','close','volume']]\n",
        "            # data for new column labels that will use the pct_change of the closing data.\n",
        "            # pct_change measure change between current and prior element. Map these into a 1x2\n",
        "            # array to show if the pct_change > (our desired threshold) or less than.\n",
        "            \n",
        "      df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "      df = df.dropna()\n",
        "      self.data.append((file.split('/')[-1], df.iloc[0], df[200:200+num_historical_days].values))\n",
        "      #split the df into arrays of length num_historical_days and append\n",
        "      # to data, i.e. array of df[curr - num_days : curr] -> a batch of values\n",
        "      # appending if price went up or down in curr day of \"i\" we are lookin\n",
        "      # at\n",
        "      \n",
        "      \n",
        "  def gan_predict(self):\n",
        "    tf.reset_default_graph()\n",
        "    gan = GAN(num_features=5, num_historical_days=self.num_historical_days, generator_input_size=200, is_train=False)\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      saver = tf.train.Saver()\n",
        "      saver.restore(sess, self.gan_model)\n",
        "      clf = joblib.load(self.xgb_model)\n",
        "      for sym, date, data in self.data:\n",
        "        features = sess.run(gan.features, feed_dict={gan.X:[data]})\n",
        "        features = xgb.DMatrix(features)\n",
        "        print('{} {} {}'.format(str(date).split(' ')[0], sym, clf.predict(features)[0][1] > 0.5))\n",
        "        #predictions = np.array([x for x in gan_estimator.predict(p.gan_predict())])\n",
        "        #print(predictions)\n",
        "\n",
        "p = Predict()\n",
        "p.gan_predict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n",
            "Tensor(\"Relu:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_2:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"Relu_4:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_5:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_6:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "open AMZN.csv True\n",
            "open AAPL.csv True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqr-JgUUT7R8"
      },
      "source": [
        "**Confusion Matrix basically gets stored into deployed models. Open it up to have a good look. Here is the a model.** \n",
        "\n",
        "**The up and down are indicating the movement of the stock price. So if the stock is going up we predict that it is going up 93% of the time and if it is going down we predict it 87% of the time which is great!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PvhVGi89lMF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}